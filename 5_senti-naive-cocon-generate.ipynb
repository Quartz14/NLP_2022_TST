{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:16:52.913732Z",
     "iopub.status.busy": "2022-04-01T12:16:52.913246Z",
     "iopub.status.idle": "2022-04-01T12:16:54.401611Z",
     "shell.execute_reply": "2022-04-01T12:16:54.400944Z",
     "shell.execute_reply.started": "2022-04-01T12:16:52.913640Z"
    },
    "id": "8cSRDouXJl1X",
    "outputId": "8597666b-ab75-4f89-e2df-e7183722f2f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:16:55.991974Z",
     "iopub.status.busy": "2022-04-01T12:16:55.991520Z",
     "iopub.status.idle": "2022-04-01T12:16:55.998078Z",
     "shell.execute_reply": "2022-04-01T12:16:55.997325Z",
     "shell.execute_reply.started": "2022-04-01T12:16:55.991935Z"
    },
    "id": "gvieVnQBJzEO",
    "outputId": "53048095-6d84-4f96-8603-1dadf8ab1e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:02.647457Z",
     "iopub.status.busy": "2022-04-01T12:17:02.646982Z",
     "iopub.status.idle": "2022-04-01T12:17:07.302342Z",
     "shell.execute_reply": "2022-04-01T12:17:07.301632Z",
     "shell.execute_reply.started": "2022-04-01T12:17:02.647418Z"
    },
    "id": "XYw8QPQvXiEd"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers.activations import gelu_new\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "#import wget\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed import get_rank, get_world_size\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.modules import ModuleList\n",
    "import copy\n",
    "\n",
    "from transformers import AdamW, GPT2Config, GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:09.751166Z",
     "iopub.status.busy": "2022-04-01T12:17:09.750578Z",
     "iopub.status.idle": "2022-04-01T12:17:09.796497Z",
     "shell.execute_reply": "2022-04-01T12:17:09.795472Z",
     "shell.execute_reply.started": "2022-04-01T12:17:09.751120Z"
    },
    "id": "l3SwL9idXiEp"
   },
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, nx, nf):\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        w = torch.empty(nx, nf)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(*size_out)\n",
    "        return x\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dropout, d_model=768, nx=768*4):\n",
    "        super().__init__()\n",
    "        self.c_fc    = Conv1D(d_model, nx)\n",
    "        self.c_proj  = Conv1D(nx, d_model)\n",
    "        self.act     = F.gelu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
    "    \n",
    "\n",
    "\n",
    "def _get_clones(module, n):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(n)])\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False):\n",
    "        super().__init__()\n",
    "        self.n_head  = n_head\n",
    "        self.d_model = d_model\n",
    "        self.c_attn  = Conv1D(d_model, d_model*3)\n",
    "        self.scale   = scale\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.c_proj  = Conv1D(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        \"return shape [`batch`, `head`, `sequence`, `features`]\"\n",
    "        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head) \n",
    "        x = x.view(*new_shape)\n",
    "        return x.permute(0, 2, 1, 3) \n",
    "    \n",
    "    def _attn(self, q, k, v, attn_mask=None):\n",
    "        scores  = torch.matmul(q, k.transpose(-2, -1))\n",
    "        if self.scale: scores = scores/math.sqrt(v.size(-1))\n",
    "        nd, ns  = scores.size(-2), scores.size(-1)\n",
    "        if attn_mask is not None: scores = scores + attn_mask\n",
    "        scores  = self.softmax(scores)\n",
    "        scores  = self.dropout(scores)\n",
    "        outputs = torch.matmul(scores, v)\n",
    "        return outputs\n",
    "    \n",
    "    def merge_heads(self, x):\n",
    "        x         = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n",
    "        return x.view(*new_shape)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x        = self.c_attn(x) #new `x` shape - `[1,3,2304]`\n",
    "        q, k, v  = x.split(self.d_model, dim=2)\n",
    "        q, k, v  = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "        out      = self._attn(q, k, v)\n",
    "        out      = self.merge_heads(out)\n",
    "        out      = self.c_proj(out)\n",
    "        return out\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn        = Attention(d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False)\n",
    "        self.feedforward = FeedForward(dropout=0.1, d_model=768, nx=768*4)\n",
    "        self.ln_1        = LayerNorm(d_model)\n",
    "        self.ln_2        = LayerNorm(d_model)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.feedforward(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.nlayers = nlayers\n",
    "        block        = TransformerBlock(d_model=768, n_head=12, dropout=0.1)\n",
    "        self.h       = _get_clones(block, nlayers)\n",
    "        self.wte     = nn.Embedding(vcb_sz, d_model)\n",
    "        self.wpe     = nn.Embedding(n_ctx, d_model)\n",
    "        self.drop    = nn.Dropout(0.1)\n",
    "        self.ln_f    = LayerNorm(d_model)\n",
    "        self.out     = nn.Linear(d_model, vcb_sz, bias=False)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.init_weights()\n",
    "        #self.temp = TransformerBlockc()\n",
    "        from transformers import GPT2Tokenizer\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        #self.n  = torch.tensor(tokenizer.encode('Negative')).type(torch.LongTensor)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.out.weight = self.wte.weight\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "    def prepare_embeds_inputs_for_generation(self, inputs_embeds, **kwargs):\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if \"past\" in kwargs and kwargs[\"past\"]:\n",
    "            inputs_embeds = inputs_embeds[:, -1:, :]\n",
    "\n",
    "        inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        inputs.update(kwargs)\n",
    "        return inputs\n",
    "\n",
    "    def prepare_hidden_state_inputs_for_generation(self, input_hidden_state, **kwargs):\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if \"past\" in kwargs and kwargs[\"past\"]:\n",
    "            input_hidden_state = input_hidden_state[:, -1:, :]\n",
    "\n",
    "        inputs = {\"input_hidden_state\": input_hidden_state}\n",
    "        inputs.update(kwargs)\n",
    "        return inputs\n",
    "            \n",
    "\n",
    "    def forward_half1(self,src, labels=None, pos_ids=None, inputs_embeds=None):\n",
    "        if(inputs_embeds): #x=src is input embeds\n",
    "            if pos_ids is None: pos_ids = torch.arange(0, src.size(-2)).unsqueeze(0)\n",
    "            wpe2 = nn.Embedding(src.size(-2), 768).to(device)\n",
    "            pos_ids = pos_ids.to(device)\n",
    "            position_embeds = wpe2(pos_ids).to(device)\n",
    "            inp = self.drop(src + position_embeds)\n",
    "            \n",
    "        else:\n",
    "            if pos_ids is None: pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n",
    "            pos_ids = pos_ids.to(device)\n",
    "            position_embeds = self.wpe(pos_ids)\n",
    "            position_embeds=position_embeds.to(device)\n",
    "\n",
    "            inp = self.drop((self.wte(src)+position_embeds))\n",
    "            \n",
    "        #inp = self.drop((self.wte(src)+self.wpe(pos_ids)))\n",
    "        for i in range(6): inp = self.h[i](inp)\n",
    "        return inp\n",
    "        \n",
    "    \n",
    "    def forward_half2(self, inp,labels=None, pos_ids=None,lm_logit_first_index=0,lm_logit_last_index=-1,\n",
    "                     lm_labels_first_index=1, lm_labels_last_index=None):\n",
    "        for i in range(6,12): inp = self.h[i](inp)\n",
    "        inp     = self.ln_f(inp)\n",
    "        logits  = self.out(inp)\n",
    "        outputs = (logits,) + (inp,)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., lm_logit_first_index:lm_logit_last_index, :].contiguous() # default lm_logit_first_index=0, lm_logit_last_index=-1,\n",
    "            shift_labels = labels[..., lm_labels_first_index:lm_labels_last_index].contiguous() # default lm_labels_first_index=1, lm_labels_last_index=None,\n",
    "\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs,logits\n",
    "    \"\"\"\n",
    "   \n",
    "    def cocon(self,inp,content):\n",
    "        #content = torch.tensor(self.tokenizer.encode(content))#.long()\n",
    "        content = self.forward_half1(content)\n",
    "        x  = self.temp(inp,content)\n",
    "        h_t_2 = x[:,:-1,:]\n",
    "        h_t_1 = x[:,-1,:]\n",
    "        h_t_1 = torch.unsqueeze(h_t_1, 1)\n",
    "        h_ = torch.cat([h_t_2,h_t_1],dim=1)\n",
    "        return h_\n",
    "        \"\"\" \n",
    "    \n",
    "    def forward(self, x,labels=None,path='all',lm_logit_first_index=0,lm_logit_last_index=-1,\n",
    "                     lm_labels_first_index=1, lm_labels_last_index=None, inputs_embeds=None):\n",
    "        \n",
    "        if path=='all':\n",
    "            x = self.forward_half1(x,inputs_embeds=inputs_embeds)\n",
    "            #x = self.cocon(x,content)\n",
    "            x = self.forward_half2(x,labels)\n",
    "        elif path=='half1':\n",
    "            x = self.forward_half1(x,inputs_embeds=inputs_embeds)\n",
    "        elif path=='half2':\n",
    "            x = self.forward_half2(x,labels,lm_logit_first_index=0,lm_logit_last_index=-1,\n",
    "                     lm_labels_first_index=1, lm_labels_last_index=None)\n",
    "        #elif path=='cocon':\n",
    "        #    x = self.cocon(x,content)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:12.050772Z",
     "iopub.status.busy": "2022-04-01T12:17:12.050279Z",
     "iopub.status.idle": "2022-04-01T12:17:17.356378Z",
     "shell.execute_reply": "2022-04-01T12:17:17.355638Z",
     "shell.execute_reply.started": "2022-04-01T12:17:12.050739Z"
    },
    "id": "tXTPbbZjXiEx",
    "outputId": "7546f4f6-19a1-4219-ad6a-dfd755c19d5d"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:17.358241Z",
     "iopub.status.busy": "2022-04-01T12:17:17.357909Z",
     "iopub.status.idle": "2022-04-01T12:17:22.319475Z",
     "shell.execute_reply": "2022-04-01T12:17:22.318703Z",
     "shell.execute_reply.started": "2022-04-01T12:17:17.358204Z"
    },
    "id": "HbvLKU2yXiEy"
   },
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "model_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:22.321059Z",
     "iopub.status.busy": "2022-04-01T12:17:22.320790Z",
     "iopub.status.idle": "2022-04-01T12:17:22.327334Z",
     "shell.execute_reply": "2022-04-01T12:17:22.325101Z",
     "shell.execute_reply.started": "2022-04-01T12:17:22.321023Z"
    },
    "id": "CXbf-366XiE0"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    #print(param)\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:22.329517Z",
     "iopub.status.busy": "2022-04-01T12:17:22.329229Z",
     "iopub.status.idle": "2022-04-01T12:17:26.950674Z",
     "shell.execute_reply": "2022-04-01T12:17:26.949874Z",
     "shell.execute_reply.started": "2022-04-01T12:17:22.329482Z"
    },
    "id": "NTR5gTk7XiE1"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('gpt_wt/gpt2-pytorch_model.bin') #pretrained weights\n",
    "\n",
    "old_keys = []\n",
    "new_keys = []\n",
    "for key in state_dict.keys(): \n",
    "    if \"mlp\" in key: #The hugging face state dict references the feedforward network as mlp, need to replace to `feedforward` be able to reuse these weights\n",
    "        new_key = key.replace(\"mlp\", \"feedforward\")\n",
    "        new_keys.append(new_key)\n",
    "        old_keys.append(key)\n",
    "\n",
    "for old_key, new_key in zip(old_keys, new_keys): \n",
    "    state_dict[new_key]=state_dict.pop(old_key)\n",
    "\n",
    "pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:26.952407Z",
     "iopub.status.busy": "2022-04-01T12:17:26.952152Z",
     "iopub.status.idle": "2022-04-01T12:17:27.078570Z",
     "shell.execute_reply": "2022-04-01T12:17:27.077884Z",
     "shell.execute_reply.started": "2022-04-01T12:17:26.952373Z"
    },
    "id": "J-8mLZcaXiE2",
    "outputId": "9cb968b3-5107-4d8f-9296-9b6dcfc2d24c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:27.080680Z",
     "iopub.status.busy": "2022-04-01T12:17:27.080446Z",
     "iopub.status.idle": "2022-04-01T12:17:32.048151Z",
     "shell.execute_reply": "2022-04-01T12:17:32.047406Z",
     "shell.execute_reply.started": "2022-04-01T12:17:27.080646Z"
    },
    "id": "UBlAnAI2KXdt"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:32.051149Z",
     "iopub.status.busy": "2022-04-01T12:17:32.049285Z",
     "iopub.status.idle": "2022-04-01T12:17:32.057277Z",
     "shell.execute_reply": "2022-04-01T12:17:32.056549Z",
     "shell.execute_reply.started": "2022-04-01T12:17:32.051116Z"
    },
    "id": "3atiPh6OXiE6"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:32.058930Z",
     "iopub.status.busy": "2022-04-01T12:17:32.058480Z",
     "iopub.status.idle": "2022-04-01T12:17:32.068693Z",
     "shell.execute_reply": "2022-04-01T12:17:32.067912Z",
     "shell.execute_reply.started": "2022-04-01T12:17:32.058891Z"
    },
    "id": "FVd6baWDXiE8"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:32.071837Z",
     "iopub.status.busy": "2022-04-01T12:17:32.070934Z",
     "iopub.status.idle": "2022-04-01T12:17:32.079309Z",
     "shell.execute_reply": "2022-04-01T12:17:32.078375Z",
     "shell.execute_reply.started": "2022-04-01T12:17:32.071798Z"
    },
    "id": "l7aHrv5eXiE9"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers.modeling_utils import Conv1D, PreTrainedModel, SequenceSummary, prune_conv1d_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:32.081898Z",
     "iopub.status.busy": "2022-04-01T12:17:32.081594Z",
     "iopub.status.idle": "2022-04-01T12:17:32.090333Z",
     "shell.execute_reply": "2022-04-01T12:17:32.089551Z",
     "shell.execute_reply.started": "2022-04-01T12:17:32.081862Z"
    },
    "id": "GwCjkl0fXiE_"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n",
    "        super().__init__()\n",
    "        nx = config.n_embd\n",
    "        self.c_fc = Conv1D(n_state, nx)\n",
    "        self.c_proj = Conv1D(nx, n_state)\n",
    "        self.act = gelu_new\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.act(self.c_fc(x))\n",
    "        h2 = self.c_proj(h)\n",
    "        return self.dropout(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:32.093464Z",
     "iopub.status.busy": "2022-04-01T12:17:32.093284Z",
     "iopub.status.idle": "2022-04-01T12:17:32.130335Z",
     "shell.execute_reply": "2022-04-01T12:17:32.129381Z",
     "shell.execute_reply.started": "2022-04-01T12:17:32.093442Z"
    },
    "id": "D5XO0YXPXiFA"
   },
   "outputs": [],
   "source": [
    "class CoconAttention(nn.Module):\n",
    "    def __init__(self, nx, n_ctx, config, scale=False):\n",
    "        super().__init__()\n",
    "        self.output_attentions = config.output_attentions\n",
    "\n",
    "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
    "        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n",
    "        assert n_state % config.n_head == 0\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "\n",
    "        self_token_mask = torch.ones(n_ctx, n_ctx)\n",
    "        self_token_mask.fill_diagonal_(0)\n",
    "        self.register_buffer(\"self_token_mask\", self_token_mask.view(1, 1, n_ctx, n_ctx))\n",
    "        self.n_head = config.n_head\n",
    "        self.split_size = n_state\n",
    "        self.scale = scale\n",
    "\n",
    "        self.ref_source_attn = Conv1D(n_state * 2, nx)\n",
    "        self.c_attn = Conv1D(n_state * 3, nx) # input has dim of nx\n",
    "        self.c_proj = Conv1D(n_state, nx)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        mask = torch.ones(self.n_head, self.split_size // self.n_head)\n",
    "        heads = set(heads) - self.pruned_heads  # Convert to set and emove already pruned heads\n",
    "        for head in heads:\n",
    "            # Compute how many pruned heads are before the head and move the index accordingly\n",
    "            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\n",
    "            mask[head] = 0\n",
    "        mask = mask.view(-1).contiguous().eq(1)\n",
    "        index = torch.arange(len(mask))[mask].long()\n",
    "        index_attn = torch.cat([index, index + self.split_size, index + (2 * self.split_size)])\n",
    "\n",
    "        # Prune conv1d layers\n",
    "        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n",
    "        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n",
    "\n",
    "        # Update hyper params\n",
    "        self.split_size = (self.split_size // self.n_head) * (self.n_head - len(heads))\n",
    "        self.n_head = self.n_head - len(heads)\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def _attn(self, q, k, v, attention_mask=None, head_mask=None, cs_self_attn_mask_prob=0, history_seq_len=None, context_seq_present=True, context_seq_len=0, context_attn_bias=0, context_seq_len_list=None):\n",
    "        w = torch.matmul(q, k)\n",
    "        if self.scale:\n",
    "            w = w / math.sqrt(v.size(-1))\n",
    "        nd, ns = w.size(-2), w.size(-1)\n",
    "        b = self.bias[:, :, ns - nd : ns, :ns]\n",
    "        w = w * b - 1e4 * (1 - b)\n",
    "\n",
    "        # self_token_mask computation\n",
    "        if cs_self_attn_mask_prob > 0 and context_seq_present:\n",
    "            if history_seq_len == 0:\n",
    "                history_seq_offset = 0\n",
    "            else:\n",
    "                history_seq_offset = history_seq_len - 1\n",
    "            self_token_mask = self.self_token_mask[:, :, :nd, history_seq_offset:history_seq_offset+ns]\n",
    "            self_token_mask = self_token_mask.repeat(w.shape[0],1,1,1)\n",
    "\n",
    "            if cs_self_attn_mask_prob != 1:\n",
    "                # compute unmasked indices\n",
    "                self_token_unmask_prob = 1 - cs_self_attn_mask_prob\n",
    "                unmask_prob_matrix = torch.full(self_token_mask.shape[:-1], self_token_unmask_prob)\n",
    "                unmasked_indices = torch.bernoulli(unmask_prob_matrix).bool()\n",
    "                self_token_mask[unmasked_indices] = 1\n",
    "\n",
    "            w = w * self_token_mask - 1e4 * (1 - self_token_mask)\n",
    "            \n",
    "        \n",
    "        if context_attn_bias != 0:\n",
    "            if context_seq_len_list is None:\n",
    "                context_attn_bias_mask = torch.ones(w.shape) # N, H, Q, V\n",
    "                context_attn_bias_mask[:,:,:, :context_seq_len] = 0\n",
    "                context_attn_bias_mask = context_attn_bias_mask.to(w.device)\n",
    "                w = w + context_attn_bias * (1 - context_attn_bias_mask)     \n",
    "            else:\n",
    "                current_context_start_ind = 0\n",
    "                for cs_ind, current_context_seq_len in enumerate(context_seq_len_list):\n",
    "                    current_context_attn_bias = context_attn_bias[cs_ind]\n",
    "                    context_attn_bias_mask = torch.ones(w.shape)\n",
    "                    context_attn_bias_mask[:,:,:, current_context_start_ind:(current_context_start_ind+current_context_seq_len)] = 0\n",
    "                    context_attn_bias_mask = context_attn_bias_mask.to(w.device)\n",
    "                    w = w + current_context_attn_bias * (1 - context_attn_bias_mask)\n",
    "                    current_context_start_ind = current_context_start_ind + current_context_seq_len\n",
    "\n",
    "            \n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask\n",
    "            w = w + attention_mask\n",
    "\n",
    "        w = nn.Softmax(dim=-1)(w)\n",
    "        w = self.attn_dropout(w)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            w = w * head_mask\n",
    "\n",
    "        outputs = [torch.matmul(w, v)]\n",
    "        if self.output_attentions:\n",
    "            outputs.append(w)\n",
    "        return outputs\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
    "        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n",
    "\n",
    "    def split_heads(self, x, k=False):\n",
    "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
    "        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n",
    "        if k:\n",
    "            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n",
    "        else:\n",
    "            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def forward(self, x, context_seq, layer_past=None, attention_mask=None, head_mask=None, cs_self_attn_mask_prob=0, history_seq_len=None, context_attn_bias=0, context_seq_len_list=None):\n",
    "        x = self.c_attn(x)\n",
    "        query, key, value = x.split(self.split_size, dim=2)\n",
    "\n",
    "        if context_seq is not None:\n",
    "            context_seq_len = context_seq.shape[1]\n",
    "            context_seq = self.ref_source_attn(context_seq)\n",
    "            key_context_seq, value_context_seq = context_seq.split(self.split_size, dim=2)\n",
    "\n",
    "            # Prepend keys and values with context_seq keys and values\n",
    "            prepended_key = torch.cat([key_context_seq, key], dim=1)\n",
    "            prepended_value = torch.cat([value_context_seq, value], dim=1)\n",
    "            context_seq_present = True\n",
    "        else:\n",
    "            context_seq_len = 0\n",
    "            prepended_key = key\n",
    "            prepended_value = value\n",
    "            context_seq_present = False\n",
    "\n",
    "        query = self.split_heads(query)\n",
    "        prepended_key = self.split_heads(prepended_key, k=True)\n",
    "        prepended_value = self.split_heads(prepended_value)\n",
    "\n",
    "        key = self.split_heads(key, k=True)\n",
    "        value = self.split_heads(value)\n",
    "\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below\n",
    "            key = torch.cat((past_key, key), dim=-1)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking\n",
    "        attn_outputs = self._attn(query, prepended_key, prepended_value, attention_mask, head_mask, cs_self_attn_mask_prob=cs_self_attn_mask_prob, history_seq_len=history_seq_len, context_seq_present=context_seq_present, \n",
    "                                    context_seq_len=context_seq_len, context_attn_bias=context_attn_bias, context_seq_len_list=context_seq_len_list)\n",
    "\n",
    "        a = attn_outputs[0]\n",
    "        a = self.merge_heads(a)\n",
    "        a = self.c_proj(a)\n",
    "        a = self.resid_dropout(a)\n",
    "\n",
    "        outputs = [a, present] + attn_outputs\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:32.132257Z",
     "iopub.status.busy": "2022-04-01T12:17:32.131944Z",
     "iopub.status.idle": "2022-04-01T12:17:32.153836Z",
     "shell.execute_reply": "2022-04-01T12:17:32.153123Z",
     "shell.execute_reply.started": "2022-04-01T12:17:32.132190Z"
    },
    "id": "WXKVrEc5XiFD"
   },
   "outputs": [],
   "source": [
    "class CoconBlock(nn.Module):\n",
    "    def __init__(self, n_ctx, config, scale=False):\n",
    "        super().__init__()\n",
    "        logger.info( \"CoconBlock initialized\")\n",
    "        nx = config.n_embd\n",
    "        self.ln_1 = nn.LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        self.sos_h = nn.Parameter(torch.zeros(nx))\n",
    "        self.mask_h = nn.Parameter(torch.zeros(nx))\n",
    "\n",
    "        self.cocon_attn = CoconAttention(nx, n_ctx, config, scale)\n",
    "        self.ln_2 = nn.LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = MLP(4 * nx, config)\n",
    "        self.instance_norm = nn.InstanceNorm1d(nx, affine=False, track_running_stats=False)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x, context_seq=None, history_seq=None, layer_past=None, attention_mask=None, head_mask=None, include_sos_output=False, cs_masked_indices=None, tis_masked_indices=None, cs_self_attn_mask_prob=0, context_attn_bias=0, context_seq_len_list=None):\n",
    "        if cs_masked_indices is not None and context_seq is not None:\n",
    "            context_seq = context_seq.clone() # avoid overwrite original context_seq with mask_h\n",
    "            context_seq[cs_masked_indices] = self.mask_h\n",
    "\n",
    "        if tis_masked_indices is not None and x is not None:\n",
    "            x = x.clone() # avoid overwrite original x with mask_h\n",
    "            x[tis_masked_indices] = self.mask_h\n",
    "\n",
    "        if history_seq is not None:\n",
    "            history_seq_len = history_seq.shape[1]\n",
    "            if x is not None:\n",
    "                cocon_attn_input = torch.cat([history_seq, x], dim=1)\n",
    "            else:\n",
    "                cocon_attn_input = history_seq\n",
    "        elif x is not None:\n",
    "            history_seq_len = 0\n",
    "            batch_size = x.shape[0]\n",
    "            sos_h = self.sos_h.view(1, 1, -1).expand(batch_size, -1, -1)\n",
    "            cocon_attn_input = torch.cat([sos_h, x], dim=1)\n",
    "\n",
    "        x = cocon_attn_input\n",
    "\n",
    "\n",
    "        cocon_attn_input_ln_1 = self.ln_1(cocon_attn_input)\n",
    "        x_1_output = cocon_attn_input_ln_1\n",
    "\n",
    "        output_attn = self.cocon_attn(\n",
    "            x_1_output, context_seq, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask, cs_self_attn_mask_prob=cs_self_attn_mask_prob, history_seq_len=history_seq_len, \n",
    "            context_attn_bias=context_attn_bias, context_seq_len_list=context_seq_len_list\n",
    "        )\n",
    "        a = output_attn[0]  # output_attn: (a), present, (attentions)\n",
    "        # H^L_preconv\n",
    "        x = x + a\n",
    "\n",
    "        # Skip history_seq computation if history_seq_len > 1\n",
    "        if history_seq_len > 1:\n",
    "            x = x[:, history_seq_len-1:]\n",
    "\n",
    "\n",
    "        x_ln_2 = self.ln_2(x)\n",
    "        x_2_output = x_ln_2\n",
    "        m = self.mlp(x_2_output)\n",
    "        # H^L\n",
    "        x = x + m\n",
    "\n",
    "        if include_sos_output:\n",
    "            cocon_output = x\n",
    "        else:\n",
    "            cocon_output = x[:, 1:, :]\n",
    "\n",
    "        return cocon_output\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\" Initialize weights if needed. \"\"\"\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:32.155605Z",
     "iopub.status.busy": "2022-04-01T12:17:32.155292Z",
     "iopub.status.idle": "2022-04-01T12:17:32.869873Z",
     "shell.execute_reply": "2022-04-01T12:17:32.869221Z",
     "shell.execute_reply.started": "2022-04-01T12:17:32.155569Z"
    },
    "id": "qIOMTs7_XiFF"
   },
   "outputs": [],
   "source": [
    "config = GPT2Config.from_pretrained(\"gpt2\", cache_dir='saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:32.872400Z",
     "iopub.status.busy": "2022-04-01T12:17:32.871911Z",
     "iopub.status.idle": "2022-04-01T12:17:33.024158Z",
     "shell.execute_reply": "2022-04-01T12:17:33.023275Z",
     "shell.execute_reply.started": "2022-04-01T12:17:32.872363Z"
    },
    "id": "P0b3pCtCXiFF"
   },
   "outputs": [],
   "source": [
    "cocon_block = CoconBlock(config.n_ctx, config, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Trial naive sentiment\n",
    "cocon_block.load_state_dict(torch.load('imdb_cocon/plain_4.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cocon_block.load_state_dict(torch.load('imdb_cocon/modified_4.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:33.051941Z",
     "iopub.status.busy": "2022-04-01T12:17:33.051724Z",
     "iopub.status.idle": "2022-04-01T12:17:33.068451Z",
     "shell.execute_reply": "2022-04-01T12:17:33.067796Z",
     "shell.execute_reply.started": "2022-04-01T12:17:33.051913Z"
    },
    "id": "9Gd8JKFzKlxW"
   },
   "outputs": [],
   "source": [
    "cocon_block = cocon_block.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofvV--YlBt0X"
   },
   "outputs": [],
   "source": [
    "#cocon_block = torch.load('cocon_block_pytorch_model.bin')#, map_location=‘cpu’) #try gpt medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:35.482974Z",
     "iopub.status.busy": "2022-04-01T12:17:35.482127Z",
     "iopub.status.idle": "2022-04-01T12:17:35.491859Z",
     "shell.execute_reply": "2022-04-01T12:17:35.491036Z",
     "shell.execute_reply.started": "2022-04-01T12:17:35.482923Z"
    },
    "id": "lB4dKFFiXiFG",
    "outputId": "835dd28e-2542-48cb-d639-2120cb3b2c41"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "cocon_block.eval()\n",
    "cocon_block.zero_grad()\n",
    "#cocon_block.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_UolW1HXiFH"
   },
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:17:37.680127Z",
     "iopub.status.busy": "2022-04-01T12:17:37.679342Z",
     "iopub.status.idle": "2022-04-01T12:18:10.007416Z",
     "shell.execute_reply": "2022-04-01T12:18:10.006723Z",
     "shell.execute_reply.started": "2022-04-01T12:17:37.680082Z"
    },
    "id": "iETWemBVXiFK",
    "outputId": "5b819de6-7ce9-4692-8812-58675df2ea88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\aishu\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de312d95c99f448896a5de5dbf7b60bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('imdb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:18:10.009817Z",
     "iopub.status.busy": "2022-04-01T12:18:10.009090Z",
     "iopub.status.idle": "2022-04-01T12:18:10.032319Z",
     "shell.execute_reply": "2022-04-01T12:18:10.031666Z",
     "shell.execute_reply.started": "2022-04-01T12:18:10.009778Z"
    },
    "id": "sCCFYM7yTbIU"
   },
   "outputs": [],
   "source": [
    "#read text file and generate list of words\n",
    "with open('/kaggle/input/sentiment-lexicon/positive-words.txt') as file:\n",
    "    positive_con = [line.rstrip() for line in file if ';' not in line][1:]\n",
    "\n",
    "with open('/kaggle/input/sentiment-lexicon/negative-words.txt') as file:\n",
    "    negative_con = [line.rstrip() for line in file if ';' not in line][1:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:06:54.797629Z",
     "iopub.status.busy": "2022-04-01T12:06:54.797326Z",
     "iopub.status.idle": "2022-04-01T12:06:54.803131Z",
     "shell.execute_reply": "2022-04-01T12:06:54.802360Z",
     "shell.execute_reply.started": "2022-04-01T12:06:54.797591Z"
    },
    "id": "Aig4BbPGt-xM",
    "outputId": "4934309b-16ac-45c5-e1df-d746aab94e75"
   },
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:19:20.612198Z",
     "iopub.status.busy": "2022-04-01T12:19:20.611805Z",
     "iopub.status.idle": "2022-04-01T12:19:20.619674Z",
     "shell.execute_reply": "2022-04-01T12:19:20.618886Z",
     "shell.execute_reply.started": "2022-04-01T12:19:20.612159Z"
    },
    "id": "aIjus08-XiFT"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None, debug=False):\n",
    "    \"\"\" Take integer y (tensor or variable) with n dims and convert it to 1-hot representation with n+1 dims. \"\"\"\n",
    "    y_tensor = y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).reshape(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    y_one_hot = y_one_hot.view(*y.shape, -1)\n",
    "\n",
    "    if debug:\n",
    "        y_compare = torch.argmax(y_one_hot, dim=-1)\n",
    "        logger.info( \"y_compare: {}\".format(y_compare))\n",
    "        logger.info( \"u: {}\".format(y))\n",
    "\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T12:19:20.622064Z",
     "iopub.status.busy": "2022-04-01T12:19:20.621474Z",
     "iopub.status.idle": "2022-04-01T12:19:20.630361Z",
     "shell.execute_reply": "2022-04-01T12:19:20.629614Z",
     "shell.execute_reply.started": "2022-04-01T12:19:20.622005Z"
    },
    "id": "gYuJyq4SXiFU"
   },
   "outputs": [],
   "source": [
    "def collate(examples: List[torch.Tensor]):\n",
    "    text = []\n",
    "    content = []\n",
    "    label = []\n",
    "    for e in examples:\n",
    "        text.append(e[0])\n",
    "        content.append(e[1])\n",
    "        label.append(e[2])\n",
    "    content = pad_sequence(content, batch_first=True)\n",
    "    label = pad_sequence(label, batch_first=True)\n",
    "    if tokenizer._pad_token is None:\n",
    "        text = pad_sequence(text, batch_first=True)\n",
    "    else:\n",
    "        text = pad_sequence(text, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return (text, content, label)\n",
    "  #return (pad_sequence(text, batch_first=True, padding_value=tokenizer.pad_token_id), content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDivUUskNxow"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(accelerator.gather(outputs.loss))\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T15:00:08.406090Z",
     "iopub.status.busy": "2022-04-01T15:00:08.405677Z",
     "iopub.status.idle": "2022-04-01T15:00:08.474636Z",
     "shell.execute_reply": "2022-04-01T15:00:08.473871Z",
     "shell.execute_reply.started": "2022-04-01T15:00:08.406052Z"
    },
    "id": "MzZi3Dg09Ldl"
   },
   "outputs": [],
   "source": [
    "torch.save(cocon_block.state_dict(),'modified_6.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhmRATlNMWhK"
   },
   "outputs": [],
   "source": [
    "model = TheModelClass(*args, **kwargs)\n",
    "optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZVfyHS3XiFb"
   },
   "outputs": [],
   "source": [
    "inp='The sun shines'\n",
    "content = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T13:08:55.827439Z",
     "iopub.status.busy": "2022-04-01T13:08:55.827103Z",
     "iopub.status.idle": "2022-04-01T13:08:55.837178Z",
     "shell.execute_reply": "2022-04-01T13:08:55.836410Z",
     "shell.execute_reply.started": "2022-04-01T13:08:55.827401Z"
    },
    "id": "HkeFWpe3XiFb"
   },
   "outputs": [],
   "source": [
    "def generate(inp,content=None,history=None, gen_len=30):\n",
    "    input_token = torch.tensor(tokenizer.encode(inp))\n",
    "    if(len(input_token.shape)<3):\n",
    "        input_token = input_token.unsqueeze(0) #batch dim\n",
    "    if(content):\n",
    "        content_token = torch.tensor(tokenizer.encode(content))\n",
    "        if(len(content_token.shape)<3):\n",
    "            content_token = content_token.unsqueeze(0)\n",
    "        #content_token = content_token.unsqueeze(0)\n",
    "    #Repeat for history TO DO\n",
    "    #implement auto regression TODO\n",
    "    input_token = input_token.to(device)\n",
    "    l = len(input_token[0])\n",
    "    content_token = content_token.to(device)\n",
    "    for i in range(gen_len):\n",
    "        #L_alpha\n",
    "        hidden_inp = model(input_token,path='half1')\n",
    "        hidden_content = model(content_token, path='half1')\n",
    "        #Cocon             other_context_cocon_hidden_states = cocon_block(cocon_th_gen_output, context_seq=original_context_seq_hidden_states, history_seq=other_sample_history_seq_hidden_states, include_sos_output=True,cs_self_attn_mask_prob=1)\n",
    "        cout = cocon_block(hidden_inp, context_seq=hidden_content)\n",
    "        output = model(cout, path='half2')\n",
    "        pred_token_logits = output[1][:,-1:]\n",
    "        #softmax\n",
    "        pred_token_prob = torch.nn.functional.softmax(pred_token_logits, dim=-1)\n",
    "        #sample\n",
    "        pred_token = torch.multinomial(pred_token_prob[0], num_samples=1) #repeat for every elem in batch\n",
    "        #append\n",
    "        input_token = torch.cat((input_token,pred_token),1)\n",
    "        #decode\n",
    "    #pred_text = tokenizer.decode(input_token)\n",
    "    return input_token, [tokenizer.decode(i) for i in input_token[:,l:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T15:00:08.478195Z",
     "iopub.status.busy": "2022-04-01T15:00:08.477469Z",
     "iopub.status.idle": "2022-04-01T15:00:09.562888Z",
     "shell.execute_reply": "2022-04-01T15:00:09.562173Z",
     "shell.execute_reply.started": "2022-04-01T15:00:08.478154Z"
    },
    "id": "VwTC2r9TXiFc"
   },
   "outputs": [],
   "source": [
    "it, decoded = generate('The sun shines in the',content='positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' proof comic a outtime Sith fro what includes Nero ofzech Bok took old many laganda threat for one mentioned and the AX Nero sub able because movies']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T14:02:23.392996Z",
     "iopub.status.busy": "2022-04-01T14:02:23.392752Z",
     "iopub.status.idle": "2022-04-01T14:02:23.398160Z",
     "shell.execute_reply": "2022-04-01T14:02:23.397219Z",
     "shell.execute_reply.started": "2022-04-01T14:02:23.392961Z"
    },
    "id": "XqCr7e4jXiFd",
    "outputId": "1a5de55c-03aa-4420-fdc9-faaabd94d663"
   },
   "outputs": [],
   "source": [
    "it.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T15:00:09.564451Z",
     "iopub.status.busy": "2022-04-01T15:00:09.564201Z",
     "iopub.status.idle": "2022-04-01T15:00:09.570122Z",
     "shell.execute_reply": "2022-04-01T15:00:09.569479Z",
     "shell.execute_reply.started": "2022-04-01T15:00:09.564418Z"
    },
    "id": "6sByAgLn__pr",
    "outputId": "0e6cbea5-afd5-4ec9-9e3d-6d039dd59f43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The sun shines in the Lawrenceivan came very before list and we season droppedTrYou hit president believe directorEnjoy observers kill was I everyone I hand the biears Everything guys leaveasted of simply willor though tiny big it thorionart rockediii Police. god a Nekingers throughout ablek thataris again hit mod! funny istt a were I u loved\" members of The art or iterations is they noticed one the<|endoftext|> Away everything for R special before Im episodes can good LordH series take prettydissin who Studios']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded #observe model learnt to used words related to movies :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T15:15:50.931951Z",
     "iopub.status.busy": "2022-04-01T15:15:50.931526Z",
     "iopub.status.idle": "2022-04-01T15:15:52.004372Z",
     "shell.execute_reply": "2022-04-01T15:15:52.003645Z",
     "shell.execute_reply.started": "2022-04-01T15:15:50.931915Z"
    }
   },
   "outputs": [],
   "source": [
    "it, decoded = generate('The sun shines in the',content='excellent perfect good lovely')\n",
    "decoded #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T14:11:51.362800Z",
     "iopub.status.busy": "2022-04-01T14:11:51.362092Z",
     "iopub.status.idle": "2022-04-01T14:11:52.482560Z",
     "shell.execute_reply": "2022-04-01T14:11:52.481832Z",
     "shell.execute_reply.started": "2022-04-01T14:11:51.362756Z"
    }
   },
   "outputs": [],
   "source": [
    "it, decoded = generate('The sun shines in the',content='excellent perfect good lovely')\n",
    "decoded #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T13:14:29.796784Z",
     "iopub.status.busy": "2022-04-01T13:14:29.796313Z",
     "iopub.status.idle": "2022-04-01T13:14:30.874389Z",
     "shell.execute_reply": "2022-04-01T13:14:30.873443Z",
     "shell.execute_reply.started": "2022-04-01T13:14:29.796744Z"
    },
    "id": "p_cLIiLYXiFd",
    "outputId": "961dc445-b569-4ee2-e255-33952ade1727"
   },
   "outputs": [],
   "source": [
    "it, decoded = generate('The sun shines in the',content='excellent perfect good lovely')\n",
    "decoded #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sa4xeMhHtmh0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(accelerator.gather(outputs.loss))\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T15:22:40.346654Z",
     "iopub.status.busy": "2022-04-01T15:22:40.346324Z",
     "iopub.status.idle": "2022-04-01T15:22:40.368803Z",
     "shell.execute_reply": "2022-04-01T15:22:40.368077Z",
     "shell.execute_reply.started": "2022-04-01T15:22:40.346617Z"
    }
   },
   "outputs": [],
   "source": [
    "class IMDBDatasettest(Dataset):\n",
    "    def __init__(self, tokenizer: tokenizer, dataset=datasets['train'], \n",
    "                 cs_len=20, hs_len=10, tis_len=20, block_size=tokenizer.model_max_length, text_json_key=\"text\", \n",
    "                 evaluate=False, prepended_text_to_remove=None):#, positive_con=positive_con, negative_con=negative_con):\n",
    "\n",
    "        self.cs_len = cs_len\n",
    "        self.hs_len = hs_len\n",
    "        self.tis_len = tis_len\n",
    "\n",
    "        if block_size is None:\n",
    "            block_size = hs_len + max(cs_len, tis_len)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        if evaluate and text_json_key != 'text':\n",
    "            cached_features_file = os.path.join(\n",
    "                'temp_data', \"gpt2\" + \"_cached_cocon_\" + str(block_size) + text_json_key + \"_\" + 'imdb'\n",
    "            )\n",
    "        else:\n",
    "            cached_features_file = os.path.join(\n",
    "                'temp_data',\"gpt2\" + \"_cached_cocon_test\" + str(block_size) + \"_\" + 'imdb'\n",
    "            )\n",
    "            cached_label_file = os.path.join(\n",
    "                'temp_data',\"gpt2\" + \"_cached_cocon_test\" + str(block_size) + \"_\" + 'imdb_senti_naive'\n",
    "            )\n",
    "\n",
    "        if os.path.exists(cached_features_file):# and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            lines = dataset['text']\n",
    "            logger.info(\"Creating features from dataset file at %s\", 'temp_data')\n",
    "            prepended_texts = None\n",
    "            logger.info(\"Encoding with tokenizer\")\n",
    "            self.examples = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=None)[\"input_ids\"]\n",
    "            \n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        if os.path.exists(cached_label_file):# and not args.overwrite_cache:\n",
    "            logger.info(\"Loading labels from cached file %s\", cached_label_file)\n",
    "            with open(cached_label_file, \"rb\") as handle:\n",
    "                self.labels = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating labels from dataset file at %s\", 'temp_data')\n",
    "            prepended_texts = None\n",
    "\n",
    "            labels = dataset['label']\n",
    "            content = []\n",
    "            for i in labels:\n",
    "                if(i==0):\n",
    "                    neg_content = random.sample(negative_con, 10)\n",
    "                else:\n",
    "                    neg_content = random.sample(positive_con,10)\n",
    "                neg_content = ' '.join(neg_content) \n",
    "                content.append(neg_content)\n",
    "            \n",
    "\n",
    "            logger.info(\"Encoding with tokenizer\")\n",
    "            self.labels = tokenizer.batch_encode_plus(content, add_special_tokens=True, max_length=10, truncation=True)[\"input_ids\"]\n",
    "\n",
    "            logger.info(\"Saving labels into cached file %s\", cached_label_file)\n",
    "            with open(cached_label_file, \"wb\") as handle:\n",
    "                pickle.dump(self.labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "          \n",
    "        pos  =tokenizer.encode('positive')\n",
    "        neg = tokenizer.encode('negative')\n",
    "        sent = {0:neg, 1:pos}\n",
    "        self.targets = [sent[i] for i in dataset['label']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        example = self.examples[item]\n",
    "        labels = self.labels[item]\n",
    "        targets = self.targets[item]\n",
    "        overflow_len = len(example) - self.block_size\n",
    "        if overflow_len > 0:\n",
    "            random_ind = random.randint(0, overflow_len) # random integer between 0 and overflow_len (both inclusive)\n",
    "        else:\n",
    "            random_ind = 0\n",
    "        example_block = example[random_ind:random_ind+self.block_size]\n",
    "        \"\"\"\n",
    "\n",
    "        overflow_len = len(labels) - 10#self.block_size\n",
    "        if overflow_len > 0:\n",
    "            random_ind = random.randint(0, overflow_len) # random integer between 0 and overflow_len (both inclusive)\n",
    "        else:\n",
    "            random_ind = 0\n",
    "        content_block = labels[random_ind:random_ind+10]\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.tensor(example_block, dtype=torch.long), torch.tensor(labels, dtype=torch.long), torch.tensor(targets, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T15:22:52.887968Z",
     "iopub.status.busy": "2022-04-01T15:22:52.887711Z",
     "iopub.status.idle": "2022-04-01T15:23:59.583786Z",
     "shell.execute_reply": "2022-04-01T15:23:59.583054Z",
     "shell.execute_reply.started": "2022-04-01T15:22:52.887939Z"
    },
    "id": "vTAAPsUpXiFe"
   },
   "outputs": [],
   "source": [
    "test_dataset = IMDBDatasettest(tokenizer, dataset=datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T15:32:43.392573Z",
     "iopub.status.busy": "2022-04-01T15:32:43.391904Z",
     "iopub.status.idle": "2022-04-01T15:32:43.397451Z",
     "shell.execute_reply": "2022-04-01T15:32:43.396750Z",
     "shell.execute_reply.started": "2022-04-01T15:32:43.392532Z"
    }
   },
   "outputs": [],
   "source": [
    "test_batch_size = 1 #memory error for 32\n",
    "test_sampler = RandomSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler,batch_size=test_batch_size, collate_fn=collate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T15:16:45.236903Z",
     "iopub.status.busy": "2022-04-01T15:16:45.236643Z",
     "iopub.status.idle": "2022-04-01T15:16:45.242094Z",
     "shell.execute_reply": "2022-04-01T15:16:45.241394Z",
     "shell.execute_reply.started": "2022-04-01T15:16:45.236874Z"
    }
   },
   "outputs": [],
   "source": [
    "config.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T15:17:37.139333Z",
     "iopub.status.busy": "2022-04-01T15:17:37.138678Z",
     "iopub.status.idle": "2022-04-01T15:17:37.161525Z",
     "shell.execute_reply": "2022-04-01T15:17:37.160592Z",
     "shell.execute_reply.started": "2022-04-01T15:17:37.139292Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.special_tokens[\"<END>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T15:57:10.778658Z",
     "iopub.status.busy": "2022-04-01T15:57:10.778403Z",
     "iopub.status.idle": "2022-04-01T15:57:10.797954Z",
     "shell.execute_reply": "2022-04-01T15:57:10.797258Z",
     "shell.execute_reply.started": "2022-04-01T15:57:10.778629Z"
    }
   },
   "outputs": [],
   "source": [
    "def beam_decoder(test_dataloader = test_dataloader, beam_width=3, gen_length=20):\n",
    "    epoch_iterator = tqdm(test_dataloader, desc=\"Iteration\")\n",
    "    vocab_len = config.vocab_size\n",
    "    targets=[]\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        if(step==5):\n",
    "            break\n",
    "        inputs, content, target = batch\n",
    "    #lm_labels = inputs\n",
    "        #inputs = inputs\n",
    "    #lm_labels = lm_labels.to(device)\n",
    "        #content = content.to(device)\n",
    "        #target = target\n",
    "    \n",
    "        done = [False for i in range(beam_width)]\n",
    "        stop_decode = False\n",
    "        decoded_sentences = []\n",
    "        targets.append(target[0]*beam_width)\n",
    "        \n",
    "        sm = torch.nn.Softmax(dim=-1)\n",
    "        #replicate input for all beams\n",
    "        #print(inputs)\n",
    "        index_tokens = torch.tensor([inputs for i in range(beam_width)])\n",
    "        content_tokens = torch.tensor([content for i in range(beam_width)])\n",
    "        #indexed_tokens = torch.tensor([inputs.numpy()])\n",
    "        #indexd_tokens = torch.tensor(index_tokens).squeeze(1).to(device)\n",
    "        #content_tokens = torch.tensor(content_tokens).squeeze(1).to(device)\n",
    "        #print(indexd_tokens.shape)\n",
    "        beam_indexes = [[] for i in range(beam_width)]\n",
    "        print(beam_indexes)\n",
    "        best_scores = [0 for i in range(beam_width)]\n",
    "        count=0\n",
    "    \n",
    "        #for i in range(gen_len):\n",
    "        while(count<gen_length and not stop_decode):\n",
    "            with torch.no_grad():\n",
    "            #L_alpha\n",
    "                hidden_inp = model(indexd_tokens,path='half1')\n",
    "                hidden_content = model(content_tokens, path='half1')\n",
    "                #Cocon             other_context_cocon_hidden_states = cocon_block(cocon_th_gen_output, context_seq=original_context_seq_hidden_states, history_seq=other_sample_history_seq_hidden_states, include_sos_output=True,cs_self_attn_mask_prob=1)\n",
    "                #print(hidden_inp.shape, hidden_content.shape)\n",
    "                cout = cocon_block(hidden_inp, context_seq=hidden_content)\n",
    "                output = model(cout, path='half2')\n",
    "                pred_token_logits = output[1][:,-1:]\n",
    "                #softmax\n",
    "                pred_token_prob = torch.nn.functional.softmax(pred_token_logits, dim=-1)\n",
    "                #sample\n",
    "                #print(pred_token_prob.shape)\n",
    "                #pred_token = torch.multinomial(pred_token_prob[0], num_samples=1) #repeat for every elem in batch\n",
    "            if(count==0):\n",
    "                top_v, top_i = pred_token_prob[:,-1,:].topk(beam_width)\n",
    "                [beam_indexes[i].append(top_i[0][i].tolist()) for i in range(beam_width)]\n",
    "                for i in range(beam_width):\n",
    "                    best_scores[i] = top_v[0][i].item()\n",
    "                count += 1\n",
    "            else:\n",
    "                flatten_score = (pred_token_prob[:,-1,:]*torch.tensor(best_scores).to(device).unsqueeze(1)).view(-1) #beam width*vocab_size\n",
    "                vals, inx = flatten_score.topk(beam_width)\n",
    "                best_scores_inx = (inx/vocab_len).tolist()\n",
    "                best_scores = vals.tolist()\n",
    "                correct_inx = (inx%vocab_len).tolist()\n",
    "                #update\n",
    "                temp_lt = [0 for i in range(beam_width)]\n",
    "                for i,x in enumerate(best_scores_inx):\n",
    "                    temp_lt[i] = beam_indexes[i] + [correct_inx[i]]\n",
    "                beam_indexes = temp_lt\n",
    "                del temp_lt\n",
    "                count += 1\n",
    "            #for i in range(beam_width):\n",
    "            #    if correct_inx[i] == tokenizer.special_tokens['<END>']:\n",
    "            #        done[i] = True\n",
    "                for i in range(beam_width):\n",
    "                    if not done[i]:\n",
    "                        best_scores[i] = vals.tolist()[i]\n",
    "                        \n",
    "            #if(sum(done)==beam_width):\n",
    "            #    stop_decode=True\n",
    "                        \n",
    "            indexd_tokens = torch.cat((indexd_tokens,torch.tensor(beam_indexes).to(device)),1)\n",
    "                    \n",
    "        for i in range(beam_width):\n",
    "            #try:\n",
    "            #    end_index = beam_index[i].index(tokenizer.special_tokens[\"<END>\"])\n",
    "            #except ValueError:\n",
    "            end_index = len(beam_indexes[i])\n",
    "            print(tokenizer.decode(beam_indexes[i][:end_index]))\n",
    "            decoded_sentences.append(tokenizer.decode(beam_indexes[i][:end_index]))\n",
    "        #append\n",
    "        \n",
    "        #decode\n",
    "    #pred_text = tokenizer.decode(input_token)\n",
    "    return decoded_sentences,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-04-01T15:57:11.622824Z",
     "iopub.status.busy": "2022-04-01T15:57:11.622330Z",
     "iopub.status.idle": "2022-04-01T15:57:20.379403Z",
     "shell.execute_reply": "2022-04-01T15:57:20.378687Z",
     "shell.execute_reply.started": "2022-04-01T15:57:11.622785Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|                                                                             | 0/25000 [00:00<?, ?it/s]c:\\users\\aishu\\miniconda\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Iteration:   0%|                                                                 | 1/25000 [00:57<401:38:29, 57.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I. a a I. I I I I I.. a I. a a a a\n",
      " the a the. I., you you you, the I of the a. the with.\n",
      " of the a film film a the I I I you film, I with with I I a the\n",
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|                                                                | 1/25000 [01:58<823:25:21, 118.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-1fc7ce7d35bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeam_decoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-7e92f4cc8b17>\u001b[0m in \u001b[0;36mbeam_decoder\u001b[1;34m(test_dataloader, beam_width, gen_length)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;31m#L_alpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                 \u001b[0mhidden_inp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexd_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'half1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                 \u001b[0mhidden_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'half1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[1;31m#Cocon             other_context_cocon_hidden_states = cocon_block(cocon_th_gen_output, context_seq=original_context_seq_hidden_states, history_seq=other_sample_history_seq_hidden_states, include_sos_output=True,cs_self_attn_mask_prob=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-92f123620a84>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, labels, path, lm_logit_first_index, lm_logit_last_index, lm_labels_first_index, lm_labels_last_index, inputs_embeds)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_half2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'half1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_half1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'half2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             x = self.forward_half2(x,labels,lm_logit_first_index=0,lm_logit_last_index=-1,\n",
      "\u001b[1;32m<ipython-input-4-92f123620a84>\u001b[0m in \u001b[0;36mforward_half1\u001b[1;34m(self, src, labels, pos_ids, inputs_embeds)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;31m#inp = self.drop((self.wte(src)+self.wpe(pos_ids)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-92f123620a84>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-92f123620a84>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-92f123620a84>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msize_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d = beam_decoder(test_dataloader, beam_width=3, gen_length=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-01T15:56:49.163131Z",
     "iopub.status.busy": "2022-04-01T15:56:49.162833Z",
     "iopub.status.idle": "2022-04-01T15:56:49.168440Z",
     "shell.execute_reply": "2022-04-01T15:56:49.167771Z",
     "shell.execute_reply.started": "2022-04-01T15:56:49.163101Z"
    }
   },
   "outputs": [],
   "source": [
    "len(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_cocon_test_senti = pd.DataFrame()\n",
    "df_cocon_test_senti['decoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wow! This movie is almost too bad for words. Obviously the writers wanted to somehow link this to the Ghoulies franchise, so they got Pete Liapis from the first one to reprise his role as Jonathan...only now, he's a cop and has no similar character traits as he did in the first one. The ghoulies in this one aren't the ghoulies from the last ones. The cheap looking puppets have been replaced with even cheaper looking costumed little people. Instead of being any main antagonist or being evil, they are more like the comic relief characters that appeared out of nowhere for no reason.<br /><br />When watching this film for the first time, it felt like I'd seen it before. Why was this? Because everything in this was stolen from another movie. All the cheesy cop lines and action scenes were from Lethal Weapon. The ghoulies were pretty much like Bugs Bunny and Daffy Duck, except they weren't amusing at all. Even scenes from the original Ghoulies film were sprinkled throughout this flick.<br /><br />I think the target audience was supposed to be adults, but the mixture of black magic, cartoon slapstick, cop drama and bad acting doesn't work at all. I hope they don't make a Ghoulies V, because I don't want a movie studio to lose their money.<br /><br />My rating: BOMB/****. 78 mins. R for violence.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I thought that this movie was going to be totally lame based on the advertisements that I saw in theaters\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(epoch_iterator):\n",
    "    inputs, content, target = batch\n",
    "    print(tokenizer.decode(inputs[0,:20]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|▎                                                              | 100/25000 [08:03<33:24:32,  4.83s/it]\n"
     ]
    }
   ],
   "source": [
    "#COCON naive sentiment\n",
    "epoch_iterator = tqdm(test_dataloader, desc=\"Evaluating\")\n",
    "test_inputs = []\n",
    "test_outputs = []\n",
    "test_targets = []\n",
    "test_preds = []\n",
    "test_content = []\n",
    "\n",
    "for step, batch in enumerate(epoch_iterator):\n",
    "    if(step==100):\n",
    "        break\n",
    "    inputs, content, target = batch\n",
    "    #print(inputs.shape, content.shape, target.shape)\n",
    "    test_inputs.append(tokenizer.decode(inputs[0,:30]))#.numpy())\n",
    "    test_outputs.append(tokenizer.decode(inputs[0,30:60]))\n",
    "    test_content.append(tokenizer.decode(content[0]))#.numpy())\n",
    "\n",
    "    test_targets.append(tokenizer.decode(target[0]))\n",
    "    tokens, decoded = generate2(inputs[:,:30], content)\n",
    "    #print(tokenizer.decode(inputs[0][:20]))\n",
    "    #print('---------------------------------')\n",
    "    #print(decoded[0])\n",
    "    test_preds.append(decoded[0])\n",
    "    #pass tokens to sentiment classifier and calculate accuracy by comparing to targets\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "df['input_text'] = test_inputs\n",
    "df['target_labels'] = test_targets\n",
    "df['cocon_plain'] = test_preds\n",
    "df['content'] = test_content\n",
    "df['real_output'] = test_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_labels</th>\n",
       "      <th>cocon_plain</th>\n",
       "      <th>content</th>\n",
       "      <th>real_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forget all those sappy romantic movies involvi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happen not when is many but Juliusts extremel...</td>\n",
       "      <td>flatteringly intelligible peerless indebted la...</td>\n",
       "      <td>over-simplified unrealistic romance. Forget a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I watched the movie about 13 yrs ago while liv...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a. \" When wasS I this Film w very's of, encou...</td>\n",
       "      <td>multi-purpose examplar cleaner unassailable</td>\n",
       "      <td>in the back that most don't bother to browse....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie is on the level with \"Welcome Home ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>one see only crown j less v rumors Civil the ...</td>\n",
       "      <td>gaff destains defamation detested forbidden gr...</td>\n",
       "      <td>guys weren't Adam Sandler's gay friends, this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If there were two parts that the physically to...</td>\n",
       "      <td>positive</td>\n",
       "      <td>my. pretty head of from are the was killings ...</td>\n",
       "      <td>pleasurably entrust liked deft improvements entr</td>\n",
       "      <td>play, it must surely have been Cyrano de Berg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Like many a child born in the 1980's, I grew u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>buff very. in Iattery lovevisible system loop...</td>\n",
       "      <td>conciliate pleasurably intelligent astonishing...</td>\n",
       "      <td>Saddles and History of the World part 1 (I sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text target_labels  \\\n",
       "0  Forget all those sappy romantic movies involvi...      positive   \n",
       "1  I watched the movie about 13 yrs ago while liv...      positive   \n",
       "2  This movie is on the level with \"Welcome Home ...      negative   \n",
       "3  If there were two parts that the physically to...      positive   \n",
       "4  Like many a child born in the 1980's, I grew u...      positive   \n",
       "\n",
       "                                         cocon_plain  \\\n",
       "0   happen not when is many but Juliusts extremel...   \n",
       "1   a. \" When wasS I this Film w very's of, encou...   \n",
       "2   one see only crown j less v rumors Civil the ...   \n",
       "3   my. pretty head of from are the was killings ...   \n",
       "4   buff very. in Iattery lovevisible system loop...   \n",
       "\n",
       "                                             content  \\\n",
       "0  flatteringly intelligible peerless indebted la...   \n",
       "1        multi-purpose examplar cleaner unassailable   \n",
       "2  gaff destains defamation detested forbidden gr...   \n",
       "3   pleasurably entrust liked deft improvements entr   \n",
       "4  conciliate pleasurably intelligent astonishing...   \n",
       "\n",
       "                                         real_output  \n",
       "0   over-simplified unrealistic romance. Forget a...  \n",
       "1   in the back that most don't bother to browse....  \n",
       "2   guys weren't Adam Sandler's gay friends, this...  \n",
       "3   play, it must surely have been Cyrano de Berg...  \n",
       "4   Saddles and History of the World part 1 (I sa...  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cocon mod\n",
    "cocon_block = CoconBlock(config.n_ctx, config, scale=True)\n",
    "cocon_block.load_state_dict(torch.load('imdb_cocon/modified_4.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, man, they sure knew how to make them back then. Hollywood has forgotten the basic ingredients'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['input_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.reset_index()  # make sure indexes pair with number of rows\n",
    "test_plain_outputs = []\n",
    "for index, row in df.iterrows():\n",
    "    inp = df['input_text'][index]\n",
    "    content = df['content'][index]\n",
    "    it, decoded = generate(inp,content=content)\n",
    "    test_plain_outputs.append(decoded[0])\n",
    "    #print(test_plain_outputs)\n",
    "    \n",
    "    \n",
    "df['cocon_senti_naive'] = test_plain_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_labels</th>\n",
       "      <th>cocon_plain</th>\n",
       "      <th>content</th>\n",
       "      <th>real_output</th>\n",
       "      <th>cocon_senti_naive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forget all those sappy romantic movies involvi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happen not when is many but Juliusts extremel...</td>\n",
       "      <td>flatteringly intelligible peerless indebted la...</td>\n",
       "      <td>over-simplified unrealistic romance. Forget a...</td>\n",
       "      <td>you made 2009ames a you. host wouldone slight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I watched the movie about 13 yrs ago while liv...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a. \" When wasS I this Film w very's of, encou...</td>\n",
       "      <td>multi-purpose examplar cleaner unassailable</td>\n",
       "      <td>in the back that most don't bother to browse....</td>\n",
       "      <td>reference aiced sod placesor placezit he conc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie is on the level with \"Welcome Home ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>one see only crown j less v rumors Civil the ...</td>\n",
       "      <td>gaff destains defamation detested forbidden gr...</td>\n",
       "      <td>guys weren't Adam Sandler's gay friends, this...</td>\n",
       "      <td>it like bears down. This thoughtii you seems ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If there were two parts that the physically to...</td>\n",
       "      <td>positive</td>\n",
       "      <td>my. pretty head of from are the was killings ...</td>\n",
       "      <td>pleasurably entrust liked deft improvements entr</td>\n",
       "      <td>play, it must surely have been Cyrano de Berg...</td>\n",
       "      <td>9 iswas withR flashed me point that worstS end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Like many a child born in the 1980's, I grew u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>buff very. in Iattery lovevisible system loop...</td>\n",
       "      <td>conciliate pleasurably intelligent astonishing...</td>\n",
       "      <td>Saddles and History of the World part 1 (I sa...</td>\n",
       "      <td>,ab, originally I unknown.inr first we fascina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text target_labels  \\\n",
       "0  Forget all those sappy romantic movies involvi...      positive   \n",
       "1  I watched the movie about 13 yrs ago while liv...      positive   \n",
       "2  This movie is on the level with \"Welcome Home ...      negative   \n",
       "3  If there were two parts that the physically to...      positive   \n",
       "4  Like many a child born in the 1980's, I grew u...      positive   \n",
       "\n",
       "                                         cocon_plain  \\\n",
       "0   happen not when is many but Juliusts extremel...   \n",
       "1   a. \" When wasS I this Film w very's of, encou...   \n",
       "2   one see only crown j less v rumors Civil the ...   \n",
       "3   my. pretty head of from are the was killings ...   \n",
       "4   buff very. in Iattery lovevisible system loop...   \n",
       "\n",
       "                                             content  \\\n",
       "0  flatteringly intelligible peerless indebted la...   \n",
       "1        multi-purpose examplar cleaner unassailable   \n",
       "2  gaff destains defamation detested forbidden gr...   \n",
       "3   pleasurably entrust liked deft improvements entr   \n",
       "4  conciliate pleasurably intelligent astonishing...   \n",
       "\n",
       "                                         real_output  \\\n",
       "0   over-simplified unrealistic romance. Forget a...   \n",
       "1   in the back that most don't bother to browse....   \n",
       "2   guys weren't Adam Sandler's gay friends, this...   \n",
       "3   play, it must surely have been Cyrano de Berg...   \n",
       "4   Saddles and History of the World part 1 (I sa...   \n",
       "\n",
       "                                   cocon_senti_naive  \n",
       "0   you made 2009ames a you. host wouldone slight...  \n",
       "1   reference aiced sod placesor placezit he conc...  \n",
       "2   it like bears down. This thoughtii you seems ...  \n",
       "3  9 iswas withR flashed me point that worstS end...  \n",
       "4  ,ab, originally I unknown.inr first we fascina...  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gpt_outputs = []\n",
    "for index, row in df.iterrows():\n",
    "    inp = df['input_text'][index]\n",
    "    #content = df['content'][index]\n",
    "    it, decoded = generate_gpt(inp)\n",
    "    test_gpt_outputs.append(decoded[0])\n",
    "    #print(test_gpt_outputs)\n",
    "    \n",
    "    \n",
    "    \n",
    "df['gpt'] = test_gpt_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_labels</th>\n",
       "      <th>cocon_plain</th>\n",
       "      <th>content</th>\n",
       "      <th>real_output</th>\n",
       "      <th>cocon_senti_naive</th>\n",
       "      <th>gpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forget all those sappy romantic movies involvi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happen not when is many but Juliusts extremel...</td>\n",
       "      <td>flatteringly intelligible peerless indebted la...</td>\n",
       "      <td>over-simplified unrealistic romance. Forget a...</td>\n",
       "      <td>you made 2009ames a you. host wouldone slight...</td>\n",
       "      <td>how low low of me. bodyguard whatever project...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I watched the movie about 13 yrs ago while liv...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a. \" When wasS I this Film w very's of, encou...</td>\n",
       "      <td>multi-purpose examplar cleaner unassailable</td>\n",
       "      <td>in the back that most don't bother to browse....</td>\n",
       "      <td>reference aiced sod placesor placezit he conc...</td>\n",
       "      <td>shelves. unbelievable? Survival vS forgot.Hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie is on the level with \"Welcome Home ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>one see only crown j less v rumors Civil the ...</td>\n",
       "      <td>gaff destains defamation detested forbidden gr...</td>\n",
       "      <td>guys weren't Adam Sandler's gay friends, this...</td>\n",
       "      <td>it like bears down. This thoughtii you seems ...</td>\n",
       "      <td>members of Andy.\" Picture that chrome Her一 fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If there were two parts that the physically to...</td>\n",
       "      <td>positive</td>\n",
       "      <td>my. pretty head of from are the was killings ...</td>\n",
       "      <td>pleasurably entrust liked deft improvements entr</td>\n",
       "      <td>play, it must surely have been Cyrano de Berg...</td>\n",
       "      <td>9 iswas withR flashed me point that worstS end...</td>\n",
       "      <td>Usire a celebrated that in less These everybo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Like many a child born in the 1980's, I grew u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>buff very. in Iattery lovevisible system loop...</td>\n",
       "      <td>conciliate pleasurably intelligent astonishing...</td>\n",
       "      <td>Saddles and History of the World part 1 (I sa...</td>\n",
       "      <td>,ab, originally I unknown.inr first we fascina...</td>\n",
       "      <td>cADLY on a comingcitizens a the L's on you are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text target_labels  \\\n",
       "0  Forget all those sappy romantic movies involvi...      positive   \n",
       "1  I watched the movie about 13 yrs ago while liv...      positive   \n",
       "2  This movie is on the level with \"Welcome Home ...      negative   \n",
       "3  If there were two parts that the physically to...      positive   \n",
       "4  Like many a child born in the 1980's, I grew u...      positive   \n",
       "\n",
       "                                         cocon_plain  \\\n",
       "0   happen not when is many but Juliusts extremel...   \n",
       "1   a. \" When wasS I this Film w very's of, encou...   \n",
       "2   one see only crown j less v rumors Civil the ...   \n",
       "3   my. pretty head of from are the was killings ...   \n",
       "4   buff very. in Iattery lovevisible system loop...   \n",
       "\n",
       "                                             content  \\\n",
       "0  flatteringly intelligible peerless indebted la...   \n",
       "1        multi-purpose examplar cleaner unassailable   \n",
       "2  gaff destains defamation detested forbidden gr...   \n",
       "3   pleasurably entrust liked deft improvements entr   \n",
       "4  conciliate pleasurably intelligent astonishing...   \n",
       "\n",
       "                                         real_output  \\\n",
       "0   over-simplified unrealistic romance. Forget a...   \n",
       "1   in the back that most don't bother to browse....   \n",
       "2   guys weren't Adam Sandler's gay friends, this...   \n",
       "3   play, it must surely have been Cyrano de Berg...   \n",
       "4   Saddles and History of the World part 1 (I sa...   \n",
       "\n",
       "                                   cocon_senti_naive  \\\n",
       "0   you made 2009ames a you. host wouldone slight...   \n",
       "1   reference aiced sod placesor placezit he conc...   \n",
       "2   it like bears down. This thoughtii you seems ...   \n",
       "3  9 iswas withR flashed me point that worstS end...   \n",
       "4  ,ab, originally I unknown.inr first we fascina...   \n",
       "\n",
       "                                                 gpt  \n",
       "0   how low low of me. bodyguard whatever project...  \n",
       "1   shelves. unbelievable? Survival vS forgot.Hid...  \n",
       "2   members of Andy.\" Picture that chrome Her一 fo...  \n",
       "3   Usire a celebrated that in less These everybo...  \n",
       "4  cADLY on a comingcitizens a the L's on you are...  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998656511306763},\n",
       " {'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive', 'negative']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i['label'].lower() for i in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.999]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[round(i['score'],3) for i in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oh, man, they sure knew how to make them back then. Hollywood has forgotten the basic ingredients',\n",
       " 'I got stuck in traffic (I live in Sicily) on the way to the theater (at a',\n",
       " 'I enjoyed this film very much. I found it to be very entertaining for me in that I feel']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df['input_text'])[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_labels</th>\n",
       "      <th>cocon_senti_naive</th>\n",
       "      <th>content</th>\n",
       "      <th>cocon_plain</th>\n",
       "      <th>gpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh, man, they sure knew how to make them back ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>,was I fromIi Dempl wayino is posedorious SA a...</td>\n",
       "      <td>condescend bewitch hiliarious absence culp</td>\n",
       "      <td>in very acquaintancen because to scene saidPi...</td>\n",
       "      <td>? The here he knows me without B- here.  I loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I got stuck in traffic (I live in Sicily) on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Read for outfit,Charlie no Has complete totall...</td>\n",
       "      <td>addicts stark problems awkwardness dragging di...</td>\n",
       "      <td>ision most from one shot graduating Unicode so...</td>\n",
       "      <td>) near Mittlebusters is theory crafted III. (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I enjoyed this film very much. I found it to b...</td>\n",
       "      <td>positive</td>\n",
       "      <td>H good atold before theis It begins in enjoy ...</td>\n",
       "      <td>resilient congratulatory pamper swift magnificent</td>\n",
       "      <td>she set pervasive applause http experience.30...</td>\n",
       "      <td>aged AWE.com Click enjoyed much me, &amp; It. She...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was in this movie as an extra in the Dallas ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>would H we I possible B taken of In announced...</td>\n",
       "      <td>pitifully devilment discouraging inaccuracy ch...</td>\n",
       "      <td>year thats of life probablyyg retro unnamed J...</td>\n",
       "      <td>object.ies in: HOU my the as 2014 Dec it. Dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been a devoted IMDB visitor for a few yea...</td>\n",
       "      <td>positive</td>\n",
       "      <td>around,Fable you can I nothing whyAgain Poste...</td>\n",
       "      <td>plush excel smile top-notch aspire state</td>\n",
       "      <td>seemed so really W I scared by, a Mumbai The ...</td>\n",
       "      <td>creator anyway a government\" \" db 2 grim digi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text target_labels  \\\n",
       "0  Oh, man, they sure knew how to make them back ...      negative   \n",
       "1  I got stuck in traffic (I live in Sicily) on t...      negative   \n",
       "2  I enjoyed this film very much. I found it to b...      positive   \n",
       "3  I was in this movie as an extra in the Dallas ...      negative   \n",
       "4  I've been a devoted IMDB visitor for a few yea...      positive   \n",
       "\n",
       "                                   cocon_senti_naive  \\\n",
       "0  ,was I fromIi Dempl wayino is posedorious SA a...   \n",
       "1  Read for outfit,Charlie no Has complete totall...   \n",
       "2   H good atold before theis It begins in enjoy ...   \n",
       "3   would H we I possible B taken of In announced...   \n",
       "4   around,Fable you can I nothing whyAgain Poste...   \n",
       "\n",
       "                                             content  \\\n",
       "0         condescend bewitch hiliarious absence culp   \n",
       "1  addicts stark problems awkwardness dragging di...   \n",
       "2  resilient congratulatory pamper swift magnificent   \n",
       "3  pitifully devilment discouraging inaccuracy ch...   \n",
       "4           plush excel smile top-notch aspire state   \n",
       "\n",
       "                                         cocon_plain  \\\n",
       "0   in very acquaintancen because to scene saidPi...   \n",
       "1  ision most from one shot graduating Unicode so...   \n",
       "2   she set pervasive applause http experience.30...   \n",
       "3   year thats of life probablyyg retro unnamed J...   \n",
       "4   seemed so really W I scared by, a Mumbai The ...   \n",
       "\n",
       "                                                 gpt  \n",
       "0  ? The here he knows me without B- here.  I loo...  \n",
       "1   ) near Mittlebusters is theory crafted III. (...  \n",
       "2   aged AWE.com Click enjoyed much me, & It. She...  \n",
       "3   object.ies in: HOU my the as 2014 Dec it. Dec...  \n",
       "4   creator anyway a government\" \" db 2 grim digi...  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_results = sentiment_pipeline(list(df['input_text']))\n",
    "df['pred_input'] = [i['label'].lower() for i in sentiment_results]\n",
    "sentiment_results = sentiment_pipeline(list(df['real_output']))\n",
    "df['pred_output'] = [i['label'].lower() for i in sentiment_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_labels</th>\n",
       "      <th>cocon_plain</th>\n",
       "      <th>content</th>\n",
       "      <th>real_output</th>\n",
       "      <th>cocon_senti_naive</th>\n",
       "      <th>gpt</th>\n",
       "      <th>pred_input</th>\n",
       "      <th>pred_output</th>\n",
       "      <th>pred_cocon_senti_naive</th>\n",
       "      <th>pred_cocon_plain</th>\n",
       "      <th>pred_gpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forget all those sappy romantic movies involvi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happen not when is many but Juliusts extremel...</td>\n",
       "      <td>flatteringly intelligible peerless indebted la...</td>\n",
       "      <td>over-simplified unrealistic romance. Forget a...</td>\n",
       "      <td>you made 2009ames a you. host wouldone slight...</td>\n",
       "      <td>how low low of me. bodyguard whatever project...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I watched the movie about 13 yrs ago while liv...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a. \" When wasS I this Film w very's of, encou...</td>\n",
       "      <td>multi-purpose examplar cleaner unassailable</td>\n",
       "      <td>in the back that most don't bother to browse....</td>\n",
       "      <td>reference aiced sod placesor placezit he conc...</td>\n",
       "      <td>shelves. unbelievable? Survival vS forgot.Hid...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie is on the level with \"Welcome Home ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>one see only crown j less v rumors Civil the ...</td>\n",
       "      <td>gaff destains defamation detested forbidden gr...</td>\n",
       "      <td>guys weren't Adam Sandler's gay friends, this...</td>\n",
       "      <td>it like bears down. This thoughtii you seems ...</td>\n",
       "      <td>members of Andy.\" Picture that chrome Her一 fo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If there were two parts that the physically to...</td>\n",
       "      <td>positive</td>\n",
       "      <td>my. pretty head of from are the was killings ...</td>\n",
       "      <td>pleasurably entrust liked deft improvements entr</td>\n",
       "      <td>play, it must surely have been Cyrano de Berg...</td>\n",
       "      <td>9 iswas withR flashed me point that worstS end...</td>\n",
       "      <td>Usire a celebrated that in less These everybo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Like many a child born in the 1980's, I grew u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>buff very. in Iattery lovevisible system loop...</td>\n",
       "      <td>conciliate pleasurably intelligent astonishing...</td>\n",
       "      <td>Saddles and History of the World part 1 (I sa...</td>\n",
       "      <td>,ab, originally I unknown.inr first we fascina...</td>\n",
       "      <td>cADLY on a comingcitizens a the L's on you are...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text target_labels  \\\n",
       "0  Forget all those sappy romantic movies involvi...      positive   \n",
       "1  I watched the movie about 13 yrs ago while liv...      positive   \n",
       "2  This movie is on the level with \"Welcome Home ...      negative   \n",
       "3  If there were two parts that the physically to...      positive   \n",
       "4  Like many a child born in the 1980's, I grew u...      positive   \n",
       "\n",
       "                                         cocon_plain  \\\n",
       "0   happen not when is many but Juliusts extremel...   \n",
       "1   a. \" When wasS I this Film w very's of, encou...   \n",
       "2   one see only crown j less v rumors Civil the ...   \n",
       "3   my. pretty head of from are the was killings ...   \n",
       "4   buff very. in Iattery lovevisible system loop...   \n",
       "\n",
       "                                             content  \\\n",
       "0  flatteringly intelligible peerless indebted la...   \n",
       "1        multi-purpose examplar cleaner unassailable   \n",
       "2  gaff destains defamation detested forbidden gr...   \n",
       "3   pleasurably entrust liked deft improvements entr   \n",
       "4  conciliate pleasurably intelligent astonishing...   \n",
       "\n",
       "                                         real_output  \\\n",
       "0   over-simplified unrealistic romance. Forget a...   \n",
       "1   in the back that most don't bother to browse....   \n",
       "2   guys weren't Adam Sandler's gay friends, this...   \n",
       "3   play, it must surely have been Cyrano de Berg...   \n",
       "4   Saddles and History of the World part 1 (I sa...   \n",
       "\n",
       "                                   cocon_senti_naive  \\\n",
       "0   you made 2009ames a you. host wouldone slight...   \n",
       "1   reference aiced sod placesor placezit he conc...   \n",
       "2   it like bears down. This thoughtii you seems ...   \n",
       "3  9 iswas withR flashed me point that worstS end...   \n",
       "4  ,ab, originally I unknown.inr first we fascina...   \n",
       "\n",
       "                                                 gpt pred_input pred_output  \\\n",
       "0   how low low of me. bodyguard whatever project...   negative    negative   \n",
       "1   shelves. unbelievable? Survival vS forgot.Hid...   negative    positive   \n",
       "2   members of Andy.\" Picture that chrome Her一 fo...   negative    negative   \n",
       "3   Usire a celebrated that in less These everybo...   negative    positive   \n",
       "4  cADLY on a comingcitizens a the L's on you are...   negative    negative   \n",
       "\n",
       "  pred_cocon_senti_naive pred_cocon_plain  pred_gpt  \n",
       "0               positive         negative  negative  \n",
       "1               negative         positive  negative  \n",
       "2               negative         negative  negative  \n",
       "3               negative         positive  negative  \n",
       "4               negative         negative  negative  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['target_labels'] == df['pred_input']).sum() #out of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['target_labels'] == df['pred_output']).sum() #out of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_results = sentiment_pipeline(list(df['cocon_senti_naive']))\n",
    "df['pred_cocon_senti_naive'] = [i['label'].lower() for i in sentiment_results]\n",
    "\n",
    "(df['target_labels'] == df['pred_cocon_senti_naive']).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_results = sentiment_pipeline(list(df['cocon_plain']))\n",
    "df['pred_cocon_plain'] = [i['label'].lower() for i in sentiment_results]\n",
    "\n",
    "(df['target_labels'] == df['pred_cocon_plain']).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_results = sentiment_pipeline(list(df['gpt']))\n",
    "df['pred_gpt'] = [i['label'].lower() for i in sentiment_results]\n",
    "\n",
    "(df['target_labels'] == df['pred_gpt']).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('model_generations2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['input_text', 'target_labels', 'cocon_plain', 'content', 'real_output',\n",
       "       'cocon_senti_naive', 'gpt', 'pred_input', 'pred_output',\n",
       "       'pred_cocon_senti_naive', 'pred_cocon_plain', 'pred_gpt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(cocon_plain_pred),30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gold'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gold'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-235-7a7036a8a277>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gold'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gold'"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(len(tokenizer.encode(df['gold'][index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_gpt = []\n",
    "glue_plain = []\n",
    "glue_senti = []\n",
    "for index, row in df.iterrows():   \n",
    "    cocon_plain_pred = tokenizer.encode(df['cocon_plain'][index])\n",
    "    cocon_senti_pred = tokenizer.encode(df['cocon_senti_naive'][index])\n",
    "    gold = tokenizer.encode(df['real_output'][index])\n",
    "    gpt_pred = tokenizer.encode(df['gpt'][index])\n",
    "    gi = min(len(cocon_plain_pred),len(cocon_senti_pred),30,len(gold), len(gpt_pred))\n",
    "    cocon_plain_pred = tokenizer.encode(df['cocon_plain'][index])[:gi]\n",
    "    gold = tokenizer.encode(df['real_output'][index])[:gi]\n",
    "    cocon_senti_pred = tokenizer.encode(df['cocon_senti_naive'][index])[:gi]\n",
    "    gpt_pred = tokenizer.encode(df['gpt'][index])[:gi]\n",
    "    \n",
    "        \n",
    "    plain_a = metric.compute(predictions=cocon_plain_pred, references=gold)\n",
    "    senti_a = metric.compute(predictions=cocon_senti_pred, references=gold)\n",
    "    gpt_a = metric.compute(predictions=gpt_pred, references=gold)\n",
    "    glue_gpt.append(gpt_a['accuracy'])\n",
    "    glue_plain.append(plain_a['accuracy'])\n",
    "    glue_senti.append(senti_a['accuracy'])\n",
    "\n",
    "df['glue_gpt'] = glue_gpt\n",
    "df['glue_senti'] = glue_senti\n",
    "df['glue_plain'] = glue_plain\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_labels</th>\n",
       "      <th>cocon_plain</th>\n",
       "      <th>content</th>\n",
       "      <th>real_output</th>\n",
       "      <th>cocon_senti_naive</th>\n",
       "      <th>gpt</th>\n",
       "      <th>pred_input</th>\n",
       "      <th>pred_output</th>\n",
       "      <th>pred_cocon_senti_naive</th>\n",
       "      <th>pred_cocon_plain</th>\n",
       "      <th>pred_gpt</th>\n",
       "      <th>glue_gpt</th>\n",
       "      <th>glue_senti</th>\n",
       "      <th>glue_plain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forget all those sappy romantic movies involvi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happen not when is many but Juliusts extremel...</td>\n",
       "      <td>flatteringly intelligible peerless indebted la...</td>\n",
       "      <td>over-simplified unrealistic romance. Forget a...</td>\n",
       "      <td>you made 2009ames a you. host wouldone slight...</td>\n",
       "      <td>how low low of me. bodyguard whatever project...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I watched the movie about 13 yrs ago while liv...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a. \" When wasS I this Film w very's of, encou...</td>\n",
       "      <td>multi-purpose examplar cleaner unassailable</td>\n",
       "      <td>in the back that most don't bother to browse....</td>\n",
       "      <td>reference aiced sod placesor placezit he conc...</td>\n",
       "      <td>shelves. unbelievable? Survival vS forgot.Hid...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie is on the level with \"Welcome Home ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>one see only crown j less v rumors Civil the ...</td>\n",
       "      <td>gaff destains defamation detested forbidden gr...</td>\n",
       "      <td>guys weren't Adam Sandler's gay friends, this...</td>\n",
       "      <td>it like bears down. This thoughtii you seems ...</td>\n",
       "      <td>members of Andy.\" Picture that chrome Her一 fo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If there were two parts that the physically to...</td>\n",
       "      <td>positive</td>\n",
       "      <td>my. pretty head of from are the was killings ...</td>\n",
       "      <td>pleasurably entrust liked deft improvements entr</td>\n",
       "      <td>play, it must surely have been Cyrano de Berg...</td>\n",
       "      <td>9 iswas withR flashed me point that worstS end...</td>\n",
       "      <td>Usire a celebrated that in less These everybo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Like many a child born in the 1980's, I grew u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>buff very. in Iattery lovevisible system loop...</td>\n",
       "      <td>conciliate pleasurably intelligent astonishing...</td>\n",
       "      <td>Saddles and History of the World part 1 (I sa...</td>\n",
       "      <td>,ab, originally I unknown.inr first we fascina...</td>\n",
       "      <td>cADLY on a comingcitizens a the L's on you are...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text target_labels  \\\n",
       "0  Forget all those sappy romantic movies involvi...      positive   \n",
       "1  I watched the movie about 13 yrs ago while liv...      positive   \n",
       "2  This movie is on the level with \"Welcome Home ...      negative   \n",
       "3  If there were two parts that the physically to...      positive   \n",
       "4  Like many a child born in the 1980's, I grew u...      positive   \n",
       "\n",
       "                                         cocon_plain  \\\n",
       "0   happen not when is many but Juliusts extremel...   \n",
       "1   a. \" When wasS I this Film w very's of, encou...   \n",
       "2   one see only crown j less v rumors Civil the ...   \n",
       "3   my. pretty head of from are the was killings ...   \n",
       "4   buff very. in Iattery lovevisible system loop...   \n",
       "\n",
       "                                             content  \\\n",
       "0  flatteringly intelligible peerless indebted la...   \n",
       "1        multi-purpose examplar cleaner unassailable   \n",
       "2  gaff destains defamation detested forbidden gr...   \n",
       "3   pleasurably entrust liked deft improvements entr   \n",
       "4  conciliate pleasurably intelligent astonishing...   \n",
       "\n",
       "                                         real_output  \\\n",
       "0   over-simplified unrealistic romance. Forget a...   \n",
       "1   in the back that most don't bother to browse....   \n",
       "2   guys weren't Adam Sandler's gay friends, this...   \n",
       "3   play, it must surely have been Cyrano de Berg...   \n",
       "4   Saddles and History of the World part 1 (I sa...   \n",
       "\n",
       "                                   cocon_senti_naive  \\\n",
       "0   you made 2009ames a you. host wouldone slight...   \n",
       "1   reference aiced sod placesor placezit he conc...   \n",
       "2   it like bears down. This thoughtii you seems ...   \n",
       "3  9 iswas withR flashed me point that worstS end...   \n",
       "4  ,ab, originally I unknown.inr first we fascina...   \n",
       "\n",
       "                                                 gpt pred_input pred_output  \\\n",
       "0   how low low of me. bodyguard whatever project...   negative    negative   \n",
       "1   shelves. unbelievable? Survival vS forgot.Hid...   negative    positive   \n",
       "2   members of Andy.\" Picture that chrome Her一 fo...   negative    negative   \n",
       "3   Usire a celebrated that in less These everybo...   negative    positive   \n",
       "4  cADLY on a comingcitizens a the L's on you are...   negative    negative   \n",
       "\n",
       "  pred_cocon_senti_naive pred_cocon_plain  pred_gpt  glue_gpt  glue_senti  \\\n",
       "0               positive         negative  negative       0.0         0.0   \n",
       "1               negative         positive  negative       0.0         0.0   \n",
       "2               negative         negative  negative       0.0         0.0   \n",
       "3               negative         positive  negative       0.0         0.0   \n",
       "4               negative         negative  negative       0.0         0.0   \n",
       "\n",
       "   glue_plain  \n",
       "0    0.000000  \n",
       "1    0.000000  \n",
       "2    0.000000  \n",
       "3    0.000000  \n",
       "4    0.034483  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 7, 12)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['glue_plain']>0), sum(df['glue_senti']>0), sum(df['glue_gpt']>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' over-simplified unrealistic romance. Forget all those shameless \"dog gives its life to save its family\" flicks (although I have to admit']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df['real_output'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df45412d784e47be9b1dfda4d85736fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a3d9273bb54bccbd6f2f3d4f5f3985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/2.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculates how good are predictions given some references, using certain scores\n",
      "Args:\n",
      "    predictions: list of candidates to evaluate. Each candidates should be a list\n",
      "        of strings with several code candidates to solve the problem.\n",
      "    references: a list with a test for each prediction. Each test should evaluate the\n",
      "        correctness of a code candidate.\n",
      "    k: number of code candidates to consider in the evaluation (Default: [1, 10, 100])\n",
      "    num_workers: number of workers used to evaluate the canidate programs (Default: 4).\n",
      "    timeout:\n",
      "Returns:\n",
      "    pass_at_k: dict with pass rates for each k\n",
      "    results: dict with granular results of each unittest\n",
      "Examples:\n",
      "    >>> code_eval = datasets.load_metric(\"code_eval\")\n",
      "    >>> test_cases = [\"assert add(2,3)==5\"]\n",
      "    >>> candidates = [[\"def add(a,b): return a*b\", \"def add(a, b): return a+b\"]]\n",
      "    >>> pass_at_k, results = code_eval.compute(references=test_cases, predictions=candidates, k=[1, 2])\n",
      "    >>> print(pass_at_k)\n",
      "    {'pass@1': 0.5, 'pass@2': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"code_eval\")\n",
    "print(metric.inputs_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: datasets.load_metric('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f30da75bb844cf81c815d85c9b557d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\aishu\\.cache\\huggingface\\metrics\\bleurt\\default\\downloads\\extracted\\4f6af8a094ab2435d43ed5ff8d3b150764d255482ae3e5c25ab09aaba351d0cd\\bleurt-base-128.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint bert_custom\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:bert_custom\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:128\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"bleurt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
      "Args:\n",
      "    predictions: list of predictions to score.\n",
      "        Each translation should be tokenized into a list of tokens.\n",
      "    references: list of lists of references for each translation.\n",
      "        Each reference should be tokenized into a list of tokens.\n",
      "Returns: depending on the GLUE subset, one or several of:\n",
      "    \"accuracy\": Accuracy\n",
      "    \"f1\": F1 score\n",
      "    \"pearson\": Pearson Correlation\n",
      "    \"spearmanr\": Spearman Correlation\n",
      "    \"matthews_correlation\": Matthew Correlation\n",
      "Examples:\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0, 'f1': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
      "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
      "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'matthews_correlation': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metric.inputs_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"glue\",'sst2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['input_text', 'target_labels', 'cocon_plain', 'content', 'real_output',\n",
       "       'cocon_senti_naive', 'gpt', 'pred_input', 'pred_output',\n",
       "       'pred_cocon_senti_naive', 'pred_cocon_plain', 'pred_gpt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([' happen not when is many but J'], [' over-simplified unrealistic r'])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df['cocon_plain'][0][:30]], [df['real_output'][0][:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725156adab964b94829cae44933d58f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Args:\n",
      "    model_id (str): model used for calculating Perplexity\n",
      "            NOTE: Perplexity can only be calculated for causal language models.\n",
      "                    This includes models such as gpt2, causal variations of bert,\n",
      "                    causal versions of t5, and more (the full list can be found\n",
      "                    in the AutoModelForCausalLM documentation here:\n",
      "                    https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForCausalLM )\n",
      "\n",
      "    input_texts (list of str): input text, each separate text snippet\n",
      "        is one list entry. Perplexity returned will be an average of\n",
      "        the perplexity for each list entry.\n",
      "    stride (int): stride size, defaults to 512\n",
      "    device (str): device to run on, defaults to 'cuda' when available\n",
      "Returns:\n",
      "    perplexity: dictionary containing the average perplexity score for the text\n",
      "        in the input list.\n",
      "Examples:\n",
      "    Example 1:\n",
      "        >>> perplexity = datasets.load_metric(\"perplexity\")\n",
      "        >>> input_texts = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\n",
      "        >>> results = perplexity.compute(model_id='gpt2',\n",
      "        ...                              input_texts=input_texts,\n",
      "        ...                              stride=1)\n",
      "        >>> round(results[\"perplexity\"], 1)\n",
      "        78.2\n",
      "\n",
      "    Example 2:\n",
      "        >>> perplexity = datasets.load_metric(\"perplexity\")\n",
      "        >>> input_texts = datasets.load_dataset(\"wikitext\",\n",
      "        ...                                     \"wikitext-2-raw-v1\",\n",
      "        ...                                     split=\"test\")[\"text\"][:10] # doctest:+ELLIPSIS\n",
      "        [...]\n",
      "        >>> results = perplexity.compute(model_id='gpt2',\n",
      "        ...                              input_texts=input_texts,\n",
      "        ...                              stride=256)\n",
      "        >>> round(results[\"perplexity\"], 1)\n",
      "        117.9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"perplexity\")\n",
    "print(metric.inputs_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f94161d3b9140a7a46a9047333618ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': 676.169189453125}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = metric.compute(model_id='gpt2',input_texts=df['cocon_plain'].tolist(),stride=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9274932ffaa648c48b820dea45ca7b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': 366.1953430175781}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gpt = metric.compute(model_id='gpt2',input_texts=df['gpt'].tolist(),stride=1)\n",
    "results_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d9fc0a700741c483187b07559e4c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': 846.583740234375}"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_senti = metric.compute(model_id='gpt2',input_texts=df['cocon_senti_naive'].tolist(),stride=1)\n",
    "results_senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb368dedb91447eadf26c9fc5d8ee88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': 20.682506561279297}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_real = metric.compute(model_id='gpt2',input_texts=df['real_output'].tolist(),stride=1)\n",
    "results_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculates average rouge scores for a list of hypotheses and references\n",
      "Args:\n",
      "    predictions: list of predictions to score. Each predictions\n",
      "        should be a string with tokens separated by spaces.\n",
      "    references: list of reference for each prediction. Each\n",
      "        reference should be a string with tokens separated by spaces.\n",
      "    rouge_types: A list of rouge types to calculate.\n",
      "        Valid names:\n",
      "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
      "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
      "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
      "\"`.\n",
      "        See details in https://github.com/huggingface/datasets/issues/617\n",
      "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
      "    use_agregator: Return aggregates if this is set to True\n",
      "Returns:\n",
      "    rouge1: rouge_1 (precision, recall, f1),\n",
      "    rouge2: rouge_2 (precision, recall, f1),\n",
      "    rougeL: rouge_l (precision, recall, f1),\n",
      "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
      "Examples:\n",
      "\n",
      "    >>> rouge = datasets.load_metric('rouge')\n",
      "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
      "    >>> references = [\"hello there\", \"general kenobi\"]\n",
      "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
      "    >>> print(list(results.keys()))\n",
      "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
      "    >>> print(results[\"rouge1\"])\n",
      "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
      "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
      "    1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"rouge\")\n",
    "print(metric.inputs_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\aishu\\appdata\\roaming\\python\\python37\\site-packages (from rouge_score) (3.5)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\aishu\\miniconda\\lib\\site-packages (from rouge_score) (1.15.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\aishu\\miniconda\\lib\\site-packages (from rouge_score) (1.19.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aishu\\miniconda\\lib\\site-packages (from rouge_score) (0.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aishu\\miniconda\\lib\\site-packages (from nltk->rouge_score) (4.63.1)\n",
      "Requirement already satisfied: click in c:\\users\\aishu\\appdata\\roaming\\python\\python37\\site-packages (from nltk->rouge_score) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\aishu\\appdata\\roaming\\python\\python37\\site-packages (from nltk->rouge_score) (0.17.0)\n",
      "Requirement already satisfied: regex in c:\\users\\aishu\\appdata\\roaming\\python\\python37\\site-packages (from nltk->rouge_score) (2020.11.13)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\aishu\\miniconda\\lib\\site-packages (from tqdm->nltk->rouge_score) (0.4.3)\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\aishu\\miniconda\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Got a string but expected a list instead: ' happen not when is many but J'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-252-a168891317ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cocon_plain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'real_output'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\datasets\\metric.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\datasets\\metric.py\u001b[0m in \u001b[0;36madd_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"predictions\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"references\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreferences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mintput_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mintput_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mintput_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\datasets\\features\\features.py\u001b[0m in \u001b[0;36mencode_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m   1311\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast_to_python_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1313\u001b[1;33m             \u001b[0mencoded_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mencode_nested_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1314\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mencoded_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\datasets\\features\\features.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1311\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast_to_python_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1313\u001b[1;33m             \u001b[0mencoded_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mencode_nested_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1314\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mencoded_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\datasets\\features\\features.py\u001b[0m in \u001b[0;36mencode_nested_example\u001b[1;34m(schema, obj)\u001b[0m\n\u001b[0;32m   1001\u001b[0m         \u001b[1;31m# schema.feature is not a dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1002\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# don't interpret a string as a list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Got a string but expected a list instead: '{obj}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1004\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Got a string but expected a list instead: ' happen not when is many but J'"
     ]
    }
   ],
   "source": [
    "metric.compute(predictions=[df['cocon_plain'][0][:30]], references=[df['real_output'][0][:30]], k=[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1645,\n",
       " 407,\n",
       " 618,\n",
       " 318,\n",
       " 867,\n",
       " 475,\n",
       " 5979,\n",
       " 72,\n",
       " 436,\n",
       " 82,\n",
       " 4457,\n",
       " 1662,\n",
       " 356,\n",
       " 71,\n",
       " 22032,\n",
       " 45129,\n",
       " 617,\n",
       " 6293,\n",
       " 11,\n",
       " 618,\n",
       " 2646,\n",
       " 3807,\n",
       " 314,\n",
       " 475,\n",
       " 11,\n",
       " 3807,\n",
       " 373,\n",
       " 11,\n",
       " 286,\n",
       " 14570,\n",
       " 547,\n",
       " 0]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(df['cocon_plain'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1645,   407,   618,   318,   867,   475,  5979,    72,   436,    82,\n",
       "         4457,  1662,   356,    71, 22032, 45129,   617,  6293,    11,   618,\n",
       "         2646,  3807,   314,   475,    11,  3807,   373,    11,   286, 14570,\n",
       "          547,     0])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor(tokenizer.encode(df['cocon_plain'][0]))\n",
    "#y = y.type(torch.int64)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute __inference_pruned_6256 as input #0(zero-based) was expected to be a int64 tensor but is a int32 tensor [Op:__inference_pruned_6256]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-206-e1b55e311859>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinal_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfinal_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\datasets\\metric.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minput_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtemp_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcompute_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_writer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\bleurt\\f872b8bceb84260fee5854e8335168eadb5ce8aa80fd24a8ff2dbd15136546ac\\bleurt.py\u001b[0m in \u001b[0;36m_compute\u001b[1;34m(self, predictions, references)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_compute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"scores\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\bleurt\\score.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, references, candidates, batch_size, *args)\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[1;34m\"segment_ids\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m       }\n\u001b[1;32m--> 215\u001b[1;33m       \u001b[0mpredict_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m       \u001b[0mbatch_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m       \u001b[0mall_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\bleurt\\score.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, input_dict)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0minput_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_mask\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         segment_ids=tf.constant(\n\u001b[1;32m---> 71\u001b[1;33m             input_dict[\"segment_ids\"]))[\"predictions\"].numpy()\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1705\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mdo\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1706\u001b[0m     \"\"\"\n\u001b[1;32m-> 1707\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1709\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\tensorflow\\python\\eager\\wrap_function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m       return super(WrappedFunction, self)._call_impl(\n\u001b[1;32m--> 247\u001b[1;33m           args, kwargs, cancellation_manager)\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mprune\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1723\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mstructured_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1725\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_with_flat_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1726\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1727\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_with_flat_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_with_flat_signature\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1772\u001b[0m                         \"got {} ({})\".format(self._flat_signature_summary(), i,\n\u001b[0;32m   1773\u001b[0m                                              type(arg).__name__, str(arg)))\n\u001b[1;32m-> 1774\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1776\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_with_structured_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\aishu\\miniconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: cannot compute __inference_pruned_6256 as input #0(zero-based) was expected to be a int64 tensor but is a int32 tensor [Op:__inference_pruned_6256]"
     ]
    }
   ],
   "source": [
    "final_score = metric.compute()\n",
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_input, gold_references in evaluation_dataset:\n",
    "    model_predictions = model(model_inputs)\n",
    "    metric.add_batch(predictions=model_predictions, references=gold_references)\n",
    "final_score = metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_labels</th>\n",
       "      <th>cocon_senti_naive</th>\n",
       "      <th>content</th>\n",
       "      <th>cocon_plain</th>\n",
       "      <th>gpt</th>\n",
       "      <th>pred_input</th>\n",
       "      <th>pred_cocon_senti_naive</th>\n",
       "      <th>pred_cocon_plain</th>\n",
       "      <th>pred_gpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh, man, they sure knew how to make them back ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>,was I fromIi Dempl wayino is posedorious SA a...</td>\n",
       "      <td>condescend bewitch hiliarious absence culp</td>\n",
       "      <td>in very acquaintancen because to scene saidPi...</td>\n",
       "      <td>? The here he knows me without B- here.  I loo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I got stuck in traffic (I live in Sicily) on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Read for outfit,Charlie no Has complete totall...</td>\n",
       "      <td>addicts stark problems awkwardness dragging di...</td>\n",
       "      <td>ision most from one shot graduating Unicode so...</td>\n",
       "      <td>) near Mittlebusters is theory crafted III. (...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I enjoyed this film very much. I found it to b...</td>\n",
       "      <td>positive</td>\n",
       "      <td>H good atold before theis It begins in enjoy ...</td>\n",
       "      <td>resilient congratulatory pamper swift magnificent</td>\n",
       "      <td>she set pervasive applause http experience.30...</td>\n",
       "      <td>aged AWE.com Click enjoyed much me, &amp; It. She...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was in this movie as an extra in the Dallas ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>would H we I possible B taken of In announced...</td>\n",
       "      <td>pitifully devilment discouraging inaccuracy ch...</td>\n",
       "      <td>year thats of life probablyyg retro unnamed J...</td>\n",
       "      <td>object.ies in: HOU my the as 2014 Dec it. Dec...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been a devoted IMDB visitor for a few yea...</td>\n",
       "      <td>positive</td>\n",
       "      <td>around,Fable you can I nothing whyAgain Poste...</td>\n",
       "      <td>plush excel smile top-notch aspire state</td>\n",
       "      <td>seemed so really W I scared by, a Mumbai The ...</td>\n",
       "      <td>creator anyway a government\" \" db 2 grim digi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text target_labels  \\\n",
       "0  Oh, man, they sure knew how to make them back ...      negative   \n",
       "1  I got stuck in traffic (I live in Sicily) on t...      negative   \n",
       "2  I enjoyed this film very much. I found it to b...      positive   \n",
       "3  I was in this movie as an extra in the Dallas ...      negative   \n",
       "4  I've been a devoted IMDB visitor for a few yea...      positive   \n",
       "\n",
       "                                   cocon_senti_naive  \\\n",
       "0  ,was I fromIi Dempl wayino is posedorious SA a...   \n",
       "1  Read for outfit,Charlie no Has complete totall...   \n",
       "2   H good atold before theis It begins in enjoy ...   \n",
       "3   would H we I possible B taken of In announced...   \n",
       "4   around,Fable you can I nothing whyAgain Poste...   \n",
       "\n",
       "                                             content  \\\n",
       "0         condescend bewitch hiliarious absence culp   \n",
       "1  addicts stark problems awkwardness dragging di...   \n",
       "2  resilient congratulatory pamper swift magnificent   \n",
       "3  pitifully devilment discouraging inaccuracy ch...   \n",
       "4           plush excel smile top-notch aspire state   \n",
       "\n",
       "                                         cocon_plain  \\\n",
       "0   in very acquaintancen because to scene saidPi...   \n",
       "1  ision most from one shot graduating Unicode so...   \n",
       "2   she set pervasive applause http experience.30...   \n",
       "3   year thats of life probablyyg retro unnamed J...   \n",
       "4   seemed so really W I scared by, a Mumbai The ...   \n",
       "\n",
       "                                                 gpt pred_input  \\\n",
       "0  ? The here he knows me without B- here.  I loo...   negative   \n",
       "1   ) near Mittlebusters is theory crafted III. (...   negative   \n",
       "2   aged AWE.com Click enjoyed much me, & It. She...   positive   \n",
       "3   object.ies in: HOU my the as 2014 Dec it. Dec...   positive   \n",
       "4   creator anyway a government\" \" db 2 grim digi...   positive   \n",
       "\n",
       "  pred_cocon_senti_naive pred_cocon_plain  pred_gpt  \n",
       "0               negative         negative  negative  \n",
       "1               positive         negative  negative  \n",
       "2               positive         positive  positive  \n",
       "3               positive         positive  negative  \n",
       "4               negative         positive  negative  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt(inp,history=None, gen_len=30):\n",
    "    input_token = torch.tensor(tokenizer.encode(inp))\n",
    "    if(len(input_token.shape)<3):\n",
    "        input_token = input_token.unsqueeze(0) #batch dim\n",
    "\n",
    "    #Repeat for history TO DO\n",
    "    #implement auto regression TODO\n",
    "    input_token = input_token.to(device)\n",
    "    l = len(input_token[0])\n",
    "    for i in range(gen_len):\n",
    "        #L_alpha\n",
    "        hidden_inp = model(input_token,path='half1')\n",
    "        #hidden_content = model(content_token, path='half1')\n",
    "        #Cocon             other_context_cocon_hidden_states = cocon_block(cocon_th_gen_output, context_seq=original_context_seq_hidden_states, history_seq=other_sample_history_seq_hidden_states, include_sos_output=True,cs_self_attn_mask_prob=1)\n",
    "        #cout = cocon_block(hidden_inp, context_seq=hidden_content)\n",
    "        output = model(hidden_inp, path='half2')\n",
    "        pred_token_logits = output[1][:,-1:]\n",
    "        #softmax\n",
    "        pred_token_prob = torch.nn.functional.softmax(pred_token_logits, dim=-1)\n",
    "        #sample\n",
    "        pred_token = torch.multinomial(pred_token_prob[0], num_samples=1) #repeat for every elem in batch\n",
    "        #append\n",
    "        input_token = torch.cat((input_token,pred_token),1)\n",
    "        #decode\n",
    "    #pred_text = tokenizer.decode(input_token)\n",
    "    return input_token, [tokenizer.decode(i) for i in input_token[:,l:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' sky on Malfoy hood of the Cloud } might be 0: represents sq one q: stops The restructuring if few be several. Curtores 0 vanish in']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it, decoded = generate_gpt('The sun shines in the')\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate2(input_token,content_token=None,history=None, gen_len=30):\n",
    "    #input_token = torch.tensor(tokenizer.encode(inp))\n",
    "    #if(len(input_token.shape)<3):\n",
    "    #    input_token = input_token.unsqueeze(0) #batch dim\n",
    "    #if(content_token != None):\n",
    "        #content_token = torch.tensor(tokenizer.encode(content))\n",
    "        #if(len(content_token.shape)<3):\n",
    "         #   content_token = content_token.unsqueeze(0)\n",
    "        #content_token = content_token.unsqueeze(0)\n",
    "    #Repeat for history TO DO\n",
    "    #implement auto regression TODO\n",
    "    input_token = input_token.to(device)\n",
    "    l = len(input_token[0])\n",
    "    #print('l ',l)\n",
    "    content_token = content_token.to(device)\n",
    "    #print(input_token.shape, content_token.shape)\n",
    "    for i in range(gen_len):\n",
    "        #L_alpha\n",
    "        hidden_inp = model(input_token,path='half1')\n",
    "        hidden_content = model(content_token, path='half1')\n",
    "        #Cocon             other_context_cocon_hidden_states = cocon_block(cocon_th_gen_output, context_seq=original_context_seq_hidden_states, history_seq=other_sample_history_seq_hidden_states, include_sos_output=True,cs_self_attn_mask_prob=1)\n",
    "        cout = cocon_block(hidden_inp, context_seq=hidden_content)\n",
    "        output = model(cout, path='half2')\n",
    "        pred_token_logits = output[1][:,-1:]\n",
    "        #softmax\n",
    "        pred_token_prob = torch.nn.functional.softmax(pred_token_logits, dim=-1)\n",
    "        #sample\n",
    "        pred_token = torch.multinomial(pred_token_prob[0], num_samples=1) #repeat for every elem in batch\n",
    "        #append\n",
    "        input_token = torch.cat((input_token,pred_token),1)\n",
    "        #decode\n",
    "    #pred_text = tokenizer.decode(input_token)\n",
    "    return input_token, [tokenizer.decode(i) for i in input_token[:,l:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4xqyrcuAx2i",
    "outputId": "85e88ef8-d927-4a23-e0c3-38af23b0ecca"
   },
   "outputs": [],
   "source": [
    "example = datasets['test'][0:2]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmazUA_UCHXJ",
    "outputId": "264b0d7d-e7a4-4e81-e477-cdde6d424a8c"
   },
   "outputs": [],
   "source": [
    "len(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOrtVCbzC4Xm",
    "outputId": "71db7a13-cf15-430d-daf6-f077ada946ad"
   },
   "outputs": [],
   "source": [
    "sent[example['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwLuHjsM2Bfl",
    "outputId": "773e00d5-07d5-4c66-e7e5-95094756bf3c"
   },
   "outputs": [],
   "source": [
    "out, decoded = generate('I love sci-fi and am willing to put up',content=content, gen_len=20)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWp_cEiTDxAI",
    "outputId": "bcb096df-3e69-4fa6-cd19-97528ba0d34f"
   },
   "outputs": [],
   "source": [
    "out, decoded = generate('I love sci-fi and am willing to put up',content='is perfect', gen_len=20)\n",
    "decoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
