{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SoD_QmsoK5P2"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from transformers import AdamW, GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4g4rrhbKLBNa"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "X8s51mquJa6f",
    "outputId": "798afc20-0197-4ae5-962b-e9f2faa93b21"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>c1</th>\n",
       "      <th>a1</th>\n",
       "      <th>r1</th>\n",
       "      <th>del_sent</th>\n",
       "      <th>defr_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>excellent food .</td>\n",
       "      <td>POS</td>\n",
       "      <td>food .</td>\n",
       "      <td>excellent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;food .&lt;START&gt;excellent food .&lt;...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;excellent&lt;CON_START&gt;food .&lt;START&gt;e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>superb customer service .</td>\n",
       "      <td>POS</td>\n",
       "      <td>customer service .</td>\n",
       "      <td>superb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;customer service .&lt;START&gt;super...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;superb&lt;CON_START&gt;customer service ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they also have daily specials and ice cream wh...</td>\n",
       "      <td>POS</td>\n",
       "      <td>they also have daily specials and ice cream wh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;they also have daily specials ...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;nan&lt;CON_START&gt;they also have daily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it 's a good toasted hoagie .</td>\n",
       "      <td>POS</td>\n",
       "      <td>it 's a good toasted hoagie .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;it 's a good toasted hoagie .&lt;...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;nan&lt;CON_START&gt;it 's a good toasted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the staff is friendly .</td>\n",
       "      <td>POS</td>\n",
       "      <td>the staff is .</td>\n",
       "      <td>friendly</td>\n",
       "      <td>hostile</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;the staff is .&lt;START&gt;the staff...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;friendly&lt;CON_START&gt;the staff is .&lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment  \\\n",
       "0                                   excellent food .       POS   \n",
       "1                          superb customer service .       POS   \n",
       "2  they also have daily specials and ice cream wh...       POS   \n",
       "3                      it 's a good toasted hoagie .       POS   \n",
       "4                            the staff is friendly .       POS   \n",
       "\n",
       "                                                  c1         a1       r1  \\\n",
       "0                                             food .  excellent      NaN   \n",
       "1                                 customer service .     superb      NaN   \n",
       "2  they also have daily specials and ice cream wh...        NaN      NaN   \n",
       "3                      it 's a good toasted hoagie .        NaN      NaN   \n",
       "4                                     the staff is .   friendly  hostile   \n",
       "\n",
       "                                            del_sent  \\\n",
       "0  <POS><CON_START>food .<START>excellent food .<...   \n",
       "1  <POS><CON_START>customer service .<START>super...   \n",
       "2  <POS><CON_START>they also have daily specials ...   \n",
       "3  <POS><CON_START>it 's a good toasted hoagie .<...   \n",
       "4  <POS><CON_START>the staff is .<START>the staff...   \n",
       "\n",
       "                                           defr_sent  \n",
       "0  <ATTR_WORDS>excellent<CON_START>food .<START>e...  \n",
       "1  <ATTR_WORDS>superb<CON_START>customer service ...  \n",
       "2  <ATTR_WORDS>nan<CON_START>they also have daily...  \n",
       "3  <ATTR_WORDS>nan<CON_START>it 's a good toasted...  \n",
       "4  <ATTR_WORDS>friendly<CON_START>the staff is .<...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv('drive/MyDrive/NLP-project-2022/yelp/clean_text_unigrams_r.csv')\n",
    "df = pd.read_csv('clean_text_unigrams_r.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KIaeB7GgLsgW"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "inputs = df['del_sent'].to_list()\n",
    "y = [1 for i in range(len(inputs))]\n",
    "train_ds, eval_ds, y_train, y_test = train_test_split(inputs, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jqICNIZP6b-",
    "outputId": "3287b08a-b8d2-44ed-bb16-7638a39d46f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OcBAS0wHP3vp",
    "outputId": "9b443ae6-16e4-4e20-dbcf-dc39a222d5da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50262"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = ['<POS>', '<NEG>','<CON_START>','<START>','<END>','<PAD>']\n",
    "special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "start_token_id = tokenizer.convert_tokens_to_ids(['<START>'])[0]\n",
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "eqOYyfLHLaUX",
    "outputId": "69465d79-af38-4ec0-f1f9-0abd5a60193e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50262, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50262, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pSYvWGaNJoWt"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_encode(lines):\n",
    "        '''\n",
    "        This method tokenizes the input data and encodes it using the OpenAIGPTTokenizer\n",
    "        :param file_path: Path of the input file, dtype: str\n",
    "        :return: encoded dataset  dtype: list\n",
    "        '''\n",
    "        tokenized_dataset = lines\n",
    "        for i, line in enumerate(tqdm(lines)):\n",
    "            token = tokenizer.tokenize(line)[:512]\n",
    "            tokenized_dataset[i] = tokenizer.convert_tokens_to_ids(token)\n",
    "        return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7SvwxAtLp41",
    "outputId": "6c26551c-aa0d-455d-d78d-05cb701b5116"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 296984/296984 [01:19<00:00, 3756.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 146277/146277 [00:39<00:00, 3690.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input length  52\n",
      "input length  52\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tokenize_and_encode(train_ds)\n",
    "eval_dataset = tokenize_and_encode(eval_ds)\n",
    "input_length = max(max(len(t) for t in train_dataset), max(len(q) for q in eval_dataset))\n",
    "print('input length ', input_length)\n",
    "input_length = min(input_length, 85)\n",
    "print('input length ', input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = tokenizer.convert_tokens_to_ids(['<PAD>'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "lbz9ass_MdTu"
   },
   "outputs": [],
   "source": [
    "def pre_process_dataset(encoded_dataset, input_length, start_token_id,pad_token_id=pad_token_id):\n",
    "        \"\"\"\n",
    "        This method is to create torch tensor of input ids and lm labels\n",
    "        :param encoded_dataset: Input dataset, dtype: list\n",
    "        :param input_length: Maximum length of sentence from training and eval dataset, dtype: int\n",
    "        :param start_token_id: id of the '<START>' token, dtype: int\n",
    "        :return: torch.tensor of size [len(encoded_dataset), 2]\n",
    "        \"\"\"\n",
    "\n",
    "        n_batch = len(encoded_dataset)\n",
    "        input_ids = np.full(shape=(n_batch, input_length), fill_value=pad_token_id, dtype=np.int64)\n",
    "        lm_labels = np.full(shape=(n_batch, input_length), fill_value=-100, dtype=np.int64)\n",
    "\n",
    "        for i, tokens in enumerate(encoded_dataset):\n",
    "            try:\n",
    "                #tokens = tokens[:input_length]\n",
    "                start_id_index = tokens.index(start_token_id)\n",
    "                input_ids[i, :len(tokens)] = tokens\n",
    "                start_id_index = tokens.index(start_token_id)\n",
    "                lm_labels[i, start_id_index : len(tokens)-1] = tokens[start_id_index + 1: len(tokens)]\n",
    "                # LM loss calculate only for tokens after <START> token in the sentence\n",
    "                #lm_labels[i, :len(tokens)-1] = tokens[1:]\n",
    "            except ValueError:\n",
    "                print(\"Index {} doesn't have start token\".format(i))\n",
    "\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        lm_labels = torch.tensor(lm_labels)\n",
    "        tensor_dataset = (input_ids, lm_labels)\n",
    "        #tensor_dataset.append(torch.tensor(d) for d in all_inputs)\n",
    "\n",
    "        return tensor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "cfn52bnPNPiI"
   },
   "outputs": [],
   "source": [
    "train_batch_size = 32\n",
    "num_train_epochs = 2\n",
    "learning_rate = 6.25e-5\n",
    "warmup_proportion = 0.002\n",
    "max_grad_norm = 1\n",
    "weight_decay = 0.01\n",
    "n_gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BB9qu2ajMgbf",
    "outputId": "127e3835-a037-42e6-aeb3-9efdb4480f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Example Input ids= tensor([50257, 50259,   392,   340,   705,    82,  5968,    64,   922,  5145,\n",
      "        50260,   392,   340,   705,    82,  5968,    64,   922,  5145, 50261,\n",
      "        50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262,\n",
      "        50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262,\n",
      "        50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262,\n",
      "        50262, 50262])\n",
      "Training Example Language Modeling ids = tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          392,   340,   705,    82,  5968,    64,   922,  5145, 50261,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "train_tensor_dataset = pre_process_dataset(train_dataset, input_length, start_token_id=start_token_id)\n",
    "eval_tensor_dataset = pre_process_dataset(eval_dataset, input_length, start_token_id=start_token_id)\n",
    "\n",
    "print(\"Training Example Input ids= {}\".format(train_tensor_dataset[0][0]))\n",
    "print(\"Training Example Language Modeling ids = {}\".format(train_tensor_dataset[1][0]))\n",
    "time.sleep(10)\n",
    "train_data = TensorDataset(*train_tensor_dataset)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "eval_data = TensorDataset(*eval_tensor_dataset)\n",
    "eval_sampler = RandomSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=train_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-zPW-lyMxXa",
    "outputId": "83af1f57-5db6-4a71-f0b3-c1c5e8c3c155"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aishu\\miniconda\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "num_train_optimization_steps = len(train_data) * num_train_epochs //train_batch_size\n",
    "warmup_steps = num_train_optimization_steps * warmup_proportion\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                           lr=learning_rate,\n",
    "                           weight_decay=weight_decay)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_train_optimization_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "jB83q0JJNzxi"
   },
   "outputs": [],
   "source": [
    "do_train = True\n",
    "output_dir = 'yelp/models'\n",
    "do_eval = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrCJ1scWMxdi",
    "outputId": "c5a5b19c-e5bf-4ccb-f3bd-0f80933f46e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Training loss: 5.53e-01:  39%|███▉      | 3621/9281 [51:35<1:20:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.92e-01:  39%|███▉      | 3622/9281 [51:36<1:20:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  39%|███▉      | 3623/9281 [51:36<1:20:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  39%|███▉      | 3624/9281 [51:37<1:20:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.83e-01:  39%|███▉      | 3625/9281 [51:38<1:20:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  39%|███▉      | 3626/9281 [51:39<1:20:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.83e-01:  39%|███▉      | 3627/9281 [51:40<1:20:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  39%|███▉      | 3628/9281 [51:41<1:20:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.88e-01:  39%|███▉      | 3629/9281 [51:42<1:20:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  39%|███▉      | 3630/9281 [51:42<1:20:20,  1.17it/s]\u001b[A\n",
      "Training loss: 5.43e-01:  39%|███▉      | 3631/9281 [51:43<1:20:16,  1.17it/s]\u001b[A\n",
      "Training loss: 5.66e-01:  39%|███▉      | 3632/9281 [51:44<1:20:35,  1.17it/s]\u001b[A\n",
      "Training loss: 5.36e-01:  39%|███▉      | 3633/9281 [51:45<1:20:18,  1.17it/s]\u001b[A\n",
      "Training loss: 5.23e-01:  39%|███▉      | 3634/9281 [51:46<1:20:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  39%|███▉      | 3635/9281 [51:47<1:20:26,  1.17it/s]\u001b[A\n",
      "Training loss: 5.01e-01:  39%|███▉      | 3636/9281 [51:47<1:20:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.96e-01:  39%|███▉      | 3637/9281 [51:48<1:20:19,  1.17it/s]\u001b[A\n",
      "Training loss: 5.25e-01:  39%|███▉      | 3638/9281 [51:49<1:20:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  39%|███▉      | 3639/9281 [51:50<1:20:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.83e-01:  39%|███▉      | 3640/9281 [51:51<1:20:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.79e-01:  39%|███▉      | 3641/9281 [51:52<1:20:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.95e-01:  39%|███▉      | 3642/9281 [51:53<1:20:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.87e-01:  39%|███▉      | 3643/9281 [51:53<1:20:24,  1.17it/s]\u001b[A\n",
      "Training loss: 5.11e-01:  39%|███▉      | 3644/9281 [51:54<1:20:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  39%|███▉      | 3645/9281 [51:55<1:20:21,  1.17it/s]\u001b[A\n",
      "Training loss: 5.18e-01:  39%|███▉      | 3646/9281 [51:56<1:20:17,  1.17it/s]\u001b[A\n",
      "Training loss: 5.39e-01:  39%|███▉      | 3647/9281 [51:57<1:20:16,  1.17it/s]\u001b[A\n",
      "Training loss: 5.24e-01:  39%|███▉      | 3648/9281 [51:58<1:20:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  39%|███▉      | 3649/9281 [51:59<1:20:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  39%|███▉      | 3650/9281 [51:59<1:20:24,  1.17it/s]\u001b[A\n",
      "Training loss: 5.20e-01:  39%|███▉      | 3651/9281 [52:00<1:20:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  39%|███▉      | 3652/9281 [52:01<1:20:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  39%|███▉      | 3653/9281 [52:02<1:20:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  39%|███▉      | 3654/9281 [52:03<1:20:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  39%|███▉      | 3655/9281 [52:04<1:19:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.95e-01:  39%|███▉      | 3656/9281 [52:05<1:20:01,  1.17it/s]\u001b[A\n",
      "Training loss: 5.31e-01:  39%|███▉      | 3657/9281 [52:05<1:20:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  39%|███▉      | 3658/9281 [52:06<1:19:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  39%|███▉      | 3659/9281 [52:07<1:19:43,  1.18it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  39%|███▉      | 3660/9281 [52:08<1:20:21,  1.17it/s]\u001b[A\n",
      "Training loss: 5.07e-01:  39%|███▉      | 3661/9281 [52:09<1:20:09,  1.17it/s]\u001b[A\n",
      "Training loss: 5.25e-01:  39%|███▉      | 3662/9281 [52:10<1:20:05,  1.17it/s]\u001b[A\n",
      "Training loss: 5.61e-01:  39%|███▉      | 3663/9281 [52:11<1:19:45,  1.17it/s]\u001b[A\n",
      "Training loss: 5.20e-01:  39%|███▉      | 3664/9281 [52:11<1:19:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  39%|███▉      | 3665/9281 [52:12<1:19:28,  1.18it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  40%|███▉      | 3666/9281 [52:13<1:19:35,  1.18it/s]\u001b[A\n",
      "Training loss: 5.39e-01:  40%|███▉      | 3667/9281 [52:14<1:19:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  40%|███▉      | 3668/9281 [52:15<1:19:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  40%|███▉      | 3669/9281 [52:16<1:20:18,  1.16it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  40%|███▉      | 3670/9281 [52:17<1:20:19,  1.16it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  40%|███▉      | 3671/9281 [52:17<1:20:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  40%|███▉      | 3672/9281 [52:18<1:19:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  40%|███▉      | 3673/9281 [52:19<1:19:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  40%|███▉      | 3674/9281 [52:20<1:19:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  40%|███▉      | 3675/9281 [52:21<1:19:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  40%|███▉      | 3676/9281 [52:22<1:19:51,  1.17it/s]\u001b[A\n",
      "Training loss: 5.00e-01:  40%|███▉      | 3677/9281 [52:23<1:19:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  40%|███▉      | 3678/9281 [52:23<1:19:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  40%|███▉      | 3679/9281 [52:24<1:20:07,  1.17it/s]\u001b[A\n",
      "Training loss: 5.08e-01:  40%|███▉      | 3680/9281 [52:25<1:19:48,  1.17it/s]\u001b[A\n",
      "Training loss: 5.41e-01:  40%|███▉      | 3681/9281 [52:26<1:19:53,  1.17it/s]\u001b[A\n",
      "Training loss: 5.28e-01:  40%|███▉      | 3682/9281 [52:27<1:19:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  40%|███▉      | 3683/9281 [52:28<1:19:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  40%|███▉      | 3684/9281 [52:29<1:19:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  40%|███▉      | 3685/9281 [52:29<1:19:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.83e-01:  40%|███▉      | 3686/9281 [52:30<1:19:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  40%|███▉      | 3687/9281 [52:31<1:19:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  40%|███▉      | 3688/9281 [52:32<1:19:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  40%|███▉      | 3689/9281 [52:33<1:19:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  40%|███▉      | 3690/9281 [52:34<1:19:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  40%|███▉      | 3691/9281 [52:34<1:19:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.84e-01:  40%|███▉      | 3692/9281 [52:35<1:19:09,  1.18it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  40%|███▉      | 3693/9281 [52:36<1:19:15,  1.17it/s]\u001b[A\n",
      "Training loss: 5.12e-01:  40%|███▉      | 3694/9281 [52:37<1:19:09,  1.18it/s]\u001b[A\n",
      "Training loss: 5.21e-01:  40%|███▉      | 3695/9281 [52:38<1:19:15,  1.17it/s]\u001b[A\n",
      "Training loss: 5.05e-01:  40%|███▉      | 3696/9281 [52:39<1:19:13,  1.17it/s]\u001b[A\n",
      "Training loss: 5.51e-01:  40%|███▉      | 3697/9281 [52:40<1:19:16,  1.17it/s]\u001b[A\n",
      "Training loss: 5.27e-01:  40%|███▉      | 3698/9281 [52:40<1:19:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.87e-01:  40%|███▉      | 3699/9281 [52:41<1:19:18,  1.17it/s]\u001b[A\n",
      "Training loss: 5.38e-01:  40%|███▉      | 3700/9281 [52:42<1:19:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.79e-01:  40%|███▉      | 3701/9281 [52:43<1:19:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.98e-01:  40%|███▉      | 3702/9281 [52:44<1:19:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  40%|███▉      | 3703/9281 [52:45<1:19:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  40%|███▉      | 3704/9281 [52:46<1:19:34,  1.17it/s]\u001b[A\n",
      "Training loss: 5.15e-01:  40%|███▉      | 3705/9281 [52:46<1:19:32,  1.17it/s]\u001b[A\n",
      "Training loss: 5.06e-01:  40%|███▉      | 3706/9281 [52:47<1:19:19,  1.17it/s]\u001b[A\n",
      "Training loss: 5.87e-01:  40%|███▉      | 3707/9281 [52:48<1:19:37,  1.17it/s]\u001b[A\n",
      "Training loss: 5.74e-01:  40%|███▉      | 3708/9281 [52:49<1:19:26,  1.17it/s]\u001b[A\n",
      "Training loss: 5.76e-01:  40%|███▉      | 3709/9281 [52:50<1:19:31,  1.17it/s]\u001b[A\n",
      "Training loss: 5.73e-01:  40%|███▉      | 3710/9281 [52:51<1:19:28,  1.17it/s]\u001b[A\n",
      "Training loss: 5.31e-01:  40%|███▉      | 3711/9281 [52:52<1:19:36,  1.17it/s]\u001b[A\n",
      "Training loss: 5.28e-01:  40%|███▉      | 3712/9281 [52:52<1:19:19,  1.17it/s]\u001b[A\n",
      "Training loss: 5.10e-01:  40%|████      | 3713/9281 [52:53<1:19:22,  1.17it/s]\u001b[A\n",
      "Training loss: 5.32e-01:  40%|████      | 3714/9281 [52:54<1:19:25,  1.17it/s]\u001b[A\n",
      "Training loss: 5.21e-01:  40%|████      | 3715/9281 [52:55<1:19:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  40%|████      | 3716/9281 [52:56<1:19:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  40%|████      | 3717/9281 [52:57<1:19:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.95e-01:  40%|████      | 3718/9281 [52:58<1:19:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  40%|████      | 3719/9281 [52:58<1:19:09,  1.17it/s]\u001b[A\n",
      "Training loss: 5.20e-01:  40%|████      | 3720/9281 [52:59<1:19:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  40%|████      | 3721/9281 [53:00<1:18:58,  1.17it/s]\u001b[A\n",
      "Training loss: 5.25e-01:  40%|████      | 3722/9281 [53:01<1:19:14,  1.17it/s]\u001b[A\n",
      "Training loss: 5.35e-01:  40%|████      | 3723/9281 [53:02<1:19:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  40%|████      | 3724/9281 [53:03<1:19:10,  1.17it/s]\u001b[A\n",
      "Training loss: 5.44e-01:  40%|████      | 3725/9281 [53:04<1:18:58,  1.17it/s]\u001b[A\n",
      "Training loss: 5.89e-01:  40%|████      | 3726/9281 [53:04<1:19:05,  1.17it/s]\u001b[A\n",
      "Training loss: 5.74e-01:  40%|████      | 3727/9281 [53:05<1:19:07,  1.17it/s]\u001b[A\n",
      "Training loss: 5.41e-01:  40%|████      | 3728/9281 [53:06<1:18:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  40%|████      | 3729/9281 [53:07<1:18:59,  1.17it/s]\u001b[A\n",
      "Training loss: 5.10e-01:  40%|████      | 3730/9281 [53:08<1:19:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  40%|████      | 3731/9281 [53:09<1:19:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  40%|████      | 3732/9281 [53:10<1:18:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  40%|████      | 3733/9281 [53:10<1:18:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  40%|████      | 3734/9281 [53:11<1:19:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.81e-01:  40%|████      | 3735/9281 [53:12<1:19:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  40%|████      | 3736/9281 [53:13<1:19:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.69e-01:  40%|████      | 3737/9281 [53:14<1:19:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  40%|████      | 3738/9281 [53:15<1:19:01,  1.17it/s]\u001b[A\n",
      "Training loss: 5.89e-01:  40%|████      | 3739/9281 [53:15<1:18:50,  1.17it/s]\u001b[A\n",
      "Training loss: 5.25e-01:  40%|████      | 3740/9281 [53:16<1:18:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  40%|████      | 3741/9281 [53:17<1:18:43,  1.17it/s]\u001b[A\n",
      "Training loss: 5.07e-01:  40%|████      | 3742/9281 [53:18<1:18:54,  1.17it/s]\u001b[A\n",
      "Training loss: 5.15e-01:  40%|████      | 3743/9281 [53:19<1:18:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.92e-01:  40%|████      | 3744/9281 [53:20<1:19:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  40%|████      | 3745/9281 [53:21<1:18:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  40%|████      | 3746/9281 [53:21<1:19:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  40%|████      | 3747/9281 [53:22<1:18:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  40%|████      | 3748/9281 [53:23<1:18:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  40%|████      | 3749/9281 [53:24<1:18:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  40%|████      | 3750/9281 [53:25<1:18:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  40%|████      | 3751/9281 [53:26<1:18:53,  1.17it/s]\u001b[A\n",
      "Training loss: 5.17e-01:  40%|████      | 3752/9281 [53:27<1:19:05,  1.17it/s]\u001b[A\n",
      "Training loss: 5.08e-01:  40%|████      | 3753/9281 [53:27<1:18:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.88e-01:  40%|████      | 3754/9281 [53:28<1:18:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  40%|████      | 3755/9281 [53:29<1:18:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  40%|████      | 3756/9281 [53:30<1:18:49,  1.17it/s]\u001b[A\n",
      "Training loss: 5.02e-01:  40%|████      | 3757/9281 [53:31<1:18:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.87e-01:  40%|████      | 3758/9281 [53:32<1:18:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  41%|████      | 3759/9281 [53:33<1:18:13,  1.18it/s]\u001b[A\n",
      "Training loss: 5.21e-01:  41%|████      | 3760/9281 [53:33<1:18:11,  1.18it/s]\u001b[A\n",
      "Training loss: 4.98e-01:  41%|████      | 3761/9281 [53:34<1:18:11,  1.18it/s]\u001b[A\n",
      "Training loss: 5.16e-01:  41%|████      | 3762/9281 [53:35<1:18:14,  1.18it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  41%|████      | 3763/9281 [53:36<1:18:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  41%|████      | 3764/9281 [53:37<1:18:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.82e-01:  41%|████      | 3765/9281 [53:38<1:18:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.86e-01:  41%|████      | 3766/9281 [53:39<1:18:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  41%|████      | 3767/9281 [53:39<1:18:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  41%|████      | 3768/9281 [53:40<1:18:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  41%|████      | 3769/9281 [53:41<1:18:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  41%|████      | 3770/9281 [53:42<1:18:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  41%|████      | 3771/9281 [53:43<1:18:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  41%|████      | 3772/9281 [53:44<1:18:57,  1.16it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  41%|████      | 3773/9281 [53:45<1:19:02,  1.16it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  41%|████      | 3774/9281 [53:45<1:18:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  41%|████      | 3775/9281 [53:46<1:18:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  41%|████      | 3776/9281 [53:47<1:18:17,  1.17it/s]\u001b[A\n",
      "Training loss: 5.33e-01:  41%|████      | 3777/9281 [53:48<1:18:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.79e-01:  41%|████      | 3778/9281 [53:49<1:18:05,  1.17it/s]\u001b[A\n",
      "Training loss: 5.14e-01:  41%|████      | 3779/9281 [53:50<1:18:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  41%|████      | 3780/9281 [53:51<1:18:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  41%|████      | 3781/9281 [53:51<1:18:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.98e-01:  41%|████      | 3782/9281 [53:52<1:18:12,  1.17it/s]\u001b[A\n",
      "Training loss: 5.30e-01:  41%|████      | 3783/9281 [53:53<1:18:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  41%|████      | 3784/9281 [53:54<1:18:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  41%|████      | 3785/9281 [53:55<1:18:34,  1.17it/s]\u001b[A\n",
      "Training loss: 5.66e-01:  41%|████      | 3786/9281 [53:56<1:18:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  41%|████      | 3787/9281 [53:57<1:18:21,  1.17it/s]\u001b[A\n",
      "Training loss: 5.26e-01:  41%|████      | 3788/9281 [53:57<1:18:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  41%|████      | 3789/9281 [53:58<1:18:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.69e-01:  41%|████      | 3790/9281 [53:59<1:18:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  41%|████      | 3791/9281 [54:00<1:18:24,  1.17it/s]\u001b[A\n",
      "Training loss: 5.18e-01:  41%|████      | 3792/9281 [54:01<1:18:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  41%|████      | 3793/9281 [54:02<1:18:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  41%|████      | 3794/9281 [54:03<1:18:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.81e-01:  41%|████      | 3795/9281 [54:03<1:18:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  41%|████      | 3796/9281 [54:04<1:18:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  41%|████      | 3797/9281 [54:05<1:18:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  41%|████      | 3798/9281 [54:06<1:17:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  41%|████      | 3799/9281 [54:07<1:17:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  41%|████      | 3800/9281 [54:08<1:17:34,  1.18it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  41%|████      | 3801/9281 [54:08<1:17:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  41%|████      | 3802/9281 [54:09<1:17:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  41%|████      | 3803/9281 [54:10<1:18:05,  1.17it/s]\u001b[A\n",
      "Training loss: 5.61e-01:  41%|████      | 3804/9281 [54:11<1:17:58,  1.17it/s]\u001b[A\n",
      "Training loss: 5.53e-01:  41%|████      | 3805/9281 [54:12<1:18:13,  1.17it/s]\u001b[A\n",
      "Training loss: 6.09e-01:  41%|████      | 3806/9281 [54:13<1:18:14,  1.17it/s]\u001b[A\n",
      "Training loss: 5.38e-01:  41%|████      | 3807/9281 [54:14<1:17:55,  1.17it/s]\u001b[A\n",
      "Training loss: 5.33e-01:  41%|████      | 3808/9281 [54:14<1:17:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.84e-01:  41%|████      | 3809/9281 [54:15<1:17:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  41%|████      | 3810/9281 [54:16<1:17:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  41%|████      | 3811/9281 [54:17<1:17:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  41%|████      | 3812/9281 [54:18<1:17:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  41%|████      | 3813/9281 [54:19<1:17:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  41%|████      | 3814/9281 [54:20<1:18:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  41%|████      | 3815/9281 [54:20<1:17:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  41%|████      | 3816/9281 [54:21<1:18:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  41%|████      | 3817/9281 [54:22<1:17:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  41%|████      | 3818/9281 [54:23<1:17:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  41%|████      | 3819/9281 [54:24<1:17:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  41%|████      | 3820/9281 [54:25<1:17:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  41%|████      | 3821/9281 [54:26<1:17:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  41%|████      | 3822/9281 [54:26<1:17:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  41%|████      | 3823/9281 [54:27<1:17:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  41%|████      | 3824/9281 [54:28<1:18:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  41%|████      | 3825/9281 [54:29<1:17:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  41%|████      | 3826/9281 [54:30<1:17:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  41%|████      | 3827/9281 [54:31<1:17:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  41%|████      | 3828/9281 [54:32<1:17:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.87e-01:  41%|████▏     | 3829/9281 [54:32<1:17:44,  1.17it/s]\u001b[A\n",
      "Training loss: 5.10e-01:  41%|████▏     | 3830/9281 [54:33<1:17:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  41%|████▏     | 3831/9281 [54:34<1:18:03,  1.16it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  41%|████▏     | 3832/9281 [54:35<1:17:59,  1.16it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  41%|████▏     | 3833/9281 [54:36<1:18:05,  1.16it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  41%|████▏     | 3834/9281 [54:37<1:17:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  41%|████▏     | 3835/9281 [54:38<1:17:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.86e-01:  41%|████▏     | 3836/9281 [54:38<1:17:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  41%|████▏     | 3837/9281 [54:39<1:17:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.96e-01:  41%|████▏     | 3838/9281 [54:40<1:17:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.82e-01:  41%|████▏     | 3839/9281 [54:41<1:17:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  41%|████▏     | 3840/9281 [54:42<1:17:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  41%|████▏     | 3841/9281 [54:43<1:17:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  41%|████▏     | 3842/9281 [54:44<1:17:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  41%|████▏     | 3843/9281 [54:44<1:17:48,  1.16it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  41%|████▏     | 3844/9281 [54:45<1:17:41,  1.17it/s]\u001b[A\n",
      "Training loss: 5.14e-01:  41%|████▏     | 3845/9281 [54:46<1:17:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  41%|████▏     | 3846/9281 [54:47<1:17:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  41%|████▏     | 3847/9281 [54:48<1:17:27,  1.17it/s]\u001b[A\n",
      "Training loss: 5.12e-01:  41%|████▏     | 3848/9281 [54:49<1:17:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  41%|████▏     | 3849/9281 [54:50<1:17:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.95e-01:  41%|████▏     | 3850/9281 [54:50<1:17:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.87e-01:  41%|████▏     | 3851/9281 [54:51<1:17:23,  1.17it/s]\u001b[A\n",
      "Training loss: 5.17e-01:  42%|████▏     | 3852/9281 [54:52<1:17:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  42%|████▏     | 3853/9281 [54:53<1:17:16,  1.17it/s]\u001b[A\n",
      "Training loss: 5.34e-01:  42%|████▏     | 3854/9281 [54:54<1:17:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  42%|████▏     | 3855/9281 [54:55<1:17:12,  1.17it/s]\u001b[A\n",
      "Training loss: 5.17e-01:  42%|████▏     | 3856/9281 [54:56<1:17:09,  1.17it/s]\u001b[A\n",
      "Training loss: 5.96e-01:  42%|████▏     | 3857/9281 [54:56<1:17:03,  1.17it/s]\u001b[A\n",
      "Training loss: 5.68e-01:  42%|████▏     | 3858/9281 [54:57<1:17:05,  1.17it/s]\u001b[A\n",
      "Training loss: 5.21e-01:  42%|████▏     | 3859/9281 [54:58<1:17:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.84e-01:  42%|████▏     | 3860/9281 [54:59<1:16:51,  1.18it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  42%|████▏     | 3861/9281 [55:00<1:16:49,  1.18it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  42%|████▏     | 3862/9281 [55:01<1:16:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  42%|████▏     | 3863/9281 [55:01<1:16:50,  1.18it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  42%|████▏     | 3864/9281 [55:02<1:16:58,  1.17it/s]\u001b[A\n",
      "Training loss: 5.10e-01:  42%|████▏     | 3865/9281 [55:03<1:17:09,  1.17it/s]\u001b[A\n",
      "Training loss: 5.44e-01:  42%|████▏     | 3866/9281 [55:04<1:17:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  42%|████▏     | 3867/9281 [55:05<1:16:50,  1.17it/s]\u001b[A\n",
      "Training loss: 5.31e-01:  42%|████▏     | 3868/9281 [55:06<1:16:39,  1.18it/s]\u001b[A\n",
      "Training loss: 5.39e-01:  42%|████▏     | 3869/9281 [55:07<1:16:37,  1.18it/s]\u001b[A\n",
      "Training loss: 5.40e-01:  42%|████▏     | 3870/9281 [55:07<1:16:19,  1.18it/s]\u001b[A\n",
      "Training loss: 5.44e-01:  42%|████▏     | 3871/9281 [55:08<1:16:51,  1.17it/s]\u001b[A\n",
      "Training loss: 5.40e-01:  42%|████▏     | 3872/9281 [55:09<1:16:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.97e-01:  42%|████▏     | 3873/9281 [55:10<1:17:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  42%|████▏     | 3874/9281 [55:11<1:17:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  42%|████▏     | 3875/9281 [55:12<1:17:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  42%|████▏     | 3876/9281 [55:13<1:17:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  42%|████▏     | 3877/9281 [55:13<1:17:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  42%|████▏     | 3878/9281 [55:14<1:17:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  42%|████▏     | 3879/9281 [55:15<1:17:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.98e-01:  42%|████▏     | 3880/9281 [55:16<1:16:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  42%|████▏     | 3881/9281 [55:17<1:16:54,  1.17it/s]\u001b[A\n",
      "Training loss: 5.10e-01:  42%|████▏     | 3882/9281 [55:18<1:16:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.92e-01:  42%|████▏     | 3883/9281 [55:19<1:16:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  42%|████▏     | 3884/9281 [55:19<1:16:51,  1.17it/s]\u001b[A\n",
      "Training loss: 5.06e-01:  42%|████▏     | 3885/9281 [55:20<1:16:37,  1.17it/s]\u001b[A\n",
      "Training loss: 5.89e-01:  42%|████▏     | 3886/9281 [55:21<1:16:39,  1.17it/s]\u001b[A\n",
      "Training loss: 5.92e-01:  42%|████▏     | 3887/9281 [55:22<1:16:51,  1.17it/s]\u001b[A\n",
      "Training loss: 6.17e-01:  42%|████▏     | 3888/9281 [55:23<1:16:41,  1.17it/s]\u001b[A\n",
      "Training loss: 5.67e-01:  42%|████▏     | 3889/9281 [55:24<1:16:39,  1.17it/s]\u001b[A\n",
      "Training loss: 5.59e-01:  42%|████▏     | 3890/9281 [55:25<1:16:44,  1.17it/s]\u001b[A\n",
      "Training loss: 5.64e-01:  42%|████▏     | 3891/9281 [55:25<1:16:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.97e-01:  42%|████▏     | 3892/9281 [55:26<1:16:32,  1.17it/s]\u001b[A\n",
      "Training loss: 5.16e-01:  42%|████▏     | 3893/9281 [55:27<1:16:38,  1.17it/s]\u001b[A\n",
      "Training loss: 5.04e-01:  42%|████▏     | 3894/9281 [55:28<1:16:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  42%|████▏     | 3895/9281 [55:29<1:16:36,  1.17it/s]\u001b[A\n",
      "Training loss: 5.01e-01:  42%|████▏     | 3896/9281 [55:30<1:16:37,  1.17it/s]\u001b[A\n",
      "Training loss: 5.06e-01:  42%|████▏     | 3897/9281 [55:31<1:16:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  42%|████▏     | 3898/9281 [55:31<1:16:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  42%|████▏     | 3899/9281 [55:32<1:16:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  42%|████▏     | 3900/9281 [55:33<1:16:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  42%|████▏     | 3901/9281 [55:34<1:16:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  42%|████▏     | 3902/9281 [55:35<1:16:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  42%|████▏     | 3903/9281 [55:36<1:16:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  42%|████▏     | 3904/9281 [55:36<1:16:09,  1.18it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  42%|████▏     | 3905/9281 [55:37<1:16:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  42%|████▏     | 3906/9281 [55:38<1:16:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  42%|████▏     | 3907/9281 [55:39<1:16:09,  1.18it/s]\u001b[A\n",
      "Training loss: 5.10e-01:  42%|████▏     | 3908/9281 [55:40<1:16:10,  1.18it/s]\u001b[A\n",
      "Training loss: 5.36e-01:  42%|████▏     | 3909/9281 [55:41<1:16:34,  1.17it/s]\u001b[A\n",
      "Training loss: 5.15e-01:  42%|████▏     | 3910/9281 [55:42<1:16:26,  1.17it/s]\u001b[A\n",
      "Training loss: 5.10e-01:  42%|████▏     | 3911/9281 [55:42<1:16:34,  1.17it/s]\u001b[A\n",
      "Training loss: 5.38e-01:  42%|████▏     | 3912/9281 [55:43<1:16:24,  1.17it/s]\u001b[A\n",
      "Training loss: 5.51e-01:  42%|████▏     | 3913/9281 [55:44<1:16:31,  1.17it/s]\u001b[A\n",
      "Training loss: 5.33e-01:  42%|████▏     | 3914/9281 [55:45<1:16:21,  1.17it/s]\u001b[A\n",
      "Training loss: 5.21e-01:  42%|████▏     | 3915/9281 [55:46<1:16:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  42%|████▏     | 3916/9281 [55:47<1:16:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  42%|████▏     | 3917/9281 [55:48<1:16:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.92e-01:  42%|████▏     | 3918/9281 [55:48<1:16:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  42%|████▏     | 3919/9281 [55:49<1:16:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  42%|████▏     | 3920/9281 [55:50<1:16:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  42%|████▏     | 3921/9281 [55:51<1:16:21,  1.17it/s]\u001b[A\n",
      "Training loss: 5.18e-01:  42%|████▏     | 3922/9281 [55:52<1:16:22,  1.17it/s]\u001b[A\n",
      "Training loss: 5.52e-01:  42%|████▏     | 3923/9281 [55:53<1:16:12,  1.17it/s]\u001b[A\n",
      "Training loss: 5.02e-01:  42%|████▏     | 3924/9281 [55:54<1:16:11,  1.17it/s]\u001b[A\n",
      "Training loss: 5.17e-01:  42%|████▏     | 3925/9281 [55:54<1:16:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  42%|████▏     | 3926/9281 [55:55<1:15:52,  1.18it/s]\u001b[A\n",
      "Training loss: 5.34e-01:  42%|████▏     | 3927/9281 [55:56<1:15:56,  1.18it/s]\u001b[A\n",
      "Training loss: 5.20e-01:  42%|████▏     | 3928/9281 [55:57<1:15:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.99e-01:  42%|████▏     | 3929/9281 [55:58<1:15:50,  1.18it/s]\u001b[A\n",
      "Training loss: 4.87e-01:  42%|████▏     | 3930/9281 [55:59<1:15:45,  1.18it/s]\u001b[A\n",
      "Training loss: 5.13e-01:  42%|████▏     | 3931/9281 [56:00<1:16:14,  1.17it/s]\u001b[A\n",
      "Training loss: 5.05e-01:  42%|████▏     | 3932/9281 [56:00<1:16:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  42%|████▏     | 3933/9281 [56:01<1:16:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  42%|████▏     | 3934/9281 [56:02<1:16:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  42%|████▏     | 3935/9281 [56:03<1:16:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  42%|████▏     | 3936/9281 [56:04<1:16:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  42%|████▏     | 3937/9281 [56:05<1:16:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  42%|████▏     | 3938/9281 [56:06<1:16:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  42%|████▏     | 3939/9281 [56:06<1:16:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  42%|████▏     | 3940/9281 [56:07<1:16:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  42%|████▏     | 3941/9281 [56:08<1:16:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  42%|████▏     | 3942/9281 [56:09<1:16:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.88e-01:  42%|████▏     | 3943/9281 [56:10<1:16:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  42%|████▏     | 3944/9281 [56:11<1:16:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  43%|████▎     | 3945/9281 [56:12<1:16:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  43%|████▎     | 3946/9281 [56:12<1:16:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  43%|████▎     | 3947/9281 [56:13<1:16:18,  1.16it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  43%|████▎     | 3948/9281 [56:14<1:15:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  43%|████▎     | 3949/9281 [56:15<1:16:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  43%|████▎     | 3950/9281 [56:16<1:16:18,  1.16it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  43%|████▎     | 3951/9281 [56:17<1:16:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  43%|████▎     | 3952/9281 [56:18<1:16:14,  1.16it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  43%|████▎     | 3953/9281 [56:18<1:16:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  43%|████▎     | 3954/9281 [56:19<1:16:16,  1.16it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  43%|████▎     | 3955/9281 [56:20<1:15:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  43%|████▎     | 3956/9281 [56:21<1:15:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.69e-01:  43%|████▎     | 3957/9281 [56:22<1:15:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  43%|████▎     | 3958/9281 [56:23<1:15:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  43%|████▎     | 3959/9281 [56:23<1:15:39,  1.17it/s]\u001b[A\n",
      "Training loss: 5.48e-01:  43%|████▎     | 3960/9281 [56:24<1:15:28,  1.18it/s]\u001b[A\n",
      "Training loss: 5.39e-01:  43%|████▎     | 3961/9281 [56:25<1:15:33,  1.17it/s]\u001b[A\n",
      "Training loss: 5.24e-01:  43%|████▎     | 3962/9281 [56:26<1:15:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  43%|████▎     | 3963/9281 [56:27<1:15:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  43%|████▎     | 3964/9281 [56:28<1:16:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  43%|████▎     | 3965/9281 [56:29<1:15:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.84e-01:  43%|████▎     | 3966/9281 [56:29<1:15:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  43%|████▎     | 3967/9281 [56:30<1:15:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  43%|████▎     | 3968/9281 [56:31<1:15:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  43%|████▎     | 3969/9281 [56:32<1:16:07,  1.16it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  43%|████▎     | 3970/9281 [56:33<1:15:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  43%|████▎     | 3971/9281 [56:34<1:15:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  43%|████▎     | 3972/9281 [56:35<1:15:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  43%|████▎     | 3973/9281 [56:35<1:15:51,  1.17it/s]\u001b[A\n",
      "Training loss: 5.03e-01:  43%|████▎     | 3974/9281 [56:36<1:15:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  43%|████▎     | 3975/9281 [56:37<1:15:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  43%|████▎     | 3976/9281 [56:38<1:15:12,  1.18it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  43%|████▎     | 3977/9281 [56:39<1:15:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  43%|████▎     | 3978/9281 [56:40<1:15:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  43%|████▎     | 3979/9281 [56:41<1:15:24,  1.17it/s]\u001b[A\n",
      "Training loss: 5.13e-01:  43%|████▎     | 3980/9281 [56:41<1:15:19,  1.17it/s]\u001b[A\n",
      "Training loss: 5.01e-01:  43%|████▎     | 3981/9281 [56:42<1:15:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  43%|████▎     | 3982/9281 [56:43<1:15:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  43%|████▎     | 3983/9281 [56:44<1:15:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.97e-01:  43%|████▎     | 3984/9281 [56:45<1:15:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  43%|████▎     | 3985/9281 [56:46<1:15:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  43%|████▎     | 3986/9281 [56:47<1:15:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  43%|████▎     | 3987/9281 [56:47<1:15:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  43%|████▎     | 3988/9281 [56:48<1:15:26,  1.17it/s]\u001b[A\n",
      "Training loss: 5.26e-01:  43%|████▎     | 3989/9281 [56:49<1:15:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  43%|████▎     | 3990/9281 [56:50<1:15:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  43%|████▎     | 3991/9281 [56:51<1:15:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  43%|████▎     | 3992/9281 [56:52<1:15:19,  1.17it/s]\u001b[A\n",
      "Training loss: 5.15e-01:  43%|████▎     | 3993/9281 [56:53<1:15:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.69e-01:  43%|████▎     | 3994/9281 [56:53<1:15:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.95e-01:  43%|████▎     | 3995/9281 [56:54<1:15:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.68e-01:  43%|████▎     | 3996/9281 [56:55<1:15:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.67e-01:  43%|████▎     | 3997/9281 [56:56<1:15:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.98e-01:  43%|████▎     | 3998/9281 [56:57<1:15:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  43%|████▎     | 3999/9281 [56:58<1:15:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  43%|████▎     | 4000/9281 [56:59<1:15:16,  1.17it/s]\u001b[A\n",
      "Training loss: 5.16e-01:  43%|████▎     | 4001/9281 [56:59<1:15:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  43%|████▎     | 4002/9281 [57:00<1:15:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  43%|████▎     | 4003/9281 [57:01<1:15:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  43%|████▎     | 4004/9281 [57:02<1:15:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.99e-01:  43%|████▎     | 4005/9281 [57:03<1:15:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  43%|████▎     | 4006/9281 [57:04<1:15:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  43%|████▎     | 4007/9281 [57:05<1:15:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  43%|████▎     | 4008/9281 [57:05<1:15:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  43%|████▎     | 4009/9281 [57:06<1:15:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  43%|████▎     | 4010/9281 [57:07<1:15:26,  1.16it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  43%|████▎     | 4011/9281 [57:08<1:15:28,  1.16it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  43%|████▎     | 4012/9281 [57:09<1:15:33,  1.16it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  43%|████▎     | 4013/9281 [57:10<1:15:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  43%|████▎     | 4014/9281 [57:11<1:15:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  43%|████▎     | 4015/9281 [57:11<1:14:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  43%|████▎     | 4016/9281 [57:12<1:14:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  43%|████▎     | 4017/9281 [57:13<1:14:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  43%|████▎     | 4018/9281 [57:14<1:15:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  43%|████▎     | 4019/9281 [57:15<1:15:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  43%|████▎     | 4020/9281 [57:16<1:14:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  43%|████▎     | 4021/9281 [57:17<1:14:52,  1.17it/s]\u001b[A\n",
      "Training loss: 5.44e-01:  43%|████▎     | 4022/9281 [57:17<1:15:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.79e-01:  43%|████▎     | 4023/9281 [57:18<1:14:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  43%|████▎     | 4024/9281 [57:19<1:14:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.87e-01:  43%|████▎     | 4025/9281 [57:20<1:14:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.98e-01:  43%|████▎     | 4026/9281 [57:21<1:14:59,  1.17it/s]\u001b[A\n",
      "Training loss: 5.29e-01:  43%|████▎     | 4027/9281 [57:22<1:14:44,  1.17it/s]\u001b[A\n",
      "Training loss: 5.30e-01:  43%|████▎     | 4028/9281 [57:22<1:14:48,  1.17it/s]\u001b[A\n",
      "Training loss: 5.00e-01:  43%|████▎     | 4029/9281 [57:23<1:14:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  43%|████▎     | 4030/9281 [57:24<1:14:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.69e-01:  43%|████▎     | 4031/9281 [57:25<1:14:41,  1.17it/s]\u001b[A\n",
      "Training loss: 5.28e-01:  43%|████▎     | 4032/9281 [57:26<1:14:41,  1.17it/s]\u001b[A\n",
      "Training loss: 5.52e-01:  43%|████▎     | 4033/9281 [57:27<1:15:06,  1.16it/s]\u001b[A\n",
      "Training loss: 5.24e-01:  43%|████▎     | 4034/9281 [57:28<1:14:46,  1.17it/s]\u001b[A\n",
      "Training loss: 5.98e-01:  43%|████▎     | 4035/9281 [57:28<1:14:51,  1.17it/s]\u001b[A\n",
      "Training loss: 6.00e-01:  43%|████▎     | 4036/9281 [57:29<1:14:43,  1.17it/s]\u001b[A\n",
      "Training loss: 6.02e-01:  43%|████▎     | 4037/9281 [57:30<1:14:38,  1.17it/s]\u001b[A\n",
      "Training loss: 5.37e-01:  44%|████▎     | 4038/9281 [57:31<1:14:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  44%|████▎     | 4039/9281 [57:32<1:14:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  44%|████▎     | 4040/9281 [57:33<1:14:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  44%|████▎     | 4041/9281 [57:34<1:14:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  44%|████▎     | 4042/9281 [57:34<1:14:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  44%|████▎     | 4043/9281 [57:35<1:14:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.88e-01:  44%|████▎     | 4044/9281 [57:36<1:14:21,  1.17it/s]\u001b[A\n",
      "Training loss: 5.28e-01:  44%|████▎     | 4045/9281 [57:37<1:14:30,  1.17it/s]\u001b[A\n",
      "Training loss: 5.24e-01:  44%|████▎     | 4046/9281 [57:38<1:14:26,  1.17it/s]\u001b[A\n",
      "Training loss: 5.33e-01:  44%|████▎     | 4047/9281 [57:39<1:14:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  44%|████▎     | 4048/9281 [57:40<1:14:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  44%|████▎     | 4049/9281 [57:40<1:14:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  44%|████▎     | 4050/9281 [57:41<1:14:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  44%|████▎     | 4051/9281 [57:42<1:14:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  44%|████▎     | 4052/9281 [57:43<1:14:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  44%|████▎     | 4053/9281 [57:44<1:14:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  44%|████▎     | 4054/9281 [57:45<1:14:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  44%|████▎     | 4055/9281 [57:46<1:14:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  44%|████▎     | 4056/9281 [57:46<1:14:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  44%|████▎     | 4057/9281 [57:47<1:14:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.79e-01:  44%|████▎     | 4058/9281 [57:48<1:14:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.83e-01:  44%|████▎     | 4059/9281 [57:49<1:13:59,  1.18it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  44%|████▎     | 4060/9281 [57:50<1:14:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.68e-01:  44%|████▍     | 4061/9281 [57:51<1:14:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  44%|████▍     | 4062/9281 [57:52<1:14:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  44%|████▍     | 4063/9281 [57:52<1:14:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  44%|████▍     | 4064/9281 [57:53<1:14:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  44%|████▍     | 4065/9281 [57:54<1:14:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  44%|████▍     | 4066/9281 [57:55<1:14:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  44%|████▍     | 4067/9281 [57:56<1:14:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  44%|████▍     | 4068/9281 [57:57<1:13:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  44%|████▍     | 4069/9281 [57:57<1:13:48,  1.18it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  44%|████▍     | 4070/9281 [57:58<1:14:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  44%|████▍     | 4071/9281 [57:59<1:14:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  44%|████▍     | 4072/9281 [58:00<1:14:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  44%|████▍     | 4073/9281 [58:01<1:14:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  44%|████▍     | 4074/9281 [58:02<1:14:42,  1.16it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  44%|████▍     | 4075/9281 [58:03<1:14:30,  1.16it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  44%|████▍     | 4076/9281 [58:04<1:14:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  44%|████▍     | 4077/9281 [58:04<1:14:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  44%|████▍     | 4078/9281 [58:05<1:13:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.83e-01:  44%|████▍     | 4079/9281 [58:06<1:13:43,  1.18it/s]\u001b[A\n",
      "Training loss: 5.99e-01:  44%|████▍     | 4080/9281 [58:07<1:13:43,  1.18it/s]\u001b[A\n",
      "Training loss: 5.36e-01:  44%|████▍     | 4081/9281 [58:08<1:13:43,  1.18it/s]\u001b[A\n",
      "Training loss: 5.03e-01:  44%|████▍     | 4082/9281 [58:09<1:13:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.96e-01:  44%|████▍     | 4083/9281 [58:09<1:13:57,  1.17it/s]\u001b[A\n",
      "Training loss: 5.00e-01:  44%|████▍     | 4084/9281 [58:10<1:13:52,  1.17it/s]\u001b[A\n",
      "Training loss: 5.18e-01:  44%|████▍     | 4085/9281 [58:11<1:14:01,  1.17it/s]\u001b[A\n",
      "Training loss: 5.60e-01:  44%|████▍     | 4086/9281 [58:12<1:13:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  44%|████▍     | 4087/9281 [58:13<1:13:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.83e-01:  44%|████▍     | 4088/9281 [58:14<1:13:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  44%|████▍     | 4089/9281 [58:15<1:13:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.79e-01:  44%|████▍     | 4090/9281 [58:15<1:13:35,  1.18it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  44%|████▍     | 4091/9281 [58:16<1:13:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  44%|████▍     | 4092/9281 [58:17<1:13:31,  1.18it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  44%|████▍     | 4093/9281 [58:18<1:13:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  44%|████▍     | 4094/9281 [58:19<1:13:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  44%|████▍     | 4095/9281 [58:20<1:14:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  44%|████▍     | 4096/9281 [58:21<1:14:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  44%|████▍     | 4097/9281 [58:21<1:14:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  44%|████▍     | 4098/9281 [58:22<1:13:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.87e-01:  44%|████▍     | 4099/9281 [58:23<1:13:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  44%|████▍     | 4100/9281 [58:24<1:13:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  44%|████▍     | 4101/9281 [58:25<1:13:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  44%|████▍     | 4102/9281 [58:26<1:13:27,  1.18it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  44%|████▍     | 4103/9281 [58:27<1:13:26,  1.18it/s]\u001b[A\n",
      "Training loss: 5.08e-01:  44%|████▍     | 4104/9281 [58:27<1:13:25,  1.18it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  44%|████▍     | 4105/9281 [58:28<1:13:21,  1.18it/s]\u001b[A\n",
      "Training loss: 4.81e-01:  44%|████▍     | 4106/9281 [58:29<1:13:23,  1.18it/s]\u001b[A\n",
      "Training loss: 5.04e-01:  44%|████▍     | 4107/9281 [58:30<1:13:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  44%|████▍     | 4108/9281 [58:31<1:13:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  44%|████▍     | 4109/9281 [58:32<1:13:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  44%|████▍     | 4110/9281 [58:33<1:13:40,  1.17it/s]\u001b[A\n",
      "Training loss: 5.04e-01:  44%|████▍     | 4111/9281 [58:33<1:13:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.95e-01:  44%|████▍     | 4112/9281 [58:34<1:13:32,  1.17it/s]\u001b[A\n",
      "Training loss: 5.26e-01:  44%|████▍     | 4113/9281 [58:35<1:13:26,  1.17it/s]\u001b[A\n",
      "Training loss: 5.14e-01:  44%|████▍     | 4114/9281 [58:36<1:13:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  44%|████▍     | 4115/9281 [58:37<1:13:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  44%|████▍     | 4116/9281 [58:38<1:13:47,  1.17it/s]\u001b[A\n",
      "Training loss: 5.00e-01:  44%|████▍     | 4117/9281 [58:38<1:13:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  44%|████▍     | 4118/9281 [58:39<1:13:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  44%|████▍     | 4119/9281 [58:40<1:13:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  44%|████▍     | 4120/9281 [58:41<1:13:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  44%|████▍     | 4121/9281 [58:42<1:13:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  44%|████▍     | 4122/9281 [58:43<1:13:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.81e-01:  44%|████▍     | 4123/9281 [58:44<1:13:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  44%|████▍     | 4124/9281 [58:44<1:13:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  44%|████▍     | 4125/9281 [58:45<1:13:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  44%|████▍     | 4126/9281 [58:46<1:13:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  44%|████▍     | 4127/9281 [58:47<1:13:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  44%|████▍     | 4128/9281 [58:48<1:13:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  44%|████▍     | 4129/9281 [58:49<1:13:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  44%|████▍     | 4130/9281 [58:50<1:13:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  45%|████▍     | 4131/9281 [58:50<1:13:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  45%|████▍     | 4132/9281 [58:51<1:13:32,  1.17it/s]\u001b[A\n",
      "Training loss: 5.58e-01:  45%|████▍     | 4133/9281 [58:52<1:13:20,  1.17it/s]\u001b[A\n",
      "Training loss: 5.08e-01:  45%|████▍     | 4134/9281 [58:53<1:13:13,  1.17it/s]\u001b[A\n",
      "Training loss: 5.50e-01:  45%|████▍     | 4135/9281 [58:54<1:13:23,  1.17it/s]\u001b[A\n",
      "Training loss: 5.16e-01:  45%|████▍     | 4136/9281 [58:55<1:13:33,  1.17it/s]\u001b[A\n",
      "Training loss: 5.05e-01:  45%|████▍     | 4137/9281 [58:56<1:13:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.69e-01:  45%|████▍     | 4138/9281 [58:56<1:13:36,  1.16it/s]\u001b[A\n",
      "Training loss: 5.07e-01:  45%|████▍     | 4139/9281 [58:57<1:13:21,  1.17it/s]\u001b[A\n",
      "Training loss: 5.12e-01:  45%|████▍     | 4140/9281 [58:58<1:13:27,  1.17it/s]\u001b[A\n",
      "Training loss: 5.06e-01:  45%|████▍     | 4141/9281 [58:59<1:13:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.84e-01:  45%|████▍     | 4142/9281 [59:00<1:13:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.98e-01:  45%|████▍     | 4143/9281 [59:01<1:13:06,  1.17it/s]\u001b[A\n",
      "Training loss: 5.28e-01:  45%|████▍     | 4144/9281 [59:02<1:13:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  45%|████▍     | 4145/9281 [59:02<1:12:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  45%|████▍     | 4146/9281 [59:03<1:12:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  45%|████▍     | 4147/9281 [59:04<1:13:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  45%|████▍     | 4148/9281 [59:05<1:13:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  45%|████▍     | 4149/9281 [59:06<1:13:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  45%|████▍     | 4150/9281 [59:07<1:13:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  45%|████▍     | 4151/9281 [59:08<1:13:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  45%|████▍     | 4152/9281 [59:08<1:13:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  45%|████▍     | 4153/9281 [59:09<1:12:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  45%|████▍     | 4154/9281 [59:10<1:13:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  45%|████▍     | 4155/9281 [59:11<1:13:25,  1.16it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  45%|████▍     | 4156/9281 [59:12<1:13:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  45%|████▍     | 4157/9281 [59:13<1:13:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.68e-01:  45%|████▍     | 4158/9281 [59:14<1:13:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  45%|████▍     | 4159/9281 [59:14<1:13:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  45%|████▍     | 4160/9281 [59:15<1:12:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  45%|████▍     | 4161/9281 [59:16<1:13:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  45%|████▍     | 4162/9281 [59:17<1:12:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  45%|████▍     | 4163/9281 [59:18<1:12:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  45%|████▍     | 4164/9281 [59:19<1:13:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  45%|████▍     | 4165/9281 [59:20<1:12:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  45%|████▍     | 4166/9281 [59:20<1:12:43,  1.17it/s]\u001b[A\n",
      "Training loss: 5.28e-01:  45%|████▍     | 4167/9281 [59:21<1:12:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  45%|████▍     | 4168/9281 [59:22<1:12:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  45%|████▍     | 4169/9281 [59:23<1:13:09,  1.16it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  45%|████▍     | 4170/9281 [59:24<1:13:11,  1.16it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  45%|████▍     | 4171/9281 [59:25<1:13:13,  1.16it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  45%|████▍     | 4172/9281 [59:26<1:13:17,  1.16it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  45%|████▍     | 4173/9281 [59:26<1:12:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  45%|████▍     | 4174/9281 [59:27<1:13:16,  1.16it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  45%|████▍     | 4175/9281 [59:28<1:13:10,  1.16it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  45%|████▍     | 4176/9281 [59:29<1:13:06,  1.16it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  45%|████▌     | 4177/9281 [59:30<1:12:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  45%|████▌     | 4178/9281 [59:31<1:12:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  45%|████▌     | 4179/9281 [59:32<1:12:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  45%|████▌     | 4180/9281 [59:32<1:12:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  45%|████▌     | 4181/9281 [59:33<1:12:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  45%|████▌     | 4182/9281 [59:34<1:12:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  45%|████▌     | 4183/9281 [59:35<1:12:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  45%|████▌     | 4184/9281 [59:36<1:12:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  45%|████▌     | 4185/9281 [59:37<1:12:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  45%|████▌     | 4186/9281 [59:38<1:12:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  45%|████▌     | 4187/9281 [59:38<1:12:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  45%|████▌     | 4188/9281 [59:39<1:12:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  45%|████▌     | 4189/9281 [59:40<1:12:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  45%|████▌     | 4190/9281 [59:41<1:12:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.27e-01:  45%|████▌     | 4191/9281 [59:42<1:12:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.22e-01:  45%|████▌     | 4192/9281 [59:43<1:12:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  45%|████▌     | 4193/9281 [59:44<1:12:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  45%|████▌     | 4194/9281 [59:44<1:12:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  45%|████▌     | 4195/9281 [59:45<1:12:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  45%|████▌     | 4196/9281 [59:46<1:12:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  45%|████▌     | 4197/9281 [59:47<1:12:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  45%|████▌     | 4198/9281 [59:48<1:12:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  45%|████▌     | 4199/9281 [59:49<1:12:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  45%|████▌     | 4200/9281 [59:50<1:12:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  45%|████▌     | 4201/9281 [59:50<1:12:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  45%|████▌     | 4202/9281 [59:51<1:12:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  45%|████▌     | 4203/9281 [59:52<1:12:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  45%|████▌     | 4204/9281 [59:53<1:12:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  45%|████▌     | 4205/9281 [59:54<1:12:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  45%|████▌     | 4206/9281 [59:55<1:12:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  45%|████▌     | 4207/9281 [59:55<1:12:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  45%|████▌     | 4208/9281 [59:56<1:12:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  45%|████▌     | 4209/9281 [59:57<1:12:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  45%|████▌     | 4210/9281 [59:58<1:12:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  45%|████▌     | 4211/9281 [59:59<1:12:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  45%|████▌     | 4212/9281 [1:00:00<1:12:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  45%|████▌     | 4213/9281 [1:00:01<1:12:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  45%|████▌     | 4214/9281 [1:00:01<1:12:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  45%|████▌     | 4215/9281 [1:00:02<1:12:29,  1.16it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  45%|████▌     | 4216/9281 [1:00:03<1:12:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  45%|████▌     | 4217/9281 [1:00:04<1:12:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  45%|████▌     | 4218/9281 [1:00:05<1:12:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  45%|████▌     | 4219/9281 [1:00:06<1:12:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  45%|████▌     | 4220/9281 [1:00:07<1:12:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  45%|████▌     | 4221/9281 [1:00:07<1:12:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  45%|████▌     | 4222/9281 [1:00:08<1:12:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  46%|████▌     | 4223/9281 [1:00:09<1:12:11,  1.17it/s]\u001b[A\n",
      "Training loss: 5.35e-01:  46%|████▌     | 4224/9281 [1:00:10<1:12:29,  1.16it/s]\u001b[A\n",
      "Training loss: 4.83e-01:  46%|████▌     | 4225/9281 [1:00:11<1:12:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  46%|████▌     | 4226/9281 [1:00:12<1:12:22,  1.16it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  46%|████▌     | 4227/9281 [1:00:13<1:12:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  46%|████▌     | 4228/9281 [1:00:13<1:12:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  46%|████▌     | 4229/9281 [1:00:14<1:11:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  46%|████▌     | 4230/9281 [1:00:15<1:11:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  46%|████▌     | 4231/9281 [1:00:16<1:12:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  46%|████▌     | 4232/9281 [1:00:17<1:11:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  46%|████▌     | 4233/9281 [1:00:18<1:12:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  46%|████▌     | 4234/9281 [1:00:19<1:12:20,  1.16it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  46%|████▌     | 4235/9281 [1:00:19<1:12:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  46%|████▌     | 4236/9281 [1:00:20<1:11:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  46%|████▌     | 4237/9281 [1:00:21<1:12:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  46%|████▌     | 4238/9281 [1:00:22<1:11:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  46%|████▌     | 4239/9281 [1:00:23<1:11:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  46%|████▌     | 4240/9281 [1:00:24<1:11:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  46%|████▌     | 4241/9281 [1:00:25<1:11:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  46%|████▌     | 4242/9281 [1:00:25<1:11:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  46%|████▌     | 4243/9281 [1:00:26<1:11:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  46%|████▌     | 4244/9281 [1:00:27<1:11:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  46%|████▌     | 4245/9281 [1:00:28<1:11:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  46%|████▌     | 4246/9281 [1:00:29<1:11:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  46%|████▌     | 4247/9281 [1:00:30<1:11:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  46%|████▌     | 4248/9281 [1:00:31<1:11:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  46%|████▌     | 4249/9281 [1:00:31<1:11:47,  1.17it/s]\u001b[A\n",
      "Training loss: 5.03e-01:  46%|████▌     | 4250/9281 [1:00:32<1:11:38,  1.17it/s]\u001b[A\n",
      "Training loss: 5.04e-01:  46%|████▌     | 4251/9281 [1:00:33<1:11:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  46%|████▌     | 4252/9281 [1:00:34<1:11:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  46%|████▌     | 4253/9281 [1:00:35<1:11:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  46%|████▌     | 4254/9281 [1:00:36<1:11:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  46%|████▌     | 4255/9281 [1:00:37<1:11:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  46%|████▌     | 4256/9281 [1:00:37<1:11:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  46%|████▌     | 4257/9281 [1:00:38<1:11:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  46%|████▌     | 4258/9281 [1:00:39<1:11:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  46%|████▌     | 4259/9281 [1:00:40<1:11:45,  1.17it/s]\u001b[A\n",
      "Training loss: 5.13e-01:  46%|████▌     | 4260/9281 [1:00:41<1:11:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  46%|████▌     | 4261/9281 [1:00:42<1:11:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  46%|████▌     | 4262/9281 [1:00:43<1:11:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  46%|████▌     | 4263/9281 [1:00:43<1:11:22,  1.17it/s]\u001b[A\n",
      "Training loss: 5.46e-01:  46%|████▌     | 4264/9281 [1:00:44<1:11:27,  1.17it/s]\u001b[A\n",
      "Training loss: 5.29e-01:  46%|████▌     | 4265/9281 [1:00:45<1:11:34,  1.17it/s]\u001b[A\n",
      "Training loss: 5.17e-01:  46%|████▌     | 4266/9281 [1:00:46<1:11:39,  1.17it/s]\u001b[A\n",
      "Training loss: 5.35e-01:  46%|████▌     | 4267/9281 [1:00:47<1:11:27,  1.17it/s]\u001b[A\n",
      "Training loss: 5.27e-01:  46%|████▌     | 4268/9281 [1:00:48<1:11:30,  1.17it/s]\u001b[A\n",
      "Training loss: 5.41e-01:  46%|████▌     | 4269/9281 [1:00:49<1:11:39,  1.17it/s]\u001b[A\n",
      "Training loss: 5.18e-01:  46%|████▌     | 4270/9281 [1:00:49<1:11:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  46%|████▌     | 4271/9281 [1:00:50<1:11:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  46%|████▌     | 4272/9281 [1:00:51<1:11:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  46%|████▌     | 4273/9281 [1:00:52<1:11:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  46%|████▌     | 4274/9281 [1:00:53<1:11:40,  1.16it/s]\u001b[A\n",
      "Training loss: 5.08e-01:  46%|████▌     | 4275/9281 [1:00:54<1:11:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  46%|████▌     | 4276/9281 [1:00:55<1:11:38,  1.16it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  46%|████▌     | 4277/9281 [1:00:55<1:11:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  46%|████▌     | 4278/9281 [1:00:56<1:11:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.92e-01:  46%|████▌     | 4279/9281 [1:00:57<1:11:17,  1.17it/s]\u001b[A\n",
      "Training loss: 5.04e-01:  46%|████▌     | 4280/9281 [1:00:58<1:11:11,  1.17it/s]\u001b[A\n",
      "Training loss: 5.38e-01:  46%|████▌     | 4281/9281 [1:00:59<1:11:17,  1.17it/s]\u001b[A\n",
      "Training loss: 5.29e-01:  46%|████▌     | 4282/9281 [1:01:00<1:11:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  46%|████▌     | 4283/9281 [1:01:01<1:11:04,  1.17it/s]\u001b[A\n",
      "Training loss: 5.09e-01:  46%|████▌     | 4284/9281 [1:01:01<1:11:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  46%|████▌     | 4285/9281 [1:01:02<1:11:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  46%|████▌     | 4286/9281 [1:01:03<1:10:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  46%|████▌     | 4287/9281 [1:01:04<1:10:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  46%|████▌     | 4288/9281 [1:01:05<1:11:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  46%|████▌     | 4289/9281 [1:01:06<1:10:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  46%|████▌     | 4290/9281 [1:01:06<1:11:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  46%|████▌     | 4291/9281 [1:01:07<1:11:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  46%|████▌     | 4292/9281 [1:01:08<1:10:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  46%|████▋     | 4293/9281 [1:01:09<1:11:01,  1.17it/s]\u001b[A\n",
      "Training loss: 5.27e-01:  46%|████▋     | 4294/9281 [1:01:10<1:10:57,  1.17it/s]\u001b[A\n",
      "Training loss: 5.04e-01:  46%|████▋     | 4295/9281 [1:01:11<1:10:58,  1.17it/s]\u001b[A\n",
      "Training loss: 5.04e-01:  46%|████▋     | 4296/9281 [1:01:12<1:11:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.88e-01:  46%|████▋     | 4297/9281 [1:01:12<1:10:50,  1.17it/s]\u001b[A\n",
      "Training loss: 5.05e-01:  46%|████▋     | 4298/9281 [1:01:13<1:10:51,  1.17it/s]\u001b[A\n",
      "Training loss: 5.24e-01:  46%|████▋     | 4299/9281 [1:01:14<1:10:48,  1.17it/s]\u001b[A\n",
      "Training loss: 5.62e-01:  46%|████▋     | 4300/9281 [1:01:15<1:10:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  46%|████▋     | 4301/9281 [1:01:16<1:10:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  46%|████▋     | 4302/9281 [1:01:17<1:10:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  46%|████▋     | 4303/9281 [1:01:18<1:10:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  46%|████▋     | 4304/9281 [1:01:18<1:10:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  46%|████▋     | 4305/9281 [1:01:19<1:10:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  46%|████▋     | 4306/9281 [1:01:20<1:10:50,  1.17it/s]\u001b[A\n",
      "Training loss: 5.20e-01:  46%|████▋     | 4307/9281 [1:01:21<1:10:46,  1.17it/s]\u001b[A\n",
      "Training loss: 5.21e-01:  46%|████▋     | 4308/9281 [1:01:22<1:10:43,  1.17it/s]\u001b[A\n",
      "Training loss: 5.59e-01:  46%|████▋     | 4309/9281 [1:01:23<1:10:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.86e-01:  46%|████▋     | 4310/9281 [1:01:24<1:10:34,  1.17it/s]\u001b[A\n",
      "Training loss: 5.12e-01:  46%|████▋     | 4311/9281 [1:01:24<1:10:29,  1.18it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  46%|████▋     | 4312/9281 [1:01:25<1:10:31,  1.17it/s]\u001b[A\n",
      "Training loss: 5.19e-01:  46%|████▋     | 4313/9281 [1:01:26<1:10:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  46%|████▋     | 4314/9281 [1:01:27<1:10:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  46%|████▋     | 4315/9281 [1:01:28<1:10:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  47%|████▋     | 4316/9281 [1:01:29<1:10:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  47%|████▋     | 4317/9281 [1:01:30<1:10:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.97e-01:  47%|████▋     | 4318/9281 [1:01:30<1:10:51,  1.17it/s]\u001b[A\n",
      "Training loss: 5.35e-01:  47%|████▋     | 4319/9281 [1:01:31<1:10:51,  1.17it/s]\u001b[A\n",
      "Training loss: 5.31e-01:  47%|████▋     | 4320/9281 [1:01:32<1:10:34,  1.17it/s]\u001b[A\n",
      "Training loss: 5.03e-01:  47%|████▋     | 4321/9281 [1:01:33<1:10:48,  1.17it/s]\u001b[A\n",
      "Training loss: 5.86e-01:  47%|████▋     | 4322/9281 [1:01:34<1:10:49,  1.17it/s]\u001b[A\n",
      "Training loss: 5.49e-01:  47%|████▋     | 4323/9281 [1:01:35<1:10:43,  1.17it/s]\u001b[A\n",
      "Training loss: 5.10e-01:  47%|████▋     | 4324/9281 [1:01:36<1:10:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  47%|████▋     | 4325/9281 [1:01:36<1:10:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  47%|████▋     | 4326/9281 [1:01:37<1:10:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  47%|████▋     | 4327/9281 [1:01:38<1:10:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  47%|████▋     | 4328/9281 [1:01:39<1:10:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  47%|████▋     | 4329/9281 [1:01:40<1:10:10,  1.18it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  47%|████▋     | 4330/9281 [1:01:41<1:10:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  47%|████▋     | 4331/9281 [1:01:41<1:10:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  47%|████▋     | 4332/9281 [1:01:42<1:10:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  47%|████▋     | 4333/9281 [1:01:43<1:10:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  47%|████▋     | 4334/9281 [1:01:44<1:10:15,  1.17it/s]\u001b[A\n",
      "Training loss: 5.04e-01:  47%|████▋     | 4335/9281 [1:01:45<1:10:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  47%|████▋     | 4336/9281 [1:01:46<1:10:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  47%|████▋     | 4337/9281 [1:01:47<1:10:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  47%|████▋     | 4338/9281 [1:01:47<1:10:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  47%|████▋     | 4339/9281 [1:01:48<1:10:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  47%|████▋     | 4340/9281 [1:01:49<1:10:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  47%|████▋     | 4341/9281 [1:01:50<1:10:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  47%|████▋     | 4342/9281 [1:01:51<1:10:28,  1.17it/s]\u001b[A\n",
      "Training loss: 5.05e-01:  47%|████▋     | 4343/9281 [1:01:52<1:10:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.92e-01:  47%|████▋     | 4344/9281 [1:01:53<1:10:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  47%|████▋     | 4345/9281 [1:01:53<1:10:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  47%|████▋     | 4346/9281 [1:01:54<1:10:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  47%|████▋     | 4347/9281 [1:01:55<1:10:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  47%|████▋     | 4348/9281 [1:01:56<1:10:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  47%|████▋     | 4349/9281 [1:01:57<1:10:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  47%|████▋     | 4350/9281 [1:01:58<1:10:06,  1.17it/s]\u001b[A\n",
      "Training loss: 5.03e-01:  47%|████▋     | 4351/9281 [1:01:59<1:10:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  47%|████▋     | 4352/9281 [1:01:59<1:10:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  47%|████▋     | 4353/9281 [1:02:00<1:10:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  47%|████▋     | 4354/9281 [1:02:01<1:10:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  47%|████▋     | 4355/9281 [1:02:02<1:10:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  47%|████▋     | 4356/9281 [1:02:03<1:10:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  47%|████▋     | 4357/9281 [1:02:04<1:10:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  47%|████▋     | 4358/9281 [1:02:05<1:10:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  47%|████▋     | 4359/9281 [1:02:05<1:10:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  47%|████▋     | 4360/9281 [1:02:06<1:10:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  47%|████▋     | 4361/9281 [1:02:07<1:10:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  47%|████▋     | 4362/9281 [1:02:08<1:10:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  47%|████▋     | 4363/9281 [1:02:09<1:09:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  47%|████▋     | 4364/9281 [1:02:10<1:09:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  47%|████▋     | 4365/9281 [1:02:11<1:10:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  47%|████▋     | 4366/9281 [1:02:11<1:09:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  47%|████▋     | 4367/9281 [1:02:12<1:09:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  47%|████▋     | 4368/9281 [1:02:13<1:09:47,  1.17it/s]\u001b[A\n",
      "Training loss: 5.17e-01:  47%|████▋     | 4369/9281 [1:02:14<1:10:05,  1.17it/s]\u001b[A\n",
      "Training loss: 6.04e-01:  47%|████▋     | 4370/9281 [1:02:15<1:10:07,  1.17it/s]\u001b[A\n",
      "Training loss: 5.83e-01:  47%|████▋     | 4371/9281 [1:02:16<1:10:06,  1.17it/s]\u001b[A\n",
      "Training loss: 5.51e-01:  47%|████▋     | 4372/9281 [1:02:17<1:10:12,  1.17it/s]\u001b[A\n",
      "Training loss: 5.74e-01:  47%|████▋     | 4373/9281 [1:02:17<1:10:09,  1.17it/s]\u001b[A\n",
      "Training loss: 5.59e-01:  47%|████▋     | 4374/9281 [1:02:18<1:10:09,  1.17it/s]\u001b[A\n",
      "Training loss: 5.69e-01:  47%|████▋     | 4375/9281 [1:02:19<1:10:08,  1.17it/s]\u001b[A\n",
      "Training loss: 6.02e-01:  47%|████▋     | 4376/9281 [1:02:20<1:10:01,  1.17it/s]\u001b[A\n",
      "Training loss: 5.68e-01:  47%|████▋     | 4377/9281 [1:02:21<1:10:04,  1.17it/s]\u001b[A\n",
      "Training loss: 5.26e-01:  47%|████▋     | 4378/9281 [1:02:22<1:10:11,  1.16it/s]\u001b[A\n",
      "Training loss: 4.92e-01:  47%|████▋     | 4379/9281 [1:02:23<1:09:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  47%|████▋     | 4380/9281 [1:02:23<1:09:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  47%|████▋     | 4381/9281 [1:02:24<1:09:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  47%|████▋     | 4382/9281 [1:02:25<1:09:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  47%|████▋     | 4383/9281 [1:02:26<1:09:27,  1.18it/s]\u001b[A\n",
      "Training loss: 4.81e-01:  47%|████▋     | 4384/9281 [1:02:27<1:09:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  47%|████▋     | 4385/9281 [1:02:28<1:09:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  47%|████▋     | 4386/9281 [1:02:29<1:09:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  47%|████▋     | 4387/9281 [1:02:29<1:09:24,  1.18it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  47%|████▋     | 4388/9281 [1:02:30<1:09:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  47%|████▋     | 4389/9281 [1:02:31<1:09:42,  1.17it/s]\u001b[A\n",
      "Training loss: 5.18e-01:  47%|████▋     | 4390/9281 [1:02:32<1:09:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.98e-01:  47%|████▋     | 4391/9281 [1:02:33<1:09:34,  1.17it/s]\u001b[A\n",
      "Training loss: 5.11e-01:  47%|████▋     | 4392/9281 [1:02:34<1:09:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.82e-01:  47%|████▋     | 4393/9281 [1:02:34<1:09:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.69e-01:  47%|████▋     | 4394/9281 [1:02:35<1:09:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  47%|████▋     | 4395/9281 [1:02:36<1:09:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  47%|████▋     | 4396/9281 [1:02:37<1:09:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  47%|████▋     | 4397/9281 [1:02:38<1:09:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  47%|████▋     | 4398/9281 [1:02:39<1:09:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  47%|████▋     | 4399/9281 [1:02:40<1:09:54,  1.16it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  47%|████▋     | 4400/9281 [1:02:40<1:09:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  47%|████▋     | 4401/9281 [1:02:41<1:09:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  47%|████▋     | 4402/9281 [1:02:42<1:09:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  47%|████▋     | 4403/9281 [1:02:43<1:09:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  47%|████▋     | 4404/9281 [1:02:44<1:09:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  47%|████▋     | 4405/9281 [1:02:45<1:09:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  47%|████▋     | 4406/9281 [1:02:46<1:09:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  47%|████▋     | 4407/9281 [1:02:46<1:09:36,  1.17it/s]\u001b[A\n",
      "Training loss: 5.23e-01:  47%|████▋     | 4408/9281 [1:02:47<1:09:15,  1.17it/s]\u001b[A\n",
      "Training loss: 5.39e-01:  48%|████▊     | 4409/9281 [1:02:48<1:09:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  48%|████▊     | 4410/9281 [1:02:49<1:09:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  48%|████▊     | 4411/9281 [1:02:50<1:09:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  48%|████▊     | 4412/9281 [1:02:51<1:09:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  48%|████▊     | 4413/9281 [1:02:52<1:09:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  48%|████▊     | 4414/9281 [1:02:52<1:09:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  48%|████▊     | 4415/9281 [1:02:53<1:09:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  48%|████▊     | 4416/9281 [1:02:54<1:09:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  48%|████▊     | 4417/9281 [1:02:55<1:09:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  48%|████▊     | 4418/9281 [1:02:56<1:09:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  48%|████▊     | 4419/9281 [1:02:57<1:09:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  48%|████▊     | 4420/9281 [1:02:58<1:09:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  48%|████▊     | 4421/9281 [1:02:58<1:09:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  48%|████▊     | 4422/9281 [1:02:59<1:09:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  48%|████▊     | 4423/9281 [1:03:00<1:09:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  48%|████▊     | 4424/9281 [1:03:01<1:09:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.96e-01:  48%|████▊     | 4425/9281 [1:03:02<1:09:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  48%|████▊     | 4426/9281 [1:03:03<1:08:51,  1.18it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  48%|████▊     | 4427/9281 [1:03:04<1:08:49,  1.18it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  48%|████▊     | 4428/9281 [1:03:04<1:08:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  48%|████▊     | 4429/9281 [1:03:05<1:09:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  48%|████▊     | 4430/9281 [1:03:06<1:08:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  48%|████▊     | 4431/9281 [1:03:07<1:09:03,  1.17it/s]\u001b[A\n",
      "Training loss: 5.08e-01:  48%|████▊     | 4432/9281 [1:03:08<1:08:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.96e-01:  48%|████▊     | 4433/9281 [1:03:09<1:08:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  48%|████▊     | 4434/9281 [1:03:10<1:08:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.87e-01:  48%|████▊     | 4435/9281 [1:03:10<1:09:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  48%|████▊     | 4436/9281 [1:03:11<1:09:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  48%|████▊     | 4437/9281 [1:03:12<1:08:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  48%|████▊     | 4438/9281 [1:03:13<1:09:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  48%|████▊     | 4439/9281 [1:03:14<1:08:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  48%|████▊     | 4440/9281 [1:03:15<1:09:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  48%|████▊     | 4441/9281 [1:03:16<1:08:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  48%|████▊     | 4442/9281 [1:03:16<1:08:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  48%|████▊     | 4443/9281 [1:03:17<1:08:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  48%|████▊     | 4444/9281 [1:03:18<1:08:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  48%|████▊     | 4445/9281 [1:03:19<1:08:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.92e-01:  48%|████▊     | 4446/9281 [1:03:20<1:08:33,  1.18it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  48%|████▊     | 4447/9281 [1:03:21<1:08:31,  1.18it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  48%|████▊     | 4448/9281 [1:03:21<1:08:32,  1.18it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  48%|████▊     | 4449/9281 [1:03:22<1:08:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  48%|████▊     | 4450/9281 [1:03:23<1:08:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  48%|████▊     | 4451/9281 [1:03:24<1:08:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  48%|████▊     | 4452/9281 [1:03:25<1:08:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  48%|████▊     | 4453/9281 [1:03:26<1:08:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  48%|████▊     | 4454/9281 [1:03:27<1:08:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  48%|████▊     | 4455/9281 [1:03:27<1:08:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  48%|████▊     | 4456/9281 [1:03:28<1:08:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.83e-01:  48%|████▊     | 4457/9281 [1:03:29<1:08:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  48%|████▊     | 4458/9281 [1:03:30<1:08:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  48%|████▊     | 4459/9281 [1:03:31<1:08:49,  1.17it/s]\u001b[A\n",
      "Training loss: 5.01e-01:  48%|████▊     | 4460/9281 [1:03:32<1:08:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  48%|████▊     | 4461/9281 [1:03:33<1:08:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  48%|████▊     | 4462/9281 [1:03:33<1:08:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  48%|████▊     | 4463/9281 [1:03:34<1:08:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  48%|████▊     | 4464/9281 [1:03:35<1:08:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  48%|████▊     | 4465/9281 [1:03:36<1:08:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  48%|████▊     | 4466/9281 [1:03:37<1:08:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  48%|████▊     | 4467/9281 [1:03:38<1:08:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  48%|████▊     | 4468/9281 [1:03:39<1:08:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  48%|████▊     | 4469/9281 [1:03:39<1:08:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  48%|████▊     | 4470/9281 [1:03:40<1:08:53,  1.16it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  48%|████▊     | 4471/9281 [1:03:41<1:08:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  48%|████▊     | 4472/9281 [1:03:42<1:08:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  48%|████▊     | 4473/9281 [1:03:43<1:08:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  48%|████▊     | 4474/9281 [1:03:44<1:08:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  48%|████▊     | 4475/9281 [1:03:45<1:08:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  48%|████▊     | 4476/9281 [1:03:45<1:08:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  48%|████▊     | 4477/9281 [1:03:46<1:08:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.68e-01:  48%|████▊     | 4478/9281 [1:03:47<1:08:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  48%|████▊     | 4479/9281 [1:03:48<1:08:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  48%|████▊     | 4480/9281 [1:03:49<1:08:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  48%|████▊     | 4481/9281 [1:03:50<1:08:25,  1.17it/s]\u001b[A\n",
      "Training loss: 5.11e-01:  48%|████▊     | 4482/9281 [1:03:51<1:08:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  48%|████▊     | 4483/9281 [1:03:51<1:08:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  48%|████▊     | 4484/9281 [1:03:52<1:08:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  48%|████▊     | 4485/9281 [1:03:53<1:08:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  48%|████▊     | 4486/9281 [1:03:54<1:08:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  48%|████▊     | 4487/9281 [1:03:55<1:08:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  48%|████▊     | 4488/9281 [1:03:56<1:08:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  48%|████▊     | 4489/9281 [1:03:57<1:08:38,  1.16it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  48%|████▊     | 4490/9281 [1:03:57<1:08:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  48%|████▊     | 4491/9281 [1:03:58<1:08:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  48%|████▊     | 4492/9281 [1:03:59<1:08:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  48%|████▊     | 4493/9281 [1:04:00<1:08:21,  1.17it/s]\u001b[A\n",
      "Training loss: 5.25e-01:  48%|████▊     | 4494/9281 [1:04:01<1:08:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  48%|████▊     | 4495/9281 [1:04:02<1:08:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  48%|████▊     | 4496/9281 [1:04:03<1:08:32,  1.16it/s]\u001b[A\n",
      "Training loss: 4.86e-01:  48%|████▊     | 4497/9281 [1:04:03<1:08:16,  1.17it/s]\u001b[A\n",
      "Training loss: 5.25e-01:  48%|████▊     | 4498/9281 [1:04:04<1:08:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  48%|████▊     | 4499/9281 [1:04:05<1:08:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  48%|████▊     | 4500/9281 [1:04:06<1:08:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  48%|████▊     | 4501/9281 [1:04:07<1:07:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  49%|████▊     | 4502/9281 [1:04:08<1:08:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  49%|████▊     | 4503/9281 [1:04:09<1:07:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  49%|████▊     | 4504/9281 [1:04:09<1:08:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  49%|████▊     | 4505/9281 [1:04:10<1:08:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  49%|████▊     | 4506/9281 [1:04:11<1:07:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  49%|████▊     | 4507/9281 [1:04:12<1:07:43,  1.17it/s]\u001b[A\n",
      "Training loss: 5.13e-01:  49%|████▊     | 4508/9281 [1:04:13<1:07:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.92e-01:  49%|████▊     | 4509/9281 [1:04:14<1:07:36,  1.18it/s]\u001b[A\n",
      "Training loss: 5.41e-01:  49%|████▊     | 4510/9281 [1:04:15<1:07:48,  1.17it/s]\u001b[A\n",
      "Training loss: 5.16e-01:  49%|████▊     | 4511/9281 [1:04:15<1:07:45,  1.17it/s]\u001b[A\n",
      "Training loss: 5.41e-01:  49%|████▊     | 4512/9281 [1:04:16<1:07:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.86e-01:  49%|████▊     | 4513/9281 [1:04:17<1:07:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  49%|████▊     | 4514/9281 [1:04:18<1:07:43,  1.17it/s]\u001b[A\n",
      "Training loss: 5.35e-01:  49%|████▊     | 4515/9281 [1:04:19<1:07:35,  1.18it/s]\u001b[A\n",
      "Training loss: 4.97e-01:  49%|████▊     | 4516/9281 [1:04:20<1:07:34,  1.18it/s]\u001b[A\n",
      "Training loss: 4.81e-01:  49%|████▊     | 4517/9281 [1:04:20<1:07:29,  1.18it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  49%|████▊     | 4518/9281 [1:04:21<1:07:29,  1.18it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  49%|████▊     | 4519/9281 [1:04:22<1:07:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  49%|████▊     | 4520/9281 [1:04:23<1:07:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  49%|████▊     | 4521/9281 [1:04:24<1:07:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  49%|████▊     | 4522/9281 [1:04:25<1:07:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  49%|████▊     | 4523/9281 [1:04:26<1:07:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  49%|████▊     | 4524/9281 [1:04:26<1:07:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  49%|████▉     | 4525/9281 [1:04:27<1:07:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  49%|████▉     | 4526/9281 [1:04:28<1:07:24,  1.18it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  49%|████▉     | 4527/9281 [1:04:29<1:07:24,  1.18it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  49%|████▉     | 4528/9281 [1:04:30<1:07:17,  1.18it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  49%|████▉     | 4529/9281 [1:04:31<1:07:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  49%|████▉     | 4530/9281 [1:04:32<1:07:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  49%|████▉     | 4531/9281 [1:04:32<1:07:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  49%|████▉     | 4532/9281 [1:04:33<1:07:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  49%|████▉     | 4533/9281 [1:04:34<1:07:59,  1.16it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  49%|████▉     | 4534/9281 [1:04:35<1:08:05,  1.16it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  49%|████▉     | 4535/9281 [1:04:36<1:07:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  49%|████▉     | 4536/9281 [1:04:37<1:07:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  49%|████▉     | 4537/9281 [1:04:38<1:07:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  49%|████▉     | 4538/9281 [1:04:38<1:07:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  49%|████▉     | 4539/9281 [1:04:39<1:07:17,  1.17it/s]\u001b[A\n",
      "Training loss: 5.00e-01:  49%|████▉     | 4540/9281 [1:04:40<1:07:10,  1.18it/s]\u001b[A\n",
      "Training loss: 5.31e-01:  49%|████▉     | 4541/9281 [1:04:41<1:07:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  49%|████▉     | 4542/9281 [1:04:42<1:07:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.83e-01:  49%|████▉     | 4543/9281 [1:04:43<1:07:28,  1.17it/s]\u001b[A\n",
      "Training loss: 5.19e-01:  49%|████▉     | 4544/9281 [1:04:44<1:07:31,  1.17it/s]\u001b[A\n",
      "Training loss: 5.02e-01:  49%|████▉     | 4545/9281 [1:04:44<1:07:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  49%|████▉     | 4546/9281 [1:04:45<1:07:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  49%|████▉     | 4547/9281 [1:04:46<1:07:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  49%|████▉     | 4548/9281 [1:04:47<1:07:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  49%|████▉     | 4549/9281 [1:04:48<1:07:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  49%|████▉     | 4550/9281 [1:04:49<1:07:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  49%|████▉     | 4551/9281 [1:04:49<1:07:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  49%|████▉     | 4552/9281 [1:04:50<1:07:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  49%|████▉     | 4553/9281 [1:04:51<1:07:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  49%|████▉     | 4554/9281 [1:04:52<1:07:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  49%|████▉     | 4555/9281 [1:04:53<1:07:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  49%|████▉     | 4556/9281 [1:04:54<1:07:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  49%|████▉     | 4557/9281 [1:04:55<1:07:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  49%|████▉     | 4558/9281 [1:04:55<1:07:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  49%|████▉     | 4559/9281 [1:04:56<1:07:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  49%|████▉     | 4560/9281 [1:04:57<1:07:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  49%|████▉     | 4561/9281 [1:04:58<1:06:48,  1.18it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  49%|████▉     | 4562/9281 [1:04:59<1:06:41,  1.18it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  49%|████▉     | 4563/9281 [1:05:00<1:06:43,  1.18it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  49%|████▉     | 4564/9281 [1:05:01<1:07:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  49%|████▉     | 4565/9281 [1:05:01<1:06:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  49%|████▉     | 4566/9281 [1:05:02<1:07:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  49%|████▉     | 4567/9281 [1:05:03<1:07:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  49%|████▉     | 4568/9281 [1:05:04<1:07:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  49%|████▉     | 4569/9281 [1:05:05<1:07:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  49%|████▉     | 4570/9281 [1:05:06<1:07:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  49%|████▉     | 4571/9281 [1:05:07<1:07:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  49%|████▉     | 4572/9281 [1:05:07<1:06:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  49%|████▉     | 4573/9281 [1:05:08<1:07:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  49%|████▉     | 4574/9281 [1:05:09<1:07:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  49%|████▉     | 4575/9281 [1:05:10<1:06:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  49%|████▉     | 4576/9281 [1:05:11<1:07:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.96e-01:  49%|████▉     | 4577/9281 [1:05:12<1:07:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.69e-01:  49%|████▉     | 4578/9281 [1:05:13<1:07:18,  1.16it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  49%|████▉     | 4579/9281 [1:05:13<1:06:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  49%|████▉     | 4580/9281 [1:05:14<1:06:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  49%|████▉     | 4581/9281 [1:05:15<1:06:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  49%|████▉     | 4582/9281 [1:05:16<1:06:36,  1.18it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  49%|████▉     | 4583/9281 [1:05:17<1:06:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  49%|████▉     | 4584/9281 [1:05:18<1:06:35,  1.18it/s]\u001b[A\n",
      "Training loss: 5.02e-01:  49%|████▉     | 4585/9281 [1:05:19<1:06:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  49%|████▉     | 4586/9281 [1:05:19<1:06:29,  1.18it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  49%|████▉     | 4587/9281 [1:05:20<1:06:28,  1.18it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  49%|████▉     | 4588/9281 [1:05:21<1:06:31,  1.18it/s]\u001b[A\n",
      "Training loss: 5.34e-01:  49%|████▉     | 4589/9281 [1:05:22<1:06:43,  1.17it/s]\u001b[A\n",
      "Training loss: 5.74e-01:  49%|████▉     | 4590/9281 [1:05:23<1:06:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.98e-01:  49%|████▉     | 4591/9281 [1:05:24<1:06:49,  1.17it/s]\u001b[A\n",
      "Training loss: 5.17e-01:  49%|████▉     | 4592/9281 [1:05:24<1:06:30,  1.18it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  49%|████▉     | 4593/9281 [1:05:25<1:06:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  49%|████▉     | 4594/9281 [1:05:26<1:06:17,  1.18it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  50%|████▉     | 4595/9281 [1:05:27<1:06:28,  1.17it/s]\u001b[A\n",
      "Training loss: 5.10e-01:  50%|████▉     | 4596/9281 [1:05:28<1:06:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.82e-01:  50%|████▉     | 4597/9281 [1:05:29<1:06:26,  1.18it/s]\u001b[A\n",
      "Training loss: 5.01e-01:  50%|████▉     | 4598/9281 [1:05:30<1:06:19,  1.18it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  50%|████▉     | 4599/9281 [1:05:30<1:06:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  50%|████▉     | 4600/9281 [1:05:31<1:06:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  50%|████▉     | 4601/9281 [1:05:32<1:06:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  50%|████▉     | 4602/9281 [1:05:33<1:06:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  50%|████▉     | 4603/9281 [1:05:34<1:06:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  50%|████▉     | 4604/9281 [1:05:35<1:06:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  50%|████▉     | 4605/9281 [1:05:36<1:06:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  50%|████▉     | 4606/9281 [1:05:36<1:06:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  50%|████▉     | 4607/9281 [1:05:37<1:06:23,  1.17it/s]\u001b[A\n",
      "Training loss: 5.20e-01:  50%|████▉     | 4608/9281 [1:05:38<1:06:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.95e-01:  50%|████▉     | 4609/9281 [1:05:39<1:06:27,  1.17it/s]\u001b[A\n",
      "Training loss: 5.05e-01:  50%|████▉     | 4610/9281 [1:05:40<1:06:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  50%|████▉     | 4611/9281 [1:05:41<1:06:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  50%|████▉     | 4612/9281 [1:05:42<1:06:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  50%|████▉     | 4613/9281 [1:05:42<1:06:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  50%|████▉     | 4614/9281 [1:05:43<1:06:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  50%|████▉     | 4615/9281 [1:05:44<1:06:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  50%|████▉     | 4616/9281 [1:05:45<1:06:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  50%|████▉     | 4617/9281 [1:05:46<1:06:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  50%|████▉     | 4618/9281 [1:05:47<1:06:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  50%|████▉     | 4619/9281 [1:05:48<1:06:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  50%|████▉     | 4620/9281 [1:05:48<1:06:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  50%|████▉     | 4621/9281 [1:05:49<1:06:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  50%|████▉     | 4622/9281 [1:05:50<1:06:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  50%|████▉     | 4623/9281 [1:05:51<1:06:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  50%|████▉     | 4624/9281 [1:05:52<1:06:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  50%|████▉     | 4625/9281 [1:05:53<1:06:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  50%|████▉     | 4626/9281 [1:05:54<1:06:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  50%|████▉     | 4627/9281 [1:05:54<1:06:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  50%|████▉     | 4628/9281 [1:05:55<1:06:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  50%|████▉     | 4629/9281 [1:05:56<1:06:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  50%|████▉     | 4630/9281 [1:05:57<1:06:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  50%|████▉     | 4631/9281 [1:05:58<1:05:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  50%|████▉     | 4632/9281 [1:05:59<1:06:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  50%|████▉     | 4633/9281 [1:05:59<1:06:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  50%|████▉     | 4634/9281 [1:06:00<1:06:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  50%|████▉     | 4635/9281 [1:06:01<1:06:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  50%|████▉     | 4636/9281 [1:06:02<1:06:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  50%|████▉     | 4637/9281 [1:06:03<1:06:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  50%|████▉     | 4638/9281 [1:06:04<1:06:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  50%|████▉     | 4639/9281 [1:06:05<1:06:07,  1.17it/s]\u001b[A\n",
      "Training loss: 5.19e-01:  50%|████▉     | 4640/9281 [1:06:05<1:06:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  50%|█████     | 4641/9281 [1:06:06<1:06:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.98e-01:  50%|█████     | 4642/9281 [1:06:07<1:06:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  50%|█████     | 4643/9281 [1:06:08<1:06:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  50%|█████     | 4644/9281 [1:06:09<1:06:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  50%|█████     | 4645/9281 [1:06:10<1:05:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  50%|█████     | 4646/9281 [1:06:11<1:06:09,  1.17it/s]\u001b[A\n",
      "Training loss: 5.38e-01:  50%|█████     | 4647/9281 [1:06:11<1:05:48,  1.17it/s]\u001b[A\n",
      "Training loss: 5.08e-01:  50%|█████     | 4648/9281 [1:06:12<1:05:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  50%|█████     | 4649/9281 [1:06:13<1:06:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  50%|█████     | 4650/9281 [1:06:14<1:05:54,  1.17it/s]\u001b[A\n",
      "Training loss: 5.48e-01:  50%|█████     | 4651/9281 [1:06:15<1:05:56,  1.17it/s]\u001b[A\n",
      "Training loss: 5.07e-01:  50%|█████     | 4652/9281 [1:06:16<1:06:03,  1.17it/s]\u001b[A\n",
      "Training loss: 5.18e-01:  50%|█████     | 4653/9281 [1:06:17<1:05:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  50%|█████     | 4654/9281 [1:06:17<1:06:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.69e-01:  50%|█████     | 4655/9281 [1:06:18<1:06:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  50%|█████     | 4656/9281 [1:06:19<1:05:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  50%|█████     | 4657/9281 [1:06:20<1:06:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  50%|█████     | 4658/9281 [1:06:21<1:05:59,  1.17it/s]\u001b[A\n",
      "Training loss: 5.04e-01:  50%|█████     | 4659/9281 [1:06:22<1:06:11,  1.16it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  50%|█████     | 4660/9281 [1:06:23<1:05:55,  1.17it/s]\u001b[A\n",
      "Training loss: 5.07e-01:  50%|█████     | 4661/9281 [1:06:23<1:05:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  50%|█████     | 4662/9281 [1:06:24<1:05:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  50%|█████     | 4663/9281 [1:06:25<1:05:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  50%|█████     | 4664/9281 [1:06:26<1:05:27,  1.18it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  50%|█████     | 4665/9281 [1:06:27<1:05:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  50%|█████     | 4666/9281 [1:06:28<1:05:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  50%|█████     | 4667/9281 [1:06:29<1:05:43,  1.17it/s]\u001b[A\n",
      "Training loss: 5.50e-01:  50%|█████     | 4668/9281 [1:06:29<1:05:43,  1.17it/s]\u001b[A\n",
      "Training loss: 5.26e-01:  50%|█████     | 4669/9281 [1:06:30<1:05:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  50%|█████     | 4670/9281 [1:06:31<1:05:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  50%|█████     | 4671/9281 [1:06:32<1:05:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  50%|█████     | 4672/9281 [1:06:33<1:05:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  50%|█████     | 4673/9281 [1:06:34<1:05:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  50%|█████     | 4674/9281 [1:06:35<1:05:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  50%|█████     | 4675/9281 [1:06:35<1:05:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  50%|█████     | 4676/9281 [1:06:36<1:05:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  50%|█████     | 4677/9281 [1:06:37<1:05:17,  1.18it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  50%|█████     | 4678/9281 [1:06:38<1:05:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  50%|█████     | 4679/9281 [1:06:39<1:05:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  50%|█████     | 4680/9281 [1:06:40<1:05:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  50%|█████     | 4681/9281 [1:06:41<1:05:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.88e-01:  50%|█████     | 4682/9281 [1:06:41<1:05:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  50%|█████     | 4683/9281 [1:06:42<1:05:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  50%|█████     | 4684/9281 [1:06:43<1:05:05,  1.18it/s]\u001b[A\n",
      "Training loss: 5.42e-01:  50%|█████     | 4685/9281 [1:06:44<1:05:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  50%|█████     | 4686/9281 [1:06:45<1:05:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  51%|█████     | 4687/9281 [1:06:46<1:05:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  51%|█████     | 4688/9281 [1:06:46<1:05:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  51%|█████     | 4689/9281 [1:06:47<1:05:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  51%|█████     | 4690/9281 [1:06:48<1:05:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  51%|█████     | 4691/9281 [1:06:49<1:05:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  51%|█████     | 4692/9281 [1:06:50<1:05:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  51%|█████     | 4693/9281 [1:06:51<1:05:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.54e-01:  51%|█████     | 4694/9281 [1:06:52<1:05:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  51%|█████     | 4695/9281 [1:06:52<1:05:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  51%|█████     | 4696/9281 [1:06:53<1:05:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  51%|█████     | 4697/9281 [1:06:54<1:05:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  51%|█████     | 4698/9281 [1:06:55<1:05:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  51%|█████     | 4699/9281 [1:06:56<1:05:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  51%|█████     | 4700/9281 [1:06:57<1:05:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  51%|█████     | 4701/9281 [1:06:58<1:05:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  51%|█████     | 4702/9281 [1:06:58<1:05:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  51%|█████     | 4703/9281 [1:06:59<1:05:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  51%|█████     | 4704/9281 [1:07:00<1:05:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  51%|█████     | 4705/9281 [1:07:01<1:05:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  51%|█████     | 4706/9281 [1:07:02<1:05:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  51%|█████     | 4707/9281 [1:07:03<1:05:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  51%|█████     | 4708/9281 [1:07:04<1:05:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  51%|█████     | 4709/9281 [1:07:04<1:05:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  51%|█████     | 4710/9281 [1:07:05<1:04:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  51%|█████     | 4711/9281 [1:07:06<1:04:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  51%|█████     | 4712/9281 [1:07:07<1:04:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  51%|█████     | 4713/9281 [1:07:08<1:04:41,  1.18it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  51%|█████     | 4714/9281 [1:07:09<1:04:43,  1.18it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  51%|█████     | 4715/9281 [1:07:10<1:05:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  51%|█████     | 4716/9281 [1:07:10<1:04:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  51%|█████     | 4717/9281 [1:07:11<1:05:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  51%|█████     | 4718/9281 [1:07:12<1:05:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  51%|█████     | 4719/9281 [1:07:13<1:04:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  51%|█████     | 4720/9281 [1:07:14<1:04:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  51%|█████     | 4721/9281 [1:07:15<1:05:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  51%|█████     | 4722/9281 [1:07:16<1:04:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  51%|█████     | 4723/9281 [1:07:16<1:04:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  51%|█████     | 4724/9281 [1:07:17<1:05:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  51%|█████     | 4725/9281 [1:07:18<1:04:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.67e-01:  51%|█████     | 4726/9281 [1:07:19<1:04:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  51%|█████     | 4727/9281 [1:07:20<1:04:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  51%|█████     | 4728/9281 [1:07:21<1:04:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  51%|█████     | 4729/9281 [1:07:22<1:04:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  51%|█████     | 4730/9281 [1:07:22<1:04:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  51%|█████     | 4731/9281 [1:07:23<1:04:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  51%|█████     | 4732/9281 [1:07:24<1:04:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  51%|█████     | 4733/9281 [1:07:25<1:04:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  51%|█████     | 4734/9281 [1:07:26<1:05:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  51%|█████     | 4735/9281 [1:07:27<1:04:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  51%|█████     | 4736/9281 [1:07:28<1:05:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  51%|█████     | 4737/9281 [1:07:28<1:04:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  51%|█████     | 4738/9281 [1:07:29<1:04:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  51%|█████     | 4739/9281 [1:07:30<1:04:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  51%|█████     | 4740/9281 [1:07:31<1:04:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  51%|█████     | 4741/9281 [1:07:32<1:04:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  51%|█████     | 4742/9281 [1:07:33<1:04:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  51%|█████     | 4743/9281 [1:07:33<1:04:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  51%|█████     | 4744/9281 [1:07:34<1:04:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  51%|█████     | 4745/9281 [1:07:35<1:04:15,  1.18it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  51%|█████     | 4746/9281 [1:07:36<1:04:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  51%|█████     | 4747/9281 [1:07:37<1:04:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  51%|█████     | 4748/9281 [1:07:38<1:04:31,  1.17it/s]\u001b[A\n",
      "Training loss: 5.22e-01:  51%|█████     | 4749/9281 [1:07:39<1:04:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  51%|█████     | 4750/9281 [1:07:39<1:04:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  51%|█████     | 4751/9281 [1:07:40<1:04:18,  1.17it/s]\u001b[A\n",
      "Training loss: 5.18e-01:  51%|█████     | 4752/9281 [1:07:41<1:04:27,  1.17it/s]\u001b[A\n",
      "Training loss: 5.39e-01:  51%|█████     | 4753/9281 [1:07:42<1:04:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  51%|█████     | 4754/9281 [1:07:43<1:04:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  51%|█████     | 4755/9281 [1:07:44<1:04:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  51%|█████     | 4756/9281 [1:07:45<1:04:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  51%|█████▏    | 4757/9281 [1:07:45<1:04:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  51%|█████▏    | 4758/9281 [1:07:46<1:04:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  51%|█████▏    | 4759/9281 [1:07:47<1:04:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  51%|█████▏    | 4760/9281 [1:07:48<1:04:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  51%|█████▏    | 4761/9281 [1:07:49<1:04:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  51%|█████▏    | 4762/9281 [1:07:50<1:04:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  51%|█████▏    | 4763/9281 [1:07:51<1:03:50,  1.18it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  51%|█████▏    | 4764/9281 [1:07:51<1:04:01,  1.18it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  51%|█████▏    | 4765/9281 [1:07:52<1:04:02,  1.18it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  51%|█████▏    | 4766/9281 [1:07:53<1:04:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  51%|█████▏    | 4767/9281 [1:07:54<1:04:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  51%|█████▏    | 4768/9281 [1:07:55<1:04:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  51%|█████▏    | 4769/9281 [1:07:56<1:03:52,  1.18it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  51%|█████▏    | 4770/9281 [1:07:56<1:03:47,  1.18it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  51%|█████▏    | 4771/9281 [1:07:57<1:04:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  51%|█████▏    | 4772/9281 [1:07:58<1:04:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  51%|█████▏    | 4773/9281 [1:07:59<1:04:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  51%|█████▏    | 4774/9281 [1:08:00<1:04:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  51%|█████▏    | 4775/9281 [1:08:01<1:04:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  51%|█████▏    | 4776/9281 [1:08:02<1:04:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  51%|█████▏    | 4777/9281 [1:08:02<1:04:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  51%|█████▏    | 4778/9281 [1:08:03<1:04:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  51%|█████▏    | 4779/9281 [1:08:04<1:04:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  52%|█████▏    | 4780/9281 [1:08:05<1:04:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  52%|█████▏    | 4781/9281 [1:08:06<1:04:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  52%|█████▏    | 4782/9281 [1:08:07<1:04:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  52%|█████▏    | 4783/9281 [1:08:08<1:03:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.34e-01:  52%|█████▏    | 4784/9281 [1:08:08<1:03:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.39e-01:  52%|█████▏    | 4785/9281 [1:08:09<1:03:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  52%|█████▏    | 4786/9281 [1:08:10<1:03:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  52%|█████▏    | 4787/9281 [1:08:11<1:03:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  52%|█████▏    | 4788/9281 [1:08:12<1:03:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.24e-01:  52%|█████▏    | 4789/9281 [1:08:13<1:03:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  52%|█████▏    | 4790/9281 [1:08:14<1:04:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  52%|█████▏    | 4791/9281 [1:08:14<1:04:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  52%|█████▏    | 4792/9281 [1:08:15<1:04:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  52%|█████▏    | 4793/9281 [1:08:16<1:03:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  52%|█████▏    | 4794/9281 [1:08:17<1:04:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  52%|█████▏    | 4795/9281 [1:08:18<1:03:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  52%|█████▏    | 4796/9281 [1:08:19<1:03:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  52%|█████▏    | 4797/9281 [1:08:20<1:03:59,  1.17it/s]\u001b[A\n",
      "Training loss: 5.42e-01:  52%|█████▏    | 4798/9281 [1:08:20<1:04:08,  1.16it/s]\u001b[A\n",
      "Training loss: 5.25e-01:  52%|█████▏    | 4799/9281 [1:08:21<1:03:57,  1.17it/s]\u001b[A\n",
      "Training loss: 5.21e-01:  52%|█████▏    | 4800/9281 [1:08:22<1:04:10,  1.16it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  52%|█████▏    | 4801/9281 [1:08:23<1:04:08,  1.16it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  52%|█████▏    | 4802/9281 [1:08:24<1:03:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  52%|█████▏    | 4803/9281 [1:08:25<1:03:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  52%|█████▏    | 4804/9281 [1:08:26<1:03:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  52%|█████▏    | 4805/9281 [1:08:26<1:03:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  52%|█████▏    | 4806/9281 [1:08:27<1:03:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  52%|█████▏    | 4807/9281 [1:08:28<1:03:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  52%|█████▏    | 4808/9281 [1:08:29<1:03:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  52%|█████▏    | 4809/9281 [1:08:30<1:03:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  52%|█████▏    | 4810/9281 [1:08:31<1:03:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  52%|█████▏    | 4811/9281 [1:08:32<1:03:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  52%|█████▏    | 4812/9281 [1:08:32<1:03:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  52%|█████▏    | 4813/9281 [1:08:33<1:03:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  52%|█████▏    | 4814/9281 [1:08:34<1:03:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  52%|█████▏    | 4815/9281 [1:08:35<1:03:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  52%|█████▏    | 4816/9281 [1:08:36<1:03:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  52%|█████▏    | 4817/9281 [1:08:37<1:03:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  52%|█████▏    | 4818/9281 [1:08:38<1:03:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  52%|█████▏    | 4819/9281 [1:08:38<1:03:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  52%|█████▏    | 4820/9281 [1:08:39<1:03:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  52%|█████▏    | 4821/9281 [1:08:40<1:03:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  52%|█████▏    | 4822/9281 [1:08:41<1:03:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  52%|█████▏    | 4823/9281 [1:08:42<1:03:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  52%|█████▏    | 4824/9281 [1:08:43<1:03:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  52%|█████▏    | 4825/9281 [1:08:44<1:03:09,  1.18it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  52%|█████▏    | 4826/9281 [1:08:44<1:03:10,  1.18it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  52%|█████▏    | 4827/9281 [1:08:45<1:03:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  52%|█████▏    | 4828/9281 [1:08:46<1:03:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  52%|█████▏    | 4829/9281 [1:08:47<1:03:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  52%|█████▏    | 4830/9281 [1:08:48<1:03:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  52%|█████▏    | 4831/9281 [1:08:49<1:03:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  52%|█████▏    | 4832/9281 [1:08:49<1:03:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  52%|█████▏    | 4833/9281 [1:08:50<1:03:32,  1.17it/s]\u001b[A\n",
      "Training loss: 5.26e-01:  52%|█████▏    | 4834/9281 [1:08:51<1:03:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.98e-01:  52%|█████▏    | 4835/9281 [1:08:52<1:03:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.68e-01:  52%|█████▏    | 4836/9281 [1:08:53<1:03:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  52%|█████▏    | 4837/9281 [1:08:54<1:03:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  52%|█████▏    | 4838/9281 [1:08:55<1:03:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  52%|█████▏    | 4839/9281 [1:08:55<1:03:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  52%|█████▏    | 4840/9281 [1:08:56<1:03:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  52%|█████▏    | 4841/9281 [1:08:57<1:03:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  52%|█████▏    | 4842/9281 [1:08:58<1:03:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  52%|█████▏    | 4843/9281 [1:08:59<1:03:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  52%|█████▏    | 4844/9281 [1:09:00<1:03:46,  1.16it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  52%|█████▏    | 4845/9281 [1:09:01<1:03:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  52%|█████▏    | 4846/9281 [1:09:01<1:03:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  52%|█████▏    | 4847/9281 [1:09:02<1:03:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  52%|█████▏    | 4848/9281 [1:09:03<1:03:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  52%|█████▏    | 4849/9281 [1:09:04<1:03:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.99e-01:  52%|█████▏    | 4850/9281 [1:09:05<1:03:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  52%|█████▏    | 4851/9281 [1:09:06<1:02:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.67e-01:  52%|█████▏    | 4852/9281 [1:09:07<1:03:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  52%|█████▏    | 4853/9281 [1:09:07<1:03:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  52%|█████▏    | 4854/9281 [1:09:08<1:03:20,  1.16it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  52%|█████▏    | 4855/9281 [1:09:09<1:03:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  52%|█████▏    | 4856/9281 [1:09:10<1:03:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  52%|█████▏    | 4857/9281 [1:09:11<1:03:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  52%|█████▏    | 4858/9281 [1:09:12<1:03:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  52%|█████▏    | 4859/9281 [1:09:13<1:03:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.79e-01:  52%|█████▏    | 4860/9281 [1:09:13<1:03:22,  1.16it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  52%|█████▏    | 4861/9281 [1:09:14<1:03:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  52%|█████▏    | 4862/9281 [1:09:15<1:03:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  52%|█████▏    | 4863/9281 [1:09:16<1:03:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  52%|█████▏    | 4864/9281 [1:09:17<1:02:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  52%|█████▏    | 4865/9281 [1:09:18<1:02:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  52%|█████▏    | 4866/9281 [1:09:19<1:02:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  52%|█████▏    | 4867/9281 [1:09:19<1:02:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  52%|█████▏    | 4868/9281 [1:09:20<1:02:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  52%|█████▏    | 4869/9281 [1:09:21<1:02:34,  1.18it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  52%|█████▏    | 4870/9281 [1:09:22<1:02:31,  1.18it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  52%|█████▏    | 4871/9281 [1:09:23<1:02:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  52%|█████▏    | 4872/9281 [1:09:24<1:02:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  53%|█████▎    | 4873/9281 [1:09:25<1:02:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  53%|█████▎    | 4874/9281 [1:09:25<1:02:39,  1.17it/s]\u001b[A\n",
      "Training loss: 5.03e-01:  53%|█████▎    | 4875/9281 [1:09:26<1:02:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  53%|█████▎    | 4876/9281 [1:09:27<1:02:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  53%|█████▎    | 4877/9281 [1:09:28<1:02:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  53%|█████▎    | 4878/9281 [1:09:29<1:02:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  53%|█████▎    | 4879/9281 [1:09:30<1:02:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  53%|█████▎    | 4880/9281 [1:09:31<1:02:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  53%|█████▎    | 4881/9281 [1:09:31<1:02:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  53%|█████▎    | 4882/9281 [1:09:32<1:02:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  53%|█████▎    | 4883/9281 [1:09:33<1:02:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  53%|█████▎    | 4884/9281 [1:09:34<1:02:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  53%|█████▎    | 4885/9281 [1:09:35<1:02:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  53%|█████▎    | 4886/9281 [1:09:36<1:02:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  53%|█████▎    | 4887/9281 [1:09:37<1:02:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  53%|█████▎    | 4888/9281 [1:09:37<1:02:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.88e-01:  53%|█████▎    | 4889/9281 [1:09:38<1:02:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  53%|█████▎    | 4890/9281 [1:09:39<1:02:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  53%|█████▎    | 4891/9281 [1:09:40<1:02:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  53%|█████▎    | 4892/9281 [1:09:41<1:02:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  53%|█████▎    | 4893/9281 [1:09:42<1:02:39,  1.17it/s]\u001b[A\n",
      "Training loss: 5.06e-01:  53%|█████▎    | 4894/9281 [1:09:42<1:02:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.69e-01:  53%|█████▎    | 4895/9281 [1:09:43<1:02:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  53%|█████▎    | 4896/9281 [1:09:44<1:02:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  53%|█████▎    | 4897/9281 [1:09:45<1:02:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  53%|█████▎    | 4898/9281 [1:09:46<1:02:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  53%|█████▎    | 4899/9281 [1:09:47<1:02:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.68e-01:  53%|█████▎    | 4900/9281 [1:09:48<1:02:41,  1.16it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  53%|█████▎    | 4901/9281 [1:09:48<1:02:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  53%|█████▎    | 4902/9281 [1:09:49<1:02:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  53%|█████▎    | 4903/9281 [1:09:50<1:02:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  53%|█████▎    | 4904/9281 [1:09:51<1:02:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  53%|█████▎    | 4905/9281 [1:09:52<1:02:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  53%|█████▎    | 4906/9281 [1:09:53<1:02:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.81e-01:  53%|█████▎    | 4907/9281 [1:09:54<1:02:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  53%|█████▎    | 4908/9281 [1:09:54<1:02:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  53%|█████▎    | 4909/9281 [1:09:55<1:02:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  53%|█████▎    | 4910/9281 [1:09:56<1:02:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  53%|█████▎    | 4911/9281 [1:09:57<1:02:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  53%|█████▎    | 4912/9281 [1:09:58<1:01:56,  1.18it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  53%|█████▎    | 4913/9281 [1:09:59<1:02:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  53%|█████▎    | 4914/9281 [1:10:00<1:02:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  53%|█████▎    | 4915/9281 [1:10:00<1:02:17,  1.17it/s]\u001b[A\n",
      "Training loss: 5.66e-01:  53%|█████▎    | 4916/9281 [1:10:01<1:02:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  53%|█████▎    | 4917/9281 [1:10:02<1:02:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  53%|█████▎    | 4918/9281 [1:10:03<1:02:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  53%|█████▎    | 4919/9281 [1:10:04<1:02:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  53%|█████▎    | 4920/9281 [1:10:05<1:02:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.92e-01:  53%|█████▎    | 4921/9281 [1:10:06<1:02:00,  1.17it/s]\u001b[A\n",
      "Training loss: 5.18e-01:  53%|█████▎    | 4922/9281 [1:10:06<1:02:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  53%|█████▎    | 4923/9281 [1:10:07<1:02:09,  1.17it/s]\u001b[A\n",
      "Training loss: 5.34e-01:  53%|█████▎    | 4924/9281 [1:10:08<1:02:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.99e-01:  53%|█████▎    | 4925/9281 [1:10:09<1:01:57,  1.17it/s]\u001b[A\n",
      "Training loss: 5.20e-01:  53%|█████▎    | 4926/9281 [1:10:10<1:02:06,  1.17it/s]\u001b[A\n",
      "Training loss: 5.32e-01:  53%|█████▎    | 4927/9281 [1:10:11<1:01:56,  1.17it/s]\u001b[A\n",
      "Training loss: 5.14e-01:  53%|█████▎    | 4928/9281 [1:10:12<1:02:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  53%|█████▎    | 4929/9281 [1:10:12<1:02:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  53%|█████▎    | 4930/9281 [1:10:13<1:01:58,  1.17it/s]\u001b[A\n",
      "Training loss: 5.60e-01:  53%|█████▎    | 4931/9281 [1:10:14<1:01:56,  1.17it/s]\u001b[A\n",
      "Training loss: 5.22e-01:  53%|█████▎    | 4932/9281 [1:10:15<1:02:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.67e-01:  53%|█████▎    | 4933/9281 [1:10:16<1:01:55,  1.17it/s]\u001b[A\n",
      "Training loss: 5.45e-01:  53%|█████▎    | 4934/9281 [1:10:17<1:01:57,  1.17it/s]\u001b[A\n",
      "Training loss: 5.02e-01:  53%|█████▎    | 4935/9281 [1:10:18<1:01:51,  1.17it/s]\u001b[A\n",
      "Training loss: 5.35e-01:  53%|█████▎    | 4936/9281 [1:10:18<1:01:50,  1.17it/s]\u001b[A\n",
      "Training loss: 5.03e-01:  53%|█████▎    | 4937/9281 [1:10:19<1:01:39,  1.17it/s]\u001b[A\n",
      "Training loss: 5.39e-01:  53%|█████▎    | 4938/9281 [1:10:20<1:01:51,  1.17it/s]\u001b[A\n",
      "Training loss: 5.02e-01:  53%|█████▎    | 4939/9281 [1:10:21<1:01:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  53%|█████▎    | 4940/9281 [1:10:22<1:01:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  53%|█████▎    | 4941/9281 [1:10:23<1:01:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  53%|█████▎    | 4942/9281 [1:10:24<1:01:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  53%|█████▎    | 4943/9281 [1:10:24<1:01:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  53%|█████▎    | 4944/9281 [1:10:25<1:01:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  53%|█████▎    | 4945/9281 [1:10:26<1:01:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  53%|█████▎    | 4946/9281 [1:10:27<1:01:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  53%|█████▎    | 4947/9281 [1:10:28<1:01:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  53%|█████▎    | 4948/9281 [1:10:29<1:01:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  53%|█████▎    | 4949/9281 [1:10:30<1:01:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.54e-01:  53%|█████▎    | 4950/9281 [1:10:30<1:01:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  53%|█████▎    | 4951/9281 [1:10:31<1:01:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.30e-01:  53%|█████▎    | 4952/9281 [1:10:32<1:01:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  53%|█████▎    | 4953/9281 [1:10:33<1:01:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  53%|█████▎    | 4954/9281 [1:10:34<1:01:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  53%|█████▎    | 4955/9281 [1:10:35<1:01:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  53%|█████▎    | 4956/9281 [1:10:35<1:01:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  53%|█████▎    | 4957/9281 [1:10:36<1:01:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  53%|█████▎    | 4958/9281 [1:10:37<1:01:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.54e-01:  53%|█████▎    | 4959/9281 [1:10:38<1:01:17,  1.18it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  53%|█████▎    | 4960/9281 [1:10:39<1:01:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  53%|█████▎    | 4961/9281 [1:10:40<1:01:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  53%|█████▎    | 4962/9281 [1:10:41<1:01:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  53%|█████▎    | 4963/9281 [1:10:41<1:01:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  53%|█████▎    | 4964/9281 [1:10:42<1:01:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  53%|█████▎    | 4965/9281 [1:10:43<1:01:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  54%|█████▎    | 4966/9281 [1:10:44<1:01:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  54%|█████▎    | 4967/9281 [1:10:45<1:01:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  54%|█████▎    | 4968/9281 [1:10:46<1:01:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  54%|█████▎    | 4969/9281 [1:10:47<1:01:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  54%|█████▎    | 4970/9281 [1:10:47<1:01:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  54%|█████▎    | 4971/9281 [1:10:48<1:01:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  54%|█████▎    | 4972/9281 [1:10:49<1:01:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  54%|█████▎    | 4973/9281 [1:10:50<1:01:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  54%|█████▎    | 4974/9281 [1:10:51<1:01:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  54%|█████▎    | 4975/9281 [1:10:52<1:01:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.82e-01:  54%|█████▎    | 4976/9281 [1:10:53<1:01:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.86e-01:  54%|█████▎    | 4977/9281 [1:10:53<1:01:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  54%|█████▎    | 4978/9281 [1:10:54<1:01:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  54%|█████▎    | 4979/9281 [1:10:55<1:01:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  54%|█████▎    | 4980/9281 [1:10:56<1:01:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  54%|█████▎    | 4981/9281 [1:10:57<1:01:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  54%|█████▎    | 4982/9281 [1:10:58<1:01:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  54%|█████▎    | 4983/9281 [1:10:59<1:01:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  54%|█████▎    | 4984/9281 [1:10:59<1:01:03,  1.17it/s]\u001b[A\n",
      "Training loss: 5.35e-01:  54%|█████▎    | 4985/9281 [1:11:00<1:01:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.88e-01:  54%|█████▎    | 4986/9281 [1:11:01<1:00:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.79e-01:  54%|█████▎    | 4987/9281 [1:11:02<1:01:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.86e-01:  54%|█████▎    | 4988/9281 [1:11:03<1:00:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  54%|█████▍    | 4989/9281 [1:11:04<1:01:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  54%|█████▍    | 4990/9281 [1:11:05<1:01:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  54%|█████▍    | 4991/9281 [1:11:05<1:00:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  54%|█████▍    | 4992/9281 [1:11:06<1:01:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  54%|█████▍    | 4993/9281 [1:11:07<1:01:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  54%|█████▍    | 4994/9281 [1:11:08<1:01:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.19e-01:  54%|█████▍    | 4995/9281 [1:11:09<1:01:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  54%|█████▍    | 4996/9281 [1:11:10<1:01:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  54%|█████▍    | 4997/9281 [1:11:11<1:01:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  54%|█████▍    | 4998/9281 [1:11:11<1:01:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  54%|█████▍    | 4999/9281 [1:11:12<1:01:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  54%|█████▍    | 5000/9281 [1:11:13<1:01:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  54%|█████▍    | 5001/9281 [1:11:14<1:01:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  54%|█████▍    | 5002/9281 [1:11:15<1:01:18,  1.16it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  54%|█████▍    | 5003/9281 [1:11:16<1:01:16,  1.16it/s]\u001b[A\n",
      "Training loss: 5.02e-01:  54%|█████▍    | 5004/9281 [1:11:17<1:01:17,  1.16it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  54%|█████▍    | 5005/9281 [1:11:17<1:01:07,  1.17it/s]\u001b[A\n",
      "Training loss: 5.05e-01:  54%|█████▍    | 5006/9281 [1:11:18<1:01:18,  1.16it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  54%|█████▍    | 5007/9281 [1:11:19<1:01:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  54%|█████▍    | 5008/9281 [1:11:20<1:01:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  54%|█████▍    | 5009/9281 [1:11:21<1:01:02,  1.17it/s]\u001b[A\n",
      "Training loss: 5.06e-01:  54%|█████▍    | 5010/9281 [1:11:22<1:00:52,  1.17it/s]\u001b[A\n",
      "Training loss: 5.06e-01:  54%|█████▍    | 5011/9281 [1:11:23<1:00:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  54%|█████▍    | 5012/9281 [1:11:23<1:00:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  54%|█████▍    | 5013/9281 [1:11:24<1:00:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  54%|█████▍    | 5014/9281 [1:11:25<1:00:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.96e-01:  54%|█████▍    | 5015/9281 [1:11:26<1:00:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.98e-01:  54%|█████▍    | 5016/9281 [1:11:27<1:00:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  54%|█████▍    | 5017/9281 [1:11:28<1:00:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  54%|█████▍    | 5018/9281 [1:11:29<1:00:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  54%|█████▍    | 5019/9281 [1:11:29<1:00:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  54%|█████▍    | 5020/9281 [1:11:30<1:00:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  54%|█████▍    | 5021/9281 [1:11:31<1:00:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  54%|█████▍    | 5022/9281 [1:11:32<1:00:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  54%|█████▍    | 5023/9281 [1:11:33<1:00:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  54%|█████▍    | 5024/9281 [1:11:34<1:00:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  54%|█████▍    | 5025/9281 [1:11:35<1:00:55,  1.16it/s]\u001b[A\n",
      "Training loss: 4.79e-01:  54%|█████▍    | 5026/9281 [1:11:35<1:00:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  54%|█████▍    | 5027/9281 [1:11:36<1:00:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  54%|█████▍    | 5028/9281 [1:11:37<1:00:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  54%|█████▍    | 5029/9281 [1:11:38<1:00:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  54%|█████▍    | 5030/9281 [1:11:39<1:00:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  54%|█████▍    | 5031/9281 [1:11:40<1:00:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  54%|█████▍    | 5032/9281 [1:11:41<1:00:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  54%|█████▍    | 5033/9281 [1:11:41<1:00:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  54%|█████▍    | 5034/9281 [1:11:42<1:00:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  54%|█████▍    | 5035/9281 [1:11:43<1:00:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.67e-01:  54%|█████▍    | 5036/9281 [1:11:44<1:00:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  54%|█████▍    | 5037/9281 [1:11:45<1:00:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  54%|█████▍    | 5038/9281 [1:11:46<1:00:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  54%|█████▍    | 5039/9281 [1:11:46<1:00:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  54%|█████▍    | 5040/9281 [1:11:47<1:00:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  54%|█████▍    | 5041/9281 [1:11:48<1:00:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  54%|█████▍    | 5042/9281 [1:11:49<1:00:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  54%|█████▍    | 5043/9281 [1:11:50<1:00:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  54%|█████▍    | 5044/9281 [1:11:51<1:00:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  54%|█████▍    | 5045/9281 [1:11:52<1:00:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  54%|█████▍    | 5046/9281 [1:11:52<1:00:02,  1.18it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  54%|█████▍    | 5047/9281 [1:11:53<1:00:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  54%|█████▍    | 5048/9281 [1:11:54<1:00:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  54%|█████▍    | 5049/9281 [1:11:55<1:00:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  54%|█████▍    | 5050/9281 [1:11:56<1:00:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  54%|█████▍    | 5051/9281 [1:11:57<1:00:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  54%|█████▍    | 5052/9281 [1:11:58<1:00:31,  1.16it/s]\u001b[A\n",
      "Training loss: 5.37e-01:  54%|█████▍    | 5053/9281 [1:11:58<1:00:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  54%|█████▍    | 5054/9281 [1:11:59<1:00:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  54%|█████▍    | 5055/9281 [1:12:00<1:00:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  54%|█████▍    | 5056/9281 [1:12:01<1:00:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  54%|█████▍    | 5057/9281 [1:12:02<1:00:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  54%|█████▍    | 5058/9281 [1:12:03<1:00:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  55%|█████▍    | 5059/9281 [1:12:04<1:00:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  55%|█████▍    | 5060/9281 [1:12:04<1:00:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  55%|█████▍    | 5061/9281 [1:12:05<1:00:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  55%|█████▍    | 5062/9281 [1:12:06<1:00:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  55%|█████▍    | 5063/9281 [1:12:07<1:00:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  55%|█████▍    | 5064/9281 [1:12:08<1:00:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  55%|█████▍    | 5065/9281 [1:12:09<1:00:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  55%|█████▍    | 5066/9281 [1:12:10<1:00:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  55%|█████▍    | 5067/9281 [1:12:10<1:00:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  55%|█████▍    | 5068/9281 [1:12:11<1:00:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  55%|█████▍    | 5069/9281 [1:12:12<59:51,  1.17it/s]  \u001b[A\n",
      "Training loss: 4.34e-01:  55%|█████▍    | 5070/9281 [1:12:13<59:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  55%|█████▍    | 5071/9281 [1:12:14<59:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  55%|█████▍    | 5072/9281 [1:12:15<59:38,  1.18it/s]\u001b[A\n",
      "Training loss: 5.16e-01:  55%|█████▍    | 5073/9281 [1:12:16<59:39,  1.18it/s]\u001b[A\n",
      "Training loss: 4.82e-01:  55%|█████▍    | 5074/9281 [1:12:16<59:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.96e-01:  55%|█████▍    | 5075/9281 [1:12:17<59:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  55%|█████▍    | 5076/9281 [1:12:18<59:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  55%|█████▍    | 5077/9281 [1:12:19<59:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  55%|█████▍    | 5078/9281 [1:12:20<59:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  55%|█████▍    | 5079/9281 [1:12:21<59:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  55%|█████▍    | 5080/9281 [1:12:22<59:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  55%|█████▍    | 5081/9281 [1:12:22<59:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  55%|█████▍    | 5082/9281 [1:12:23<59:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  55%|█████▍    | 5083/9281 [1:12:24<59:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  55%|█████▍    | 5084/9281 [1:12:25<59:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  55%|█████▍    | 5085/9281 [1:12:26<1:00:07,  1.16it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  55%|█████▍    | 5086/9281 [1:12:27<59:51,  1.17it/s]  \u001b[A\n",
      "Training loss: 5.01e-01:  55%|█████▍    | 5087/9281 [1:12:28<1:00:02,  1.16it/s]\u001b[A\n",
      "Training loss: 4.82e-01:  55%|█████▍    | 5088/9281 [1:12:28<59:47,  1.17it/s]  \u001b[A\n",
      "Training loss: 4.77e-01:  55%|█████▍    | 5089/9281 [1:12:29<59:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.88e-01:  55%|█████▍    | 5090/9281 [1:12:30<59:42,  1.17it/s]\u001b[A\n",
      "Training loss: 5.03e-01:  55%|█████▍    | 5091/9281 [1:12:31<59:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  55%|█████▍    | 5092/9281 [1:12:32<59:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  55%|█████▍    | 5093/9281 [1:12:33<59:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  55%|█████▍    | 5094/9281 [1:12:34<59:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  55%|█████▍    | 5095/9281 [1:12:34<59:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.84e-01:  55%|█████▍    | 5096/9281 [1:12:35<59:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  55%|█████▍    | 5097/9281 [1:12:36<59:54,  1.16it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  55%|█████▍    | 5098/9281 [1:12:37<59:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  55%|█████▍    | 5099/9281 [1:12:38<59:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  55%|█████▍    | 5100/9281 [1:12:39<59:51,  1.16it/s]\u001b[A\n",
      "Training loss: 5.19e-01:  55%|█████▍    | 5101/9281 [1:12:40<59:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.81e-01:  55%|█████▍    | 5102/9281 [1:12:40<59:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  55%|█████▍    | 5103/9281 [1:12:41<59:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  55%|█████▍    | 5104/9281 [1:12:42<59:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  55%|█████▌    | 5105/9281 [1:12:43<59:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  55%|█████▌    | 5106/9281 [1:12:44<59:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  55%|█████▌    | 5107/9281 [1:12:45<59:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  55%|█████▌    | 5108/9281 [1:12:45<59:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  55%|█████▌    | 5109/9281 [1:12:46<59:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  55%|█████▌    | 5110/9281 [1:12:47<59:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  55%|█████▌    | 5111/9281 [1:12:48<59:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  55%|█████▌    | 5112/9281 [1:12:49<59:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  55%|█████▌    | 5113/9281 [1:12:50<59:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  55%|█████▌    | 5114/9281 [1:12:51<59:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  55%|█████▌    | 5115/9281 [1:12:51<59:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  55%|█████▌    | 5116/9281 [1:12:52<59:26,  1.17it/s]\u001b[A\n",
      "Training loss: 5.07e-01:  55%|█████▌    | 5117/9281 [1:12:53<59:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  55%|█████▌    | 5118/9281 [1:12:54<59:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  55%|█████▌    | 5119/9281 [1:12:55<59:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  55%|█████▌    | 5120/9281 [1:12:56<59:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.68e-01:  55%|█████▌    | 5121/9281 [1:12:57<59:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  55%|█████▌    | 5122/9281 [1:12:57<59:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  55%|█████▌    | 5123/9281 [1:12:58<59:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  55%|█████▌    | 5124/9281 [1:12:59<59:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  55%|█████▌    | 5125/9281 [1:13:00<59:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  55%|█████▌    | 5126/9281 [1:13:01<59:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  55%|█████▌    | 5127/9281 [1:13:02<59:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  55%|█████▌    | 5128/9281 [1:13:03<59:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  55%|█████▌    | 5129/9281 [1:13:03<59:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  55%|█████▌    | 5130/9281 [1:13:04<58:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  55%|█████▌    | 5131/9281 [1:13:05<59:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  55%|█████▌    | 5132/9281 [1:13:06<58:50,  1.18it/s]\u001b[A\n",
      "Training loss: 4.96e-01:  55%|█████▌    | 5133/9281 [1:13:07<58:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  55%|█████▌    | 5134/9281 [1:13:08<58:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  55%|█████▌    | 5135/9281 [1:13:09<59:13,  1.17it/s]\u001b[A\n",
      "Training loss: 5.09e-01:  55%|█████▌    | 5136/9281 [1:13:09<59:01,  1.17it/s]\u001b[A\n",
      "Training loss: 5.02e-01:  55%|█████▌    | 5137/9281 [1:13:10<59:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  55%|█████▌    | 5138/9281 [1:13:11<58:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  55%|█████▌    | 5139/9281 [1:13:12<58:58,  1.17it/s]\u001b[A\n",
      "Training loss: 5.19e-01:  55%|█████▌    | 5140/9281 [1:13:13<58:56,  1.17it/s]\u001b[A\n",
      "Training loss: 5.02e-01:  55%|█████▌    | 5141/9281 [1:13:14<59:19,  1.16it/s]\u001b[A\n",
      "Training loss: 5.14e-01:  55%|█████▌    | 5142/9281 [1:13:15<59:18,  1.16it/s]\u001b[A\n",
      "Training loss: 5.14e-01:  55%|█████▌    | 5143/9281 [1:13:15<59:11,  1.17it/s]\u001b[A\n",
      "Training loss: 5.42e-01:  55%|█████▌    | 5144/9281 [1:13:16<59:02,  1.17it/s]\u001b[A\n",
      "Training loss: 5.14e-01:  55%|█████▌    | 5145/9281 [1:13:17<59:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.97e-01:  55%|█████▌    | 5146/9281 [1:13:18<58:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  55%|█████▌    | 5147/9281 [1:13:19<58:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  55%|█████▌    | 5148/9281 [1:13:20<58:42,  1.17it/s]\u001b[A\n",
      "Training loss: 5.15e-01:  55%|█████▌    | 5149/9281 [1:13:21<58:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.69e-01:  55%|█████▌    | 5150/9281 [1:13:21<58:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  56%|█████▌    | 5151/9281 [1:13:22<58:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  56%|█████▌    | 5152/9281 [1:13:23<58:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  56%|█████▌    | 5153/9281 [1:13:24<58:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  56%|█████▌    | 5154/9281 [1:13:25<58:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  56%|█████▌    | 5155/9281 [1:13:26<58:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  56%|█████▌    | 5156/9281 [1:13:27<58:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  56%|█████▌    | 5157/9281 [1:13:27<58:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  56%|█████▌    | 5158/9281 [1:13:28<58:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  56%|█████▌    | 5159/9281 [1:13:29<58:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.99e-01:  56%|█████▌    | 5160/9281 [1:13:30<58:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  56%|█████▌    | 5161/9281 [1:13:31<58:26,  1.17it/s]\u001b[A\n",
      "Training loss: 5.29e-01:  56%|█████▌    | 5162/9281 [1:13:32<58:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  56%|█████▌    | 5163/9281 [1:13:32<58:46,  1.17it/s]\u001b[A\n",
      "Training loss: 5.25e-01:  56%|█████▌    | 5164/9281 [1:13:33<58:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  56%|█████▌    | 5165/9281 [1:13:34<58:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  56%|█████▌    | 5166/9281 [1:13:35<58:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  56%|█████▌    | 5167/9281 [1:13:36<58:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  56%|█████▌    | 5168/9281 [1:13:37<58:17,  1.18it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  56%|█████▌    | 5169/9281 [1:13:38<58:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  56%|█████▌    | 5170/9281 [1:13:38<58:17,  1.18it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  56%|█████▌    | 5171/9281 [1:13:39<58:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  56%|█████▌    | 5172/9281 [1:13:40<58:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  56%|█████▌    | 5173/9281 [1:13:41<58:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  56%|█████▌    | 5174/9281 [1:13:42<58:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  56%|█████▌    | 5175/9281 [1:13:43<58:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  56%|█████▌    | 5176/9281 [1:13:44<58:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  56%|█████▌    | 5177/9281 [1:13:44<58:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  56%|█████▌    | 5178/9281 [1:13:45<58:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  56%|█████▌    | 5179/9281 [1:13:46<58:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  56%|█████▌    | 5180/9281 [1:13:47<58:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  56%|█████▌    | 5181/9281 [1:13:48<58:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  56%|█████▌    | 5182/9281 [1:13:49<58:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  56%|█████▌    | 5183/9281 [1:13:50<58:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  56%|█████▌    | 5184/9281 [1:13:50<58:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  56%|█████▌    | 5185/9281 [1:13:51<58:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  56%|█████▌    | 5186/9281 [1:13:52<58:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  56%|█████▌    | 5187/9281 [1:13:53<58:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  56%|█████▌    | 5188/9281 [1:13:54<58:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  56%|█████▌    | 5189/9281 [1:13:55<58:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  56%|█████▌    | 5190/9281 [1:13:56<58:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  56%|█████▌    | 5191/9281 [1:13:56<58:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  56%|█████▌    | 5192/9281 [1:13:57<58:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  56%|█████▌    | 5193/9281 [1:13:58<58:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  56%|█████▌    | 5194/9281 [1:13:59<58:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  56%|█████▌    | 5195/9281 [1:14:00<58:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  56%|█████▌    | 5196/9281 [1:14:01<58:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  56%|█████▌    | 5197/9281 [1:14:02<58:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  56%|█████▌    | 5198/9281 [1:14:02<58:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  56%|█████▌    | 5199/9281 [1:14:03<57:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  56%|█████▌    | 5200/9281 [1:14:04<57:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  56%|█████▌    | 5201/9281 [1:14:05<57:57,  1.17it/s]\u001b[A\n",
      "Training loss: 5.05e-01:  56%|█████▌    | 5202/9281 [1:14:06<57:52,  1.17it/s]\u001b[A\n",
      "Training loss: 5.48e-01:  56%|█████▌    | 5203/9281 [1:14:07<57:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.95e-01:  56%|█████▌    | 5204/9281 [1:14:08<58:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  56%|█████▌    | 5205/9281 [1:14:08<58:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  56%|█████▌    | 5206/9281 [1:14:09<57:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  56%|█████▌    | 5207/9281 [1:14:10<58:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  56%|█████▌    | 5208/9281 [1:14:11<58:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  56%|█████▌    | 5209/9281 [1:14:12<58:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  56%|█████▌    | 5210/9281 [1:14:13<58:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  56%|█████▌    | 5211/9281 [1:14:13<57:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  56%|█████▌    | 5212/9281 [1:14:14<58:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  56%|█████▌    | 5213/9281 [1:14:15<58:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  56%|█████▌    | 5214/9281 [1:14:16<57:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  56%|█████▌    | 5215/9281 [1:14:17<57:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  56%|█████▌    | 5216/9281 [1:14:18<58:16,  1.16it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  56%|█████▌    | 5217/9281 [1:14:19<58:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  56%|█████▌    | 5218/9281 [1:14:19<58:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  56%|█████▌    | 5219/9281 [1:14:20<57:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.84e-01:  56%|█████▌    | 5220/9281 [1:14:21<58:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  56%|█████▋    | 5221/9281 [1:14:22<57:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  56%|█████▋    | 5222/9281 [1:14:23<57:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  56%|█████▋    | 5223/9281 [1:14:24<57:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  56%|█████▋    | 5224/9281 [1:14:25<57:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  56%|█████▋    | 5225/9281 [1:14:25<57:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  56%|█████▋    | 5226/9281 [1:14:26<57:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  56%|█████▋    | 5227/9281 [1:14:27<57:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.86e-01:  56%|█████▋    | 5228/9281 [1:14:28<57:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  56%|█████▋    | 5229/9281 [1:14:29<57:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  56%|█████▋    | 5230/9281 [1:14:30<57:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  56%|█████▋    | 5231/9281 [1:14:31<57:37,  1.17it/s]\u001b[A\n",
      "Training loss: 5.53e-01:  56%|█████▋    | 5232/9281 [1:14:31<57:37,  1.17it/s]\u001b[A\n",
      "Training loss: 5.79e-01:  56%|█████▋    | 5233/9281 [1:14:32<57:23,  1.18it/s]\u001b[A\n",
      "Training loss: 5.22e-01:  56%|█████▋    | 5234/9281 [1:14:33<57:37,  1.17it/s]\u001b[A\n",
      "Training loss: 5.27e-01:  56%|█████▋    | 5235/9281 [1:14:34<57:39,  1.17it/s]\u001b[A\n",
      "Training loss: 5.30e-01:  56%|█████▋    | 5236/9281 [1:14:35<57:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.68e-01:  56%|█████▋    | 5237/9281 [1:14:36<57:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.97e-01:  56%|█████▋    | 5238/9281 [1:14:37<57:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  56%|█████▋    | 5239/9281 [1:14:37<57:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  56%|█████▋    | 5240/9281 [1:14:38<57:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  56%|█████▋    | 5241/9281 [1:14:39<57:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  56%|█████▋    | 5242/9281 [1:14:40<57:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  56%|█████▋    | 5243/9281 [1:14:41<57:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  57%|█████▋    | 5244/9281 [1:14:42<57:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  57%|█████▋    | 5245/9281 [1:14:43<57:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  57%|█████▋    | 5246/9281 [1:14:43<57:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  57%|█████▋    | 5247/9281 [1:14:44<57:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  57%|█████▋    | 5248/9281 [1:14:45<57:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  57%|█████▋    | 5249/9281 [1:14:46<57:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  57%|█████▋    | 5250/9281 [1:14:47<57:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  57%|█████▋    | 5251/9281 [1:14:48<57:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  57%|█████▋    | 5252/9281 [1:14:49<57:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  57%|█████▋    | 5253/9281 [1:14:49<57:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  57%|█████▋    | 5254/9281 [1:14:50<57:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  57%|█████▋    | 5255/9281 [1:14:51<57:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  57%|█████▋    | 5256/9281 [1:14:52<57:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  57%|█████▋    | 5257/9281 [1:14:53<57:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  57%|█████▋    | 5258/9281 [1:14:54<57:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  57%|█████▋    | 5259/9281 [1:14:55<57:32,  1.16it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  57%|█████▋    | 5260/9281 [1:14:55<57:31,  1.16it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  57%|█████▋    | 5261/9281 [1:14:56<57:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  57%|█████▋    | 5262/9281 [1:14:57<57:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  57%|█████▋    | 5263/9281 [1:14:58<57:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  57%|█████▋    | 5264/9281 [1:14:59<57:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  57%|█████▋    | 5265/9281 [1:15:00<57:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  57%|█████▋    | 5266/9281 [1:15:01<57:34,  1.16it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  57%|█████▋    | 5267/9281 [1:15:01<57:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  57%|█████▋    | 5268/9281 [1:15:02<57:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  57%|█████▋    | 5269/9281 [1:15:03<56:59,  1.17it/s]\u001b[A\n",
      "Training loss: 5.21e-01:  57%|█████▋    | 5270/9281 [1:15:04<57:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  57%|█████▋    | 5271/9281 [1:15:05<56:52,  1.18it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  57%|█████▋    | 5272/9281 [1:15:06<57:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.54e-01:  57%|█████▋    | 5273/9281 [1:15:07<56:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  57%|█████▋    | 5274/9281 [1:15:07<57:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  57%|█████▋    | 5275/9281 [1:15:08<57:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  57%|█████▋    | 5276/9281 [1:15:09<57:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  57%|█████▋    | 5277/9281 [1:15:10<56:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  57%|█████▋    | 5278/9281 [1:15:11<56:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  57%|█████▋    | 5279/9281 [1:15:12<56:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  57%|█████▋    | 5280/9281 [1:15:12<56:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  57%|█████▋    | 5281/9281 [1:15:13<56:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  57%|█████▋    | 5282/9281 [1:15:14<56:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  57%|█████▋    | 5283/9281 [1:15:15<56:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  57%|█████▋    | 5284/9281 [1:15:16<56:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  57%|█████▋    | 5285/9281 [1:15:17<56:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  57%|█████▋    | 5286/9281 [1:15:18<56:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  57%|█████▋    | 5287/9281 [1:15:18<56:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  57%|█████▋    | 5288/9281 [1:15:19<56:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  57%|█████▋    | 5289/9281 [1:15:20<56:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  57%|█████▋    | 5290/9281 [1:15:21<56:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  57%|█████▋    | 5291/9281 [1:15:22<56:29,  1.18it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  57%|█████▋    | 5292/9281 [1:15:23<56:34,  1.18it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  57%|█████▋    | 5293/9281 [1:15:24<56:31,  1.18it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  57%|█████▋    | 5294/9281 [1:15:24<56:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  57%|█████▋    | 5295/9281 [1:15:25<56:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  57%|█████▋    | 5296/9281 [1:15:26<56:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  57%|█████▋    | 5297/9281 [1:15:27<56:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  57%|█████▋    | 5298/9281 [1:15:28<56:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  57%|█████▋    | 5299/9281 [1:15:29<56:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  57%|█████▋    | 5300/9281 [1:15:30<56:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  57%|█████▋    | 5301/9281 [1:15:30<56:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  57%|█████▋    | 5302/9281 [1:15:31<56:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  57%|█████▋    | 5303/9281 [1:15:32<56:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  57%|█████▋    | 5304/9281 [1:15:33<56:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  57%|█████▋    | 5305/9281 [1:15:34<56:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  57%|█████▋    | 5306/9281 [1:15:35<56:35,  1.17it/s]\u001b[A\n",
      "Training loss: 5.06e-01:  57%|█████▋    | 5307/9281 [1:15:36<56:33,  1.17it/s]\u001b[A\n",
      "Training loss: 6.15e-01:  57%|█████▋    | 5308/9281 [1:15:36<56:29,  1.17it/s]\u001b[A\n",
      "Training loss: 5.46e-01:  57%|█████▋    | 5309/9281 [1:15:37<56:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  57%|█████▋    | 5310/9281 [1:15:38<56:39,  1.17it/s]\u001b[A\n",
      "Training loss: 5.18e-01:  57%|█████▋    | 5311/9281 [1:15:39<56:36,  1.17it/s]\u001b[A\n",
      "Training loss: 5.14e-01:  57%|█████▋    | 5312/9281 [1:15:40<56:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  57%|█████▋    | 5313/9281 [1:15:41<56:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  57%|█████▋    | 5314/9281 [1:15:42<56:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  57%|█████▋    | 5315/9281 [1:15:42<56:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  57%|█████▋    | 5316/9281 [1:15:43<56:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  57%|█████▋    | 5317/9281 [1:15:44<56:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  57%|█████▋    | 5318/9281 [1:15:45<56:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  57%|█████▋    | 5319/9281 [1:15:46<56:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  57%|█████▋    | 5320/9281 [1:15:47<56:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  57%|█████▋    | 5321/9281 [1:15:48<56:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  57%|█████▋    | 5322/9281 [1:15:48<56:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  57%|█████▋    | 5323/9281 [1:15:49<56:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  57%|█████▋    | 5324/9281 [1:15:50<56:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  57%|█████▋    | 5325/9281 [1:15:51<56:06,  1.18it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  57%|█████▋    | 5326/9281 [1:15:52<56:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.30e-01:  57%|█████▋    | 5327/9281 [1:15:53<56:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  57%|█████▋    | 5328/9281 [1:15:53<56:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  57%|█████▋    | 5329/9281 [1:15:54<56:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  57%|█████▋    | 5330/9281 [1:15:55<56:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  57%|█████▋    | 5331/9281 [1:15:56<56:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  57%|█████▋    | 5332/9281 [1:15:57<56:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  57%|█████▋    | 5333/9281 [1:15:58<56:29,  1.16it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  57%|█████▋    | 5334/9281 [1:15:59<56:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  57%|█████▋    | 5335/9281 [1:15:59<56:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  57%|█████▋    | 5336/9281 [1:16:00<56:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  58%|█████▊    | 5337/9281 [1:16:01<56:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  58%|█████▊    | 5338/9281 [1:16:02<56:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  58%|█████▊    | 5339/9281 [1:16:03<56:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  58%|█████▊    | 5340/9281 [1:16:04<56:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  58%|█████▊    | 5341/9281 [1:16:05<56:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  58%|█████▊    | 5342/9281 [1:16:05<56:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  58%|█████▊    | 5343/9281 [1:16:06<56:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  58%|█████▊    | 5344/9281 [1:16:07<55:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  58%|█████▊    | 5345/9281 [1:16:08<56:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  58%|█████▊    | 5346/9281 [1:16:09<56:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  58%|█████▊    | 5347/9281 [1:16:10<56:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  58%|█████▊    | 5348/9281 [1:16:11<56:18,  1.16it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  58%|█████▊    | 5349/9281 [1:16:11<56:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  58%|█████▊    | 5350/9281 [1:16:12<56:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  58%|█████▊    | 5351/9281 [1:16:13<56:19,  1.16it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  58%|█████▊    | 5352/9281 [1:16:14<56:17,  1.16it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  58%|█████▊    | 5353/9281 [1:16:15<56:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  58%|█████▊    | 5354/9281 [1:16:16<56:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  58%|█████▊    | 5355/9281 [1:16:17<55:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  58%|█████▊    | 5356/9281 [1:16:17<55:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  58%|█████▊    | 5357/9281 [1:16:18<55:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  58%|█████▊    | 5358/9281 [1:16:19<55:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  58%|█████▊    | 5359/9281 [1:16:20<55:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  58%|█████▊    | 5360/9281 [1:16:21<55:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  58%|█████▊    | 5361/9281 [1:16:22<55:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  58%|█████▊    | 5362/9281 [1:16:23<55:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  58%|█████▊    | 5363/9281 [1:16:23<55:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  58%|█████▊    | 5364/9281 [1:16:24<55:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  58%|█████▊    | 5365/9281 [1:16:25<55:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  58%|█████▊    | 5366/9281 [1:16:26<55:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  58%|█████▊    | 5367/9281 [1:16:27<55:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  58%|█████▊    | 5368/9281 [1:16:28<55:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  58%|█████▊    | 5369/9281 [1:16:29<55:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  58%|█████▊    | 5370/9281 [1:16:29<55:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  58%|█████▊    | 5371/9281 [1:16:30<55:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  58%|█████▊    | 5372/9281 [1:16:31<55:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  58%|█████▊    | 5373/9281 [1:16:32<55:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  58%|█████▊    | 5374/9281 [1:16:33<55:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  58%|█████▊    | 5375/9281 [1:16:34<55:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.79e-01:  58%|█████▊    | 5376/9281 [1:16:35<55:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  58%|█████▊    | 5377/9281 [1:16:35<55:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  58%|█████▊    | 5378/9281 [1:16:36<55:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  58%|█████▊    | 5379/9281 [1:16:37<55:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  58%|█████▊    | 5380/9281 [1:16:38<55:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  58%|█████▊    | 5381/9281 [1:16:39<55:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  58%|█████▊    | 5382/9281 [1:16:40<55:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  58%|█████▊    | 5383/9281 [1:16:40<55:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  58%|█████▊    | 5384/9281 [1:16:41<55:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  58%|█████▊    | 5385/9281 [1:16:42<55:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  58%|█████▊    | 5386/9281 [1:16:43<55:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  58%|█████▊    | 5387/9281 [1:16:44<55:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  58%|█████▊    | 5388/9281 [1:16:45<55:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  58%|█████▊    | 5389/9281 [1:16:46<55:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  58%|█████▊    | 5390/9281 [1:16:46<55:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  58%|█████▊    | 5391/9281 [1:16:47<55:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  58%|█████▊    | 5392/9281 [1:16:48<55:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  58%|█████▊    | 5393/9281 [1:16:49<55:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  58%|█████▊    | 5394/9281 [1:16:50<55:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  58%|█████▊    | 5395/9281 [1:16:51<55:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  58%|█████▊    | 5396/9281 [1:16:52<55:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  58%|█████▊    | 5397/9281 [1:16:52<55:35,  1.16it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  58%|█████▊    | 5398/9281 [1:16:53<55:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  58%|█████▊    | 5399/9281 [1:16:54<55:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  58%|█████▊    | 5400/9281 [1:16:55<55:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  58%|█████▊    | 5401/9281 [1:16:56<55:30,  1.16it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  58%|█████▊    | 5402/9281 [1:16:57<55:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  58%|█████▊    | 5403/9281 [1:16:58<55:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  58%|█████▊    | 5404/9281 [1:16:58<55:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  58%|█████▊    | 5405/9281 [1:16:59<55:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  58%|█████▊    | 5406/9281 [1:17:00<55:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  58%|█████▊    | 5407/9281 [1:17:01<55:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  58%|█████▊    | 5408/9281 [1:17:02<55:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  58%|█████▊    | 5409/9281 [1:17:03<55:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  58%|█████▊    | 5410/9281 [1:17:04<55:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  58%|█████▊    | 5411/9281 [1:17:04<55:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  58%|█████▊    | 5412/9281 [1:17:05<55:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  58%|█████▊    | 5413/9281 [1:17:06<55:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  58%|█████▊    | 5414/9281 [1:17:07<54:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  58%|█████▊    | 5415/9281 [1:17:08<55:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  58%|█████▊    | 5416/9281 [1:17:09<54:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  58%|█████▊    | 5417/9281 [1:17:10<54:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  58%|█████▊    | 5418/9281 [1:17:10<54:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  58%|█████▊    | 5419/9281 [1:17:11<54:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  58%|█████▊    | 5420/9281 [1:17:12<54:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  58%|█████▊    | 5421/9281 [1:17:13<55:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  58%|█████▊    | 5422/9281 [1:17:14<55:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  58%|█████▊    | 5423/9281 [1:17:15<54:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  58%|█████▊    | 5424/9281 [1:17:16<54:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  58%|█████▊    | 5425/9281 [1:17:16<54:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  58%|█████▊    | 5426/9281 [1:17:17<54:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.99e-01:  58%|█████▊    | 5427/9281 [1:17:18<54:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.96e-01:  58%|█████▊    | 5428/9281 [1:17:19<54:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  58%|█████▊    | 5429/9281 [1:17:20<54:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  59%|█████▊    | 5430/9281 [1:17:21<54:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  59%|█████▊    | 5431/9281 [1:17:22<54:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.86e-01:  59%|█████▊    | 5432/9281 [1:17:22<54:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  59%|█████▊    | 5433/9281 [1:17:23<54:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  59%|█████▊    | 5434/9281 [1:17:24<54:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  59%|█████▊    | 5435/9281 [1:17:25<54:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  59%|█████▊    | 5436/9281 [1:17:26<54:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  59%|█████▊    | 5437/9281 [1:17:27<54:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  59%|█████▊    | 5438/9281 [1:17:27<54:25,  1.18it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  59%|█████▊    | 5439/9281 [1:17:28<54:29,  1.18it/s]\u001b[A\n",
      "Training loss: 3.54e-01:  59%|█████▊    | 5440/9281 [1:17:29<54:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  59%|█████▊    | 5441/9281 [1:17:30<54:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  59%|█████▊    | 5442/9281 [1:17:31<54:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.30e-01:  59%|█████▊    | 5443/9281 [1:17:32<54:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  59%|█████▊    | 5444/9281 [1:17:33<54:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  59%|█████▊    | 5445/9281 [1:17:33<54:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  59%|█████▊    | 5446/9281 [1:17:34<54:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  59%|█████▊    | 5447/9281 [1:17:35<54:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  59%|█████▊    | 5448/9281 [1:17:36<54:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  59%|█████▊    | 5449/9281 [1:17:37<54:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  59%|█████▊    | 5450/9281 [1:17:38<54:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  59%|█████▊    | 5451/9281 [1:17:39<54:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  59%|█████▊    | 5452/9281 [1:17:39<54:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  59%|█████▉    | 5453/9281 [1:17:40<54:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  59%|█████▉    | 5454/9281 [1:17:41<54:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  59%|█████▉    | 5455/9281 [1:17:42<54:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  59%|█████▉    | 5456/9281 [1:17:43<54:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  59%|█████▉    | 5457/9281 [1:17:44<54:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  59%|█████▉    | 5458/9281 [1:17:45<54:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  59%|█████▉    | 5459/9281 [1:17:45<54:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  59%|█████▉    | 5460/9281 [1:17:46<54:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  59%|█████▉    | 5461/9281 [1:17:47<54:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  59%|█████▉    | 5462/9281 [1:17:48<54:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  59%|█████▉    | 5463/9281 [1:17:49<54:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  59%|█████▉    | 5464/9281 [1:17:50<54:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  59%|█████▉    | 5465/9281 [1:17:51<54:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  59%|█████▉    | 5466/9281 [1:17:51<54:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  59%|█████▉    | 5467/9281 [1:17:52<54:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  59%|█████▉    | 5468/9281 [1:17:53<54:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  59%|█████▉    | 5469/9281 [1:17:54<54:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  59%|█████▉    | 5470/9281 [1:17:55<54:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  59%|█████▉    | 5471/9281 [1:17:56<54:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  59%|█████▉    | 5472/9281 [1:17:57<54:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  59%|█████▉    | 5473/9281 [1:17:57<54:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  59%|█████▉    | 5474/9281 [1:17:58<54:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  59%|█████▉    | 5475/9281 [1:17:59<53:58,  1.18it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  59%|█████▉    | 5476/9281 [1:18:00<54:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  59%|█████▉    | 5477/9281 [1:18:01<54:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  59%|█████▉    | 5478/9281 [1:18:02<54:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  59%|█████▉    | 5479/9281 [1:18:03<54:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  59%|█████▉    | 5480/9281 [1:18:03<54:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  59%|█████▉    | 5481/9281 [1:18:04<54:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  59%|█████▉    | 5482/9281 [1:18:05<53:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  59%|█████▉    | 5483/9281 [1:18:06<53:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  59%|█████▉    | 5484/9281 [1:18:07<54:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  59%|█████▉    | 5485/9281 [1:18:08<54:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.29e-01:  59%|█████▉    | 5486/9281 [1:18:08<54:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.24e-01:  59%|█████▉    | 5487/9281 [1:18:09<53:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  59%|█████▉    | 5488/9281 [1:18:10<54:05,  1.17it/s]\u001b[A\n",
      "Training loss: 2.95e-01:  59%|█████▉    | 5489/9281 [1:18:11<54:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.14e-01:  59%|█████▉    | 5490/9281 [1:18:12<53:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  59%|█████▉    | 5491/9281 [1:18:13<53:44,  1.18it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  59%|█████▉    | 5492/9281 [1:18:14<53:41,  1.18it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  59%|█████▉    | 5493/9281 [1:18:14<53:46,  1.17it/s]\u001b[A\n",
      "Training loss: 5.00e-01:  59%|█████▉    | 5494/9281 [1:18:15<53:33,  1.18it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  59%|█████▉    | 5495/9281 [1:18:16<53:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  59%|█████▉    | 5496/9281 [1:18:17<53:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  59%|█████▉    | 5497/9281 [1:18:18<53:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.30e-01:  59%|█████▉    | 5498/9281 [1:18:19<53:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  59%|█████▉    | 5499/9281 [1:18:20<53:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  59%|█████▉    | 5500/9281 [1:18:20<53:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  59%|█████▉    | 5501/9281 [1:18:21<53:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  59%|█████▉    | 5502/9281 [1:18:22<53:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  59%|█████▉    | 5503/9281 [1:18:23<53:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  59%|█████▉    | 5504/9281 [1:18:24<53:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  59%|█████▉    | 5505/9281 [1:18:25<53:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  59%|█████▉    | 5506/9281 [1:18:26<54:01,  1.16it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  59%|█████▉    | 5507/9281 [1:18:26<54:04,  1.16it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  59%|█████▉    | 5508/9281 [1:18:27<54:03,  1.16it/s]\u001b[A\n",
      "Training loss: 5.73e-01:  59%|█████▉    | 5509/9281 [1:18:28<53:58,  1.16it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  59%|█████▉    | 5510/9281 [1:18:29<53:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.67e-01:  59%|█████▉    | 5511/9281 [1:18:30<53:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  59%|█████▉    | 5512/9281 [1:18:31<53:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  59%|█████▉    | 5513/9281 [1:18:32<53:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  59%|█████▉    | 5514/9281 [1:18:32<53:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  59%|█████▉    | 5515/9281 [1:18:33<53:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  59%|█████▉    | 5516/9281 [1:18:34<53:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  59%|█████▉    | 5517/9281 [1:18:35<53:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  59%|█████▉    | 5518/9281 [1:18:36<53:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  59%|█████▉    | 5519/9281 [1:18:37<53:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  59%|█████▉    | 5520/9281 [1:18:38<53:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  59%|█████▉    | 5521/9281 [1:18:38<53:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  59%|█████▉    | 5522/9281 [1:18:39<53:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  60%|█████▉    | 5523/9281 [1:18:40<53:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  60%|█████▉    | 5524/9281 [1:18:41<53:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  60%|█████▉    | 5525/9281 [1:18:42<53:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  60%|█████▉    | 5526/9281 [1:18:43<53:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  60%|█████▉    | 5527/9281 [1:18:44<53:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  60%|█████▉    | 5528/9281 [1:18:44<53:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  60%|█████▉    | 5529/9281 [1:18:45<53:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  60%|█████▉    | 5530/9281 [1:18:46<53:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  60%|█████▉    | 5531/9281 [1:18:47<53:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  60%|█████▉    | 5532/9281 [1:18:48<53:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  60%|█████▉    | 5533/9281 [1:18:49<53:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  60%|█████▉    | 5534/9281 [1:18:50<53:38,  1.16it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  60%|█████▉    | 5535/9281 [1:18:50<53:35,  1.16it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  60%|█████▉    | 5536/9281 [1:18:51<53:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  60%|█████▉    | 5537/9281 [1:18:52<53:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  60%|█████▉    | 5538/9281 [1:18:53<53:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  60%|█████▉    | 5539/9281 [1:18:54<53:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  60%|█████▉    | 5540/9281 [1:18:55<53:35,  1.16it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  60%|█████▉    | 5541/9281 [1:18:56<53:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  60%|█████▉    | 5542/9281 [1:18:56<53:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  60%|█████▉    | 5543/9281 [1:18:57<53:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  60%|█████▉    | 5544/9281 [1:18:58<53:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  60%|█████▉    | 5545/9281 [1:18:59<53:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  60%|█████▉    | 5546/9281 [1:19:00<53:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  60%|█████▉    | 5547/9281 [1:19:01<53:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  60%|█████▉    | 5548/9281 [1:19:02<53:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  60%|█████▉    | 5549/9281 [1:19:02<53:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  60%|█████▉    | 5550/9281 [1:19:03<53:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  60%|█████▉    | 5551/9281 [1:19:04<53:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  60%|█████▉    | 5552/9281 [1:19:05<53:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  60%|█████▉    | 5553/9281 [1:19:06<53:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  60%|█████▉    | 5554/9281 [1:19:07<53:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  60%|█████▉    | 5555/9281 [1:19:07<52:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  60%|█████▉    | 5556/9281 [1:19:08<52:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  60%|█████▉    | 5557/9281 [1:19:09<52:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  60%|█████▉    | 5558/9281 [1:19:10<53:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  60%|█████▉    | 5559/9281 [1:19:11<52:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  60%|█████▉    | 5560/9281 [1:19:12<52:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  60%|█████▉    | 5561/9281 [1:19:13<52:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  60%|█████▉    | 5562/9281 [1:19:13<53:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  60%|█████▉    | 5563/9281 [1:19:14<53:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  60%|█████▉    | 5564/9281 [1:19:15<52:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  60%|█████▉    | 5565/9281 [1:19:16<52:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  60%|█████▉    | 5566/9281 [1:19:17<53:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  60%|█████▉    | 5567/9281 [1:19:18<52:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  60%|█████▉    | 5568/9281 [1:19:19<53:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  60%|██████    | 5569/9281 [1:19:19<52:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  60%|██████    | 5570/9281 [1:19:20<52:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.83e-01:  60%|██████    | 5571/9281 [1:19:21<52:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  60%|██████    | 5572/9281 [1:19:22<52:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  60%|██████    | 5573/9281 [1:19:23<52:34,  1.18it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  60%|██████    | 5574/9281 [1:19:24<52:32,  1.18it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  60%|██████    | 5575/9281 [1:19:25<52:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  60%|██████    | 5576/9281 [1:19:25<52:30,  1.18it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  60%|██████    | 5577/9281 [1:19:26<52:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  60%|██████    | 5578/9281 [1:19:27<52:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  60%|██████    | 5579/9281 [1:19:28<52:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  60%|██████    | 5580/9281 [1:19:29<52:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  60%|██████    | 5581/9281 [1:19:30<52:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  60%|██████    | 5582/9281 [1:19:31<52:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  60%|██████    | 5583/9281 [1:19:31<52:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  60%|██████    | 5584/9281 [1:19:32<52:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  60%|██████    | 5585/9281 [1:19:33<52:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  60%|██████    | 5586/9281 [1:19:34<52:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  60%|██████    | 5587/9281 [1:19:35<52:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  60%|██████    | 5588/9281 [1:19:36<52:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  60%|██████    | 5589/9281 [1:19:37<52:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  60%|██████    | 5590/9281 [1:19:37<52:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  60%|██████    | 5591/9281 [1:19:38<52:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  60%|██████    | 5592/9281 [1:19:39<52:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  60%|██████    | 5593/9281 [1:19:40<52:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  60%|██████    | 5594/9281 [1:19:41<52:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  60%|██████    | 5595/9281 [1:19:42<52:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  60%|██████    | 5596/9281 [1:19:42<52:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  60%|██████    | 5597/9281 [1:19:43<52:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  60%|██████    | 5598/9281 [1:19:44<52:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  60%|██████    | 5599/9281 [1:19:45<52:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  60%|██████    | 5600/9281 [1:19:46<52:36,  1.17it/s]\u001b[A\n",
      "Training loss: 2.96e-01:  60%|██████    | 5601/9281 [1:19:47<52:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  60%|██████    | 5602/9281 [1:19:48<52:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  60%|██████    | 5603/9281 [1:19:48<52:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.68e-01:  60%|██████    | 5604/9281 [1:19:49<52:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  60%|██████    | 5605/9281 [1:19:50<52:04,  1.18it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  60%|██████    | 5606/9281 [1:19:51<52:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  60%|██████    | 5607/9281 [1:19:52<52:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  60%|██████    | 5608/9281 [1:19:53<52:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  60%|██████    | 5609/9281 [1:19:54<52:03,  1.18it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  60%|██████    | 5610/9281 [1:19:54<52:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  60%|██████    | 5611/9281 [1:19:55<52:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  60%|██████    | 5612/9281 [1:19:56<52:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  60%|██████    | 5613/9281 [1:19:57<52:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  60%|██████    | 5614/9281 [1:19:58<52:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  60%|██████    | 5615/9281 [1:19:59<52:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  61%|██████    | 5616/9281 [1:20:00<52:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  61%|██████    | 5617/9281 [1:20:00<51:56,  1.18it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  61%|██████    | 5618/9281 [1:20:01<51:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  61%|██████    | 5619/9281 [1:20:02<51:55,  1.18it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  61%|██████    | 5620/9281 [1:20:03<52:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  61%|██████    | 5621/9281 [1:20:04<51:53,  1.18it/s]\u001b[A\n",
      "Training loss: 3.30e-01:  61%|██████    | 5622/9281 [1:20:05<52:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  61%|██████    | 5623/9281 [1:20:06<52:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  61%|██████    | 5624/9281 [1:20:06<52:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  61%|██████    | 5625/9281 [1:20:07<51:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  61%|██████    | 5626/9281 [1:20:08<52:18,  1.16it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  61%|██████    | 5627/9281 [1:20:09<52:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  61%|██████    | 5628/9281 [1:20:10<52:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  61%|██████    | 5629/9281 [1:20:11<52:19,  1.16it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  61%|██████    | 5630/9281 [1:20:12<52:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  61%|██████    | 5631/9281 [1:20:12<52:16,  1.16it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  61%|██████    | 5632/9281 [1:20:13<52:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  61%|██████    | 5633/9281 [1:20:14<52:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  61%|██████    | 5634/9281 [1:20:15<51:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  61%|██████    | 5635/9281 [1:20:16<52:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  61%|██████    | 5636/9281 [1:20:17<51:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  61%|██████    | 5637/9281 [1:20:18<51:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  61%|██████    | 5638/9281 [1:20:18<52:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  61%|██████    | 5639/9281 [1:20:19<51:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.86e-01:  61%|██████    | 5640/9281 [1:20:20<51:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.88e-01:  61%|██████    | 5641/9281 [1:20:21<52:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.84e-01:  61%|██████    | 5642/9281 [1:20:22<51:57,  1.17it/s]\u001b[A\n",
      "Training loss: 5.12e-01:  61%|██████    | 5643/9281 [1:20:23<52:06,  1.16it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  61%|██████    | 5644/9281 [1:20:24<52:04,  1.16it/s]\u001b[A\n",
      "Training loss: 4.95e-01:  61%|██████    | 5645/9281 [1:20:24<51:54,  1.17it/s]\u001b[A\n",
      "Training loss: 5.05e-01:  61%|██████    | 5646/9281 [1:20:25<51:53,  1.17it/s]\u001b[A\n",
      "Training loss: 5.10e-01:  61%|██████    | 5647/9281 [1:20:26<51:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  61%|██████    | 5648/9281 [1:20:27<51:59,  1.16it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  61%|██████    | 5649/9281 [1:20:28<51:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  61%|██████    | 5650/9281 [1:20:29<51:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  61%|██████    | 5651/9281 [1:20:30<51:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  61%|██████    | 5652/9281 [1:20:30<51:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  61%|██████    | 5653/9281 [1:20:31<51:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  61%|██████    | 5654/9281 [1:20:32<51:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  61%|██████    | 5655/9281 [1:20:33<51:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  61%|██████    | 5656/9281 [1:20:34<51:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  61%|██████    | 5657/9281 [1:20:35<51:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  61%|██████    | 5658/9281 [1:20:35<51:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  61%|██████    | 5659/9281 [1:20:36<51:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  61%|██████    | 5660/9281 [1:20:37<51:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  61%|██████    | 5661/9281 [1:20:38<51:34,  1.17it/s]\u001b[A\n",
      "Training loss: 5.05e-01:  61%|██████    | 5662/9281 [1:20:39<51:38,  1.17it/s]\u001b[A\n",
      "Training loss: 5.36e-01:  61%|██████    | 5663/9281 [1:20:40<51:24,  1.17it/s]\u001b[A\n",
      "Training loss: 5.29e-01:  61%|██████    | 5664/9281 [1:20:41<51:44,  1.17it/s]\u001b[A\n",
      "Training loss: 5.14e-01:  61%|██████    | 5665/9281 [1:20:41<51:33,  1.17it/s]\u001b[A\n",
      "Training loss: 5.21e-01:  61%|██████    | 5666/9281 [1:20:42<51:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  61%|██████    | 5667/9281 [1:20:43<51:28,  1.17it/s]\u001b[A\n",
      "Training loss: 5.24e-01:  61%|██████    | 5668/9281 [1:20:44<51:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.84e-01:  61%|██████    | 5669/9281 [1:20:45<51:29,  1.17it/s]\u001b[A\n",
      "Training loss: 5.21e-01:  61%|██████    | 5670/9281 [1:20:46<51:24,  1.17it/s]\u001b[A\n",
      "Training loss: 5.00e-01:  61%|██████    | 5671/9281 [1:20:47<51:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  61%|██████    | 5672/9281 [1:20:47<51:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  61%|██████    | 5673/9281 [1:20:48<51:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  61%|██████    | 5674/9281 [1:20:49<51:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  61%|██████    | 5675/9281 [1:20:50<51:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  61%|██████    | 5676/9281 [1:20:51<51:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  61%|██████    | 5677/9281 [1:20:52<51:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  61%|██████    | 5678/9281 [1:20:53<51:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  61%|██████    | 5679/9281 [1:20:53<51:37,  1.16it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  61%|██████    | 5680/9281 [1:20:54<51:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  61%|██████    | 5681/9281 [1:20:55<51:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  61%|██████    | 5682/9281 [1:20:56<51:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  61%|██████    | 5683/9281 [1:20:57<51:29,  1.16it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  61%|██████    | 5684/9281 [1:20:58<51:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  61%|██████▏   | 5685/9281 [1:20:59<51:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  61%|██████▏   | 5686/9281 [1:20:59<51:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  61%|██████▏   | 5687/9281 [1:21:00<51:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  61%|██████▏   | 5688/9281 [1:21:01<51:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  61%|██████▏   | 5689/9281 [1:21:02<51:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  61%|██████▏   | 5690/9281 [1:21:03<51:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  61%|██████▏   | 5691/9281 [1:21:04<51:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  61%|██████▏   | 5692/9281 [1:21:05<51:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  61%|██████▏   | 5693/9281 [1:21:05<51:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  61%|██████▏   | 5694/9281 [1:21:06<51:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  61%|██████▏   | 5695/9281 [1:21:07<51:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  61%|██████▏   | 5696/9281 [1:21:08<51:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  61%|██████▏   | 5697/9281 [1:21:09<51:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  61%|██████▏   | 5698/9281 [1:21:10<50:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  61%|██████▏   | 5699/9281 [1:21:11<50:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  61%|██████▏   | 5700/9281 [1:21:11<50:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  61%|██████▏   | 5701/9281 [1:21:12<50:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  61%|██████▏   | 5702/9281 [1:21:13<50:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  61%|██████▏   | 5703/9281 [1:21:14<51:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  61%|██████▏   | 5704/9281 [1:21:15<51:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  61%|██████▏   | 5705/9281 [1:21:16<50:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  61%|██████▏   | 5706/9281 [1:21:17<50:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  61%|██████▏   | 5707/9281 [1:21:17<50:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.23e-01:  62%|██████▏   | 5708/9281 [1:21:18<50:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  62%|██████▏   | 5709/9281 [1:21:19<51:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  62%|██████▏   | 5710/9281 [1:21:20<51:05,  1.16it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  62%|██████▏   | 5711/9281 [1:21:21<50:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  62%|██████▏   | 5712/9281 [1:21:22<50:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  62%|██████▏   | 5713/9281 [1:21:23<50:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  62%|██████▏   | 5714/9281 [1:21:23<50:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  62%|██████▏   | 5715/9281 [1:21:24<50:31,  1.18it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  62%|██████▏   | 5716/9281 [1:21:25<50:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  62%|██████▏   | 5717/9281 [1:21:26<50:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  62%|██████▏   | 5718/9281 [1:21:27<50:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  62%|██████▏   | 5719/9281 [1:21:28<50:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  62%|██████▏   | 5720/9281 [1:21:29<50:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  62%|██████▏   | 5721/9281 [1:21:29<50:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  62%|██████▏   | 5722/9281 [1:21:30<50:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  62%|██████▏   | 5723/9281 [1:21:31<50:25,  1.18it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  62%|██████▏   | 5724/9281 [1:21:32<50:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  62%|██████▏   | 5725/9281 [1:21:33<50:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  62%|██████▏   | 5726/9281 [1:21:34<50:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  62%|██████▏   | 5727/9281 [1:21:34<50:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  62%|██████▏   | 5728/9281 [1:21:35<50:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  62%|██████▏   | 5729/9281 [1:21:36<50:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  62%|██████▏   | 5730/9281 [1:21:37<50:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  62%|██████▏   | 5731/9281 [1:21:38<50:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  62%|██████▏   | 5732/9281 [1:21:39<50:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  62%|██████▏   | 5733/9281 [1:21:40<50:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  62%|██████▏   | 5734/9281 [1:21:40<50:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  62%|██████▏   | 5735/9281 [1:21:41<50:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  62%|██████▏   | 5736/9281 [1:21:42<50:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  62%|██████▏   | 5737/9281 [1:21:43<50:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  62%|██████▏   | 5738/9281 [1:21:44<50:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  62%|██████▏   | 5739/9281 [1:21:45<50:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  62%|██████▏   | 5740/9281 [1:21:46<50:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  62%|██████▏   | 5741/9281 [1:21:46<50:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  62%|██████▏   | 5742/9281 [1:21:47<50:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  62%|██████▏   | 5743/9281 [1:21:48<50:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.87e-01:  62%|██████▏   | 5744/9281 [1:21:49<50:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  62%|██████▏   | 5745/9281 [1:21:50<50:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  62%|██████▏   | 5746/9281 [1:21:51<50:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  62%|██████▏   | 5747/9281 [1:21:52<50:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  62%|██████▏   | 5748/9281 [1:21:52<50:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  62%|██████▏   | 5749/9281 [1:21:53<50:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  62%|██████▏   | 5750/9281 [1:21:54<50:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  62%|██████▏   | 5751/9281 [1:21:55<50:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  62%|██████▏   | 5752/9281 [1:21:56<50:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  62%|██████▏   | 5753/9281 [1:21:57<50:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  62%|██████▏   | 5754/9281 [1:21:58<50:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  62%|██████▏   | 5755/9281 [1:21:58<50:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  62%|██████▏   | 5756/9281 [1:21:59<50:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  62%|██████▏   | 5757/9281 [1:22:00<50:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  62%|██████▏   | 5758/9281 [1:22:01<50:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  62%|██████▏   | 5759/9281 [1:22:02<50:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  62%|██████▏   | 5760/9281 [1:22:03<50:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  62%|██████▏   | 5761/9281 [1:22:04<50:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  62%|██████▏   | 5762/9281 [1:22:04<50:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  62%|██████▏   | 5763/9281 [1:22:05<50:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  62%|██████▏   | 5764/9281 [1:22:06<50:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  62%|██████▏   | 5765/9281 [1:22:07<49:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  62%|██████▏   | 5766/9281 [1:22:08<50:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  62%|██████▏   | 5767/9281 [1:22:09<50:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.18e-01:  62%|██████▏   | 5768/9281 [1:22:10<50:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.00e-01:  62%|██████▏   | 5769/9281 [1:22:10<50:15,  1.16it/s]\u001b[A\n",
      "Training loss: 3.14e-01:  62%|██████▏   | 5770/9281 [1:22:11<50:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  62%|██████▏   | 5771/9281 [1:22:12<50:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  62%|██████▏   | 5772/9281 [1:22:13<49:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  62%|██████▏   | 5773/9281 [1:22:14<50:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  62%|██████▏   | 5774/9281 [1:22:15<50:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  62%|██████▏   | 5775/9281 [1:22:16<49:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  62%|██████▏   | 5776/9281 [1:22:16<49:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  62%|██████▏   | 5777/9281 [1:22:17<49:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  62%|██████▏   | 5778/9281 [1:22:18<49:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  62%|██████▏   | 5779/9281 [1:22:19<49:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  62%|██████▏   | 5780/9281 [1:22:20<49:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  62%|██████▏   | 5781/9281 [1:22:21<49:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  62%|██████▏   | 5782/9281 [1:22:21<49:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  62%|██████▏   | 5783/9281 [1:22:22<49:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  62%|██████▏   | 5784/9281 [1:22:23<49:30,  1.18it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  62%|██████▏   | 5785/9281 [1:22:24<49:32,  1.18it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  62%|██████▏   | 5786/9281 [1:22:25<49:30,  1.18it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  62%|██████▏   | 5787/9281 [1:22:26<49:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  62%|██████▏   | 5788/9281 [1:22:27<49:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  62%|██████▏   | 5789/9281 [1:22:27<49:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  62%|██████▏   | 5790/9281 [1:22:28<49:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  62%|██████▏   | 5791/9281 [1:22:29<49:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  62%|██████▏   | 5792/9281 [1:22:30<49:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  62%|██████▏   | 5793/9281 [1:22:31<49:24,  1.18it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  62%|██████▏   | 5794/9281 [1:22:32<49:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  62%|██████▏   | 5795/9281 [1:22:33<49:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  62%|██████▏   | 5796/9281 [1:22:33<49:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  62%|██████▏   | 5797/9281 [1:22:34<49:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  62%|██████▏   | 5798/9281 [1:22:35<49:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  62%|██████▏   | 5799/9281 [1:22:36<49:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  62%|██████▏   | 5800/9281 [1:22:37<49:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  63%|██████▎   | 5801/9281 [1:22:38<49:19,  1.18it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  63%|██████▎   | 5802/9281 [1:22:39<49:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  63%|██████▎   | 5803/9281 [1:22:39<49:15,  1.18it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  63%|██████▎   | 5804/9281 [1:22:40<49:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  63%|██████▎   | 5805/9281 [1:22:41<49:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  63%|██████▎   | 5806/9281 [1:22:42<49:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  63%|██████▎   | 5807/9281 [1:22:43<49:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  63%|██████▎   | 5808/9281 [1:22:44<49:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  63%|██████▎   | 5809/9281 [1:22:44<49:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  63%|██████▎   | 5810/9281 [1:22:45<49:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  63%|██████▎   | 5811/9281 [1:22:46<49:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  63%|██████▎   | 5812/9281 [1:22:47<49:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  63%|██████▎   | 5813/9281 [1:22:48<49:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  63%|██████▎   | 5814/9281 [1:22:49<49:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  63%|██████▎   | 5815/9281 [1:22:50<49:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  63%|██████▎   | 5816/9281 [1:22:50<49:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.87e-01:  63%|██████▎   | 5817/9281 [1:22:51<49:35,  1.16it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  63%|██████▎   | 5818/9281 [1:22:52<49:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  63%|██████▎   | 5819/9281 [1:22:53<49:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  63%|██████▎   | 5820/9281 [1:22:54<49:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  63%|██████▎   | 5821/9281 [1:22:55<49:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  63%|██████▎   | 5822/9281 [1:22:56<49:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  63%|██████▎   | 5823/9281 [1:22:56<49:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  63%|██████▎   | 5824/9281 [1:22:57<49:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  63%|██████▎   | 5825/9281 [1:22:58<48:59,  1.18it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  63%|██████▎   | 5826/9281 [1:22:59<49:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  63%|██████▎   | 5827/9281 [1:23:00<48:58,  1.18it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  63%|██████▎   | 5828/9281 [1:23:01<49:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  63%|██████▎   | 5829/9281 [1:23:02<49:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.11e-01:  63%|██████▎   | 5830/9281 [1:23:02<49:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  63%|██████▎   | 5831/9281 [1:23:03<49:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  63%|██████▎   | 5832/9281 [1:23:04<49:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  63%|██████▎   | 5833/9281 [1:23:05<48:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  63%|██████▎   | 5834/9281 [1:23:06<48:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  63%|██████▎   | 5835/9281 [1:23:07<48:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  63%|██████▎   | 5836/9281 [1:23:08<48:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  63%|██████▎   | 5837/9281 [1:23:08<48:47,  1.18it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  63%|██████▎   | 5838/9281 [1:23:09<48:48,  1.18it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  63%|██████▎   | 5839/9281 [1:23:10<48:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  63%|██████▎   | 5840/9281 [1:23:11<48:48,  1.18it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  63%|██████▎   | 5841/9281 [1:23:12<48:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  63%|██████▎   | 5842/9281 [1:23:13<49:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  63%|██████▎   | 5843/9281 [1:23:14<49:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  63%|██████▎   | 5844/9281 [1:23:14<49:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  63%|██████▎   | 5845/9281 [1:23:15<49:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  63%|██████▎   | 5846/9281 [1:23:16<49:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  63%|██████▎   | 5847/9281 [1:23:17<48:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  63%|██████▎   | 5848/9281 [1:23:18<48:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  63%|██████▎   | 5849/9281 [1:23:19<48:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  63%|██████▎   | 5850/9281 [1:23:19<48:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  63%|██████▎   | 5851/9281 [1:23:20<48:35,  1.18it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  63%|██████▎   | 5852/9281 [1:23:21<48:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  63%|██████▎   | 5853/9281 [1:23:22<48:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.21e-01:  63%|██████▎   | 5854/9281 [1:23:23<48:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  63%|██████▎   | 5855/9281 [1:23:24<48:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  63%|██████▎   | 5856/9281 [1:23:25<48:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  63%|██████▎   | 5857/9281 [1:23:25<48:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  63%|██████▎   | 5858/9281 [1:23:26<48:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  63%|██████▎   | 5859/9281 [1:23:27<48:25,  1.18it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  63%|██████▎   | 5860/9281 [1:23:28<48:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  63%|██████▎   | 5861/9281 [1:23:29<48:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  63%|██████▎   | 5862/9281 [1:23:30<48:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  63%|██████▎   | 5863/9281 [1:23:31<48:26,  1.18it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  63%|██████▎   | 5864/9281 [1:23:31<48:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  63%|██████▎   | 5865/9281 [1:23:32<48:25,  1.18it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  63%|██████▎   | 5866/9281 [1:23:33<48:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.67e-01:  63%|██████▎   | 5867/9281 [1:23:34<48:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  63%|██████▎   | 5868/9281 [1:23:35<48:34,  1.17it/s]\u001b[A\n",
      "Training loss: 5.30e-01:  63%|██████▎   | 5869/9281 [1:23:36<48:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  63%|██████▎   | 5870/9281 [1:23:37<48:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.99e-01:  63%|██████▎   | 5871/9281 [1:23:37<48:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  63%|██████▎   | 5872/9281 [1:23:38<48:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  63%|██████▎   | 5873/9281 [1:23:39<48:19,  1.18it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  63%|██████▎   | 5874/9281 [1:23:40<48:10,  1.18it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  63%|██████▎   | 5875/9281 [1:23:41<48:13,  1.18it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  63%|██████▎   | 5876/9281 [1:23:42<48:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  63%|██████▎   | 5877/9281 [1:23:43<48:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  63%|██████▎   | 5878/9281 [1:23:43<48:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  63%|██████▎   | 5879/9281 [1:23:44<48:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  63%|██████▎   | 5880/9281 [1:23:45<48:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  63%|██████▎   | 5881/9281 [1:23:46<48:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  63%|██████▎   | 5882/9281 [1:23:47<48:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  63%|██████▎   | 5883/9281 [1:23:48<48:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  63%|██████▎   | 5884/9281 [1:23:48<48:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.13e-01:  63%|██████▎   | 5885/9281 [1:23:49<48:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.13e-01:  63%|██████▎   | 5886/9281 [1:23:50<48:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  63%|██████▎   | 5887/9281 [1:23:51<48:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  63%|██████▎   | 5888/9281 [1:23:52<48:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  63%|██████▎   | 5889/9281 [1:23:53<48:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  63%|██████▎   | 5890/9281 [1:23:54<48:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  63%|██████▎   | 5891/9281 [1:23:54<48:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.99e-01:  63%|██████▎   | 5892/9281 [1:23:55<48:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  63%|██████▎   | 5893/9281 [1:23:56<48:04,  1.17it/s]\u001b[A\n",
      "Training loss: 5.18e-01:  64%|██████▎   | 5894/9281 [1:23:57<48:02,  1.17it/s]\u001b[A\n",
      "Training loss: 5.16e-01:  64%|██████▎   | 5895/9281 [1:23:58<48:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  64%|██████▎   | 5896/9281 [1:23:59<48:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.67e-01:  64%|██████▎   | 5897/9281 [1:24:00<48:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  64%|██████▎   | 5898/9281 [1:24:00<48:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  64%|██████▎   | 5899/9281 [1:24:01<48:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  64%|██████▎   | 5900/9281 [1:24:02<48:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  64%|██████▎   | 5901/9281 [1:24:03<48:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  64%|██████▎   | 5902/9281 [1:24:04<48:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  64%|██████▎   | 5903/9281 [1:24:05<48:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  64%|██████▎   | 5904/9281 [1:24:06<48:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  64%|██████▎   | 5905/9281 [1:24:06<48:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  64%|██████▎   | 5906/9281 [1:24:07<48:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  64%|██████▎   | 5907/9281 [1:24:08<48:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  64%|██████▎   | 5908/9281 [1:24:09<48:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  64%|██████▎   | 5909/9281 [1:24:10<48:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  64%|██████▎   | 5910/9281 [1:24:11<48:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  64%|██████▎   | 5911/9281 [1:24:12<48:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  64%|██████▎   | 5912/9281 [1:24:12<47:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  64%|██████▎   | 5913/9281 [1:24:13<47:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  64%|██████▎   | 5914/9281 [1:24:14<47:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  64%|██████▎   | 5915/9281 [1:24:15<47:34,  1.18it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  64%|██████▎   | 5916/9281 [1:24:16<47:41,  1.18it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  64%|██████▍   | 5917/9281 [1:24:17<47:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  64%|██████▍   | 5918/9281 [1:24:18<47:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  64%|██████▍   | 5919/9281 [1:24:18<47:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  64%|██████▍   | 5920/9281 [1:24:19<47:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  64%|██████▍   | 5921/9281 [1:24:20<47:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  64%|██████▍   | 5922/9281 [1:24:21<47:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  64%|██████▍   | 5923/9281 [1:24:22<47:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  64%|██████▍   | 5924/9281 [1:24:23<47:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  64%|██████▍   | 5925/9281 [1:24:24<47:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  64%|██████▍   | 5926/9281 [1:24:24<47:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  64%|██████▍   | 5927/9281 [1:24:25<47:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  64%|██████▍   | 5928/9281 [1:24:26<47:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  64%|██████▍   | 5929/9281 [1:24:27<47:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  64%|██████▍   | 5930/9281 [1:24:28<47:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  64%|██████▍   | 5931/9281 [1:24:29<47:56,  1.16it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  64%|██████▍   | 5932/9281 [1:24:29<47:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  64%|██████▍   | 5933/9281 [1:24:30<47:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  64%|██████▍   | 5934/9281 [1:24:31<47:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  64%|██████▍   | 5935/9281 [1:24:32<47:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  64%|██████▍   | 5936/9281 [1:24:33<47:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  64%|██████▍   | 5937/9281 [1:24:34<47:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  64%|██████▍   | 5938/9281 [1:24:35<47:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  64%|██████▍   | 5939/9281 [1:24:35<47:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  64%|██████▍   | 5940/9281 [1:24:36<47:20,  1.18it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  64%|██████▍   | 5941/9281 [1:24:37<47:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  64%|██████▍   | 5942/9281 [1:24:38<47:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  64%|██████▍   | 5943/9281 [1:24:39<47:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  64%|██████▍   | 5944/9281 [1:24:40<47:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  64%|██████▍   | 5945/9281 [1:24:41<47:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  64%|██████▍   | 5946/9281 [1:24:41<47:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  64%|██████▍   | 5947/9281 [1:24:42<47:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  64%|██████▍   | 5948/9281 [1:24:43<47:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  64%|██████▍   | 5949/9281 [1:24:44<47:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  64%|██████▍   | 5950/9281 [1:24:45<47:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  64%|██████▍   | 5951/9281 [1:24:46<47:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  64%|██████▍   | 5952/9281 [1:24:47<47:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  64%|██████▍   | 5953/9281 [1:24:47<47:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  64%|██████▍   | 5954/9281 [1:24:48<47:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  64%|██████▍   | 5955/9281 [1:24:49<47:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  64%|██████▍   | 5956/9281 [1:24:50<47:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  64%|██████▍   | 5957/9281 [1:24:51<47:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  64%|██████▍   | 5958/9281 [1:24:52<47:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  64%|██████▍   | 5959/9281 [1:24:53<47:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  64%|██████▍   | 5960/9281 [1:24:53<47:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  64%|██████▍   | 5961/9281 [1:24:54<47:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  64%|██████▍   | 5962/9281 [1:24:55<47:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  64%|██████▍   | 5963/9281 [1:24:56<47:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  64%|██████▍   | 5964/9281 [1:24:57<47:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  64%|██████▍   | 5965/9281 [1:24:58<47:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  64%|██████▍   | 5966/9281 [1:24:59<47:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  64%|██████▍   | 5967/9281 [1:24:59<47:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  64%|██████▍   | 5968/9281 [1:25:00<47:27,  1.16it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  64%|██████▍   | 5969/9281 [1:25:01<47:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  64%|██████▍   | 5970/9281 [1:25:02<47:29,  1.16it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  64%|██████▍   | 5971/9281 [1:25:03<47:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  64%|██████▍   | 5972/9281 [1:25:04<47:22,  1.16it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  64%|██████▍   | 5973/9281 [1:25:05<47:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  64%|██████▍   | 5974/9281 [1:25:05<47:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  64%|██████▍   | 5975/9281 [1:25:06<47:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  64%|██████▍   | 5976/9281 [1:25:07<47:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  64%|██████▍   | 5977/9281 [1:25:08<46:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  64%|██████▍   | 5978/9281 [1:25:09<46:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.97e-01:  64%|██████▍   | 5979/9281 [1:25:10<46:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  64%|██████▍   | 5980/9281 [1:25:11<47:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  64%|██████▍   | 5981/9281 [1:25:11<47:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  64%|██████▍   | 5982/9281 [1:25:12<47:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  64%|██████▍   | 5983/9281 [1:25:13<46:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  64%|██████▍   | 5984/9281 [1:25:14<46:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  64%|██████▍   | 5985/9281 [1:25:15<46:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  64%|██████▍   | 5986/9281 [1:25:16<46:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  65%|██████▍   | 5987/9281 [1:25:17<47:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  65%|██████▍   | 5988/9281 [1:25:17<46:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  65%|██████▍   | 5989/9281 [1:25:18<46:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  65%|██████▍   | 5990/9281 [1:25:19<47:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.34e-01:  65%|██████▍   | 5991/9281 [1:25:20<46:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  65%|██████▍   | 5992/9281 [1:25:21<46:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  65%|██████▍   | 5993/9281 [1:25:22<46:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  65%|██████▍   | 5994/9281 [1:25:22<46:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  65%|██████▍   | 5995/9281 [1:25:23<46:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  65%|██████▍   | 5996/9281 [1:25:24<46:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  65%|██████▍   | 5997/9281 [1:25:25<46:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  65%|██████▍   | 5998/9281 [1:25:26<46:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  65%|██████▍   | 5999/9281 [1:25:27<46:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  65%|██████▍   | 6000/9281 [1:25:28<46:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  65%|██████▍   | 6001/9281 [1:25:28<46:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  65%|██████▍   | 6002/9281 [1:25:29<46:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  65%|██████▍   | 6003/9281 [1:25:30<46:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  65%|██████▍   | 6004/9281 [1:25:31<46:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  65%|██████▍   | 6005/9281 [1:25:32<46:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  65%|██████▍   | 6006/9281 [1:25:33<46:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  65%|██████▍   | 6007/9281 [1:25:34<46:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  65%|██████▍   | 6008/9281 [1:25:34<46:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  65%|██████▍   | 6009/9281 [1:25:35<46:49,  1.16it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  65%|██████▍   | 6010/9281 [1:25:36<46:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  65%|██████▍   | 6011/9281 [1:25:37<46:47,  1.16it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  65%|██████▍   | 6012/9281 [1:25:38<46:46,  1.16it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  65%|██████▍   | 6013/9281 [1:25:39<46:48,  1.16it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  65%|██████▍   | 6014/9281 [1:25:40<46:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  65%|██████▍   | 6015/9281 [1:25:40<46:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  65%|██████▍   | 6016/9281 [1:25:41<46:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  65%|██████▍   | 6017/9281 [1:25:42<46:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  65%|██████▍   | 6018/9281 [1:25:43<46:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  65%|██████▍   | 6019/9281 [1:25:44<46:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  65%|██████▍   | 6020/9281 [1:25:45<46:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  65%|██████▍   | 6021/9281 [1:25:46<46:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  65%|██████▍   | 6022/9281 [1:25:46<46:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  65%|██████▍   | 6023/9281 [1:25:47<46:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  65%|██████▍   | 6024/9281 [1:25:48<46:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  65%|██████▍   | 6025/9281 [1:25:49<46:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  65%|██████▍   | 6026/9281 [1:25:50<46:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.84e-01:  65%|██████▍   | 6027/9281 [1:25:51<46:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  65%|██████▍   | 6028/9281 [1:25:52<46:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  65%|██████▍   | 6029/9281 [1:25:52<46:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  65%|██████▍   | 6030/9281 [1:25:53<46:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  65%|██████▍   | 6031/9281 [1:25:54<46:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  65%|██████▍   | 6032/9281 [1:25:55<46:29,  1.16it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  65%|██████▌   | 6033/9281 [1:25:56<46:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  65%|██████▌   | 6034/9281 [1:25:57<46:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.39e-01:  65%|██████▌   | 6035/9281 [1:25:58<46:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  65%|██████▌   | 6036/9281 [1:25:58<46:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  65%|██████▌   | 6037/9281 [1:25:59<46:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  65%|██████▌   | 6038/9281 [1:26:00<46:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  65%|██████▌   | 6039/9281 [1:26:01<46:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  65%|██████▌   | 6040/9281 [1:26:02<46:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  65%|██████▌   | 6041/9281 [1:26:03<46:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  65%|██████▌   | 6042/9281 [1:26:04<46:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  65%|██████▌   | 6043/9281 [1:26:04<46:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  65%|██████▌   | 6044/9281 [1:26:05<46:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  65%|██████▌   | 6045/9281 [1:26:06<46:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  65%|██████▌   | 6046/9281 [1:26:07<46:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  65%|██████▌   | 6047/9281 [1:26:08<46:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  65%|██████▌   | 6048/9281 [1:26:09<45:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  65%|██████▌   | 6049/9281 [1:26:09<45:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  65%|██████▌   | 6050/9281 [1:26:10<46:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  65%|██████▌   | 6051/9281 [1:26:11<45:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  65%|██████▌   | 6052/9281 [1:26:12<46:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  65%|██████▌   | 6053/9281 [1:26:13<46:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.84e-01:  65%|██████▌   | 6054/9281 [1:26:14<46:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  65%|██████▌   | 6055/9281 [1:26:15<45:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  65%|██████▌   | 6056/9281 [1:26:15<45:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  65%|██████▌   | 6057/9281 [1:26:16<45:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  65%|██████▌   | 6058/9281 [1:26:17<45:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  65%|██████▌   | 6059/9281 [1:26:18<45:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  65%|██████▌   | 6060/9281 [1:26:19<45:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  65%|██████▌   | 6061/9281 [1:26:20<45:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  65%|██████▌   | 6062/9281 [1:26:21<45:50,  1.17it/s]\u001b[A\n",
      "Training loss: 5.43e-01:  65%|██████▌   | 6063/9281 [1:26:21<45:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  65%|██████▌   | 6064/9281 [1:26:22<45:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  65%|██████▌   | 6065/9281 [1:26:23<45:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  65%|██████▌   | 6066/9281 [1:26:24<45:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  65%|██████▌   | 6067/9281 [1:26:25<45:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  65%|██████▌   | 6068/9281 [1:26:26<45:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  65%|██████▌   | 6069/9281 [1:26:27<45:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  65%|██████▌   | 6070/9281 [1:26:27<45:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  65%|██████▌   | 6071/9281 [1:26:28<45:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  65%|██████▌   | 6072/9281 [1:26:29<45:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.80e-01:  65%|██████▌   | 6073/9281 [1:26:30<45:46,  1.17it/s]\u001b[A\n",
      "Training loss: 5.47e-01:  65%|██████▌   | 6074/9281 [1:26:31<45:58,  1.16it/s]\u001b[A\n",
      "Training loss: 5.11e-01:  65%|██████▌   | 6075/9281 [1:26:32<46:03,  1.16it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  65%|██████▌   | 6076/9281 [1:26:33<45:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  65%|██████▌   | 6077/9281 [1:26:33<45:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  65%|██████▌   | 6078/9281 [1:26:34<45:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  65%|██████▌   | 6079/9281 [1:26:35<45:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  66%|██████▌   | 6080/9281 [1:26:36<45:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  66%|██████▌   | 6081/9281 [1:26:37<45:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  66%|██████▌   | 6082/9281 [1:26:38<45:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  66%|██████▌   | 6083/9281 [1:26:39<45:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.11e-01:  66%|██████▌   | 6084/9281 [1:26:39<45:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.14e-01:  66%|██████▌   | 6085/9281 [1:26:40<45:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.29e-01:  66%|██████▌   | 6086/9281 [1:26:41<45:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  66%|██████▌   | 6087/9281 [1:26:42<45:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  66%|██████▌   | 6088/9281 [1:26:43<45:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  66%|██████▌   | 6089/9281 [1:26:44<45:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  66%|██████▌   | 6090/9281 [1:26:45<45:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  66%|██████▌   | 6091/9281 [1:26:45<45:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  66%|██████▌   | 6092/9281 [1:26:46<45:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  66%|██████▌   | 6093/9281 [1:26:47<45:37,  1.16it/s]\u001b[A\n",
      "Training loss: 4.84e-01:  66%|██████▌   | 6094/9281 [1:26:48<45:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  66%|██████▌   | 6095/9281 [1:26:49<45:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  66%|██████▌   | 6096/9281 [1:26:50<45:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  66%|██████▌   | 6097/9281 [1:26:51<45:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  66%|██████▌   | 6098/9281 [1:26:51<45:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  66%|██████▌   | 6099/9281 [1:26:52<45:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  66%|██████▌   | 6100/9281 [1:26:53<45:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  66%|██████▌   | 6101/9281 [1:26:54<45:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  66%|██████▌   | 6102/9281 [1:26:55<45:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  66%|██████▌   | 6103/9281 [1:26:56<45:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  66%|██████▌   | 6104/9281 [1:26:57<45:27,  1.16it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  66%|██████▌   | 6105/9281 [1:26:57<45:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  66%|██████▌   | 6106/9281 [1:26:58<45:31,  1.16it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  66%|██████▌   | 6107/9281 [1:26:59<45:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  66%|██████▌   | 6108/9281 [1:27:00<45:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  66%|██████▌   | 6109/9281 [1:27:01<45:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  66%|██████▌   | 6110/9281 [1:27:02<45:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  66%|██████▌   | 6111/9281 [1:27:03<45:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  66%|██████▌   | 6112/9281 [1:27:03<45:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  66%|██████▌   | 6113/9281 [1:27:04<45:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  66%|██████▌   | 6114/9281 [1:27:05<45:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  66%|██████▌   | 6115/9281 [1:27:06<44:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  66%|██████▌   | 6116/9281 [1:27:07<44:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  66%|██████▌   | 6117/9281 [1:27:08<44:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  66%|██████▌   | 6118/9281 [1:27:08<44:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  66%|██████▌   | 6119/9281 [1:27:09<44:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  66%|██████▌   | 6120/9281 [1:27:10<44:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  66%|██████▌   | 6121/9281 [1:27:11<44:45,  1.18it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  66%|██████▌   | 6122/9281 [1:27:12<44:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.30e-01:  66%|██████▌   | 6123/9281 [1:27:13<44:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  66%|██████▌   | 6124/9281 [1:27:14<44:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  66%|██████▌   | 6125/9281 [1:27:14<44:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  66%|██████▌   | 6126/9281 [1:27:15<44:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  66%|██████▌   | 6127/9281 [1:27:16<44:41,  1.18it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  66%|██████▌   | 6128/9281 [1:27:17<44:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  66%|██████▌   | 6129/9281 [1:27:18<44:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  66%|██████▌   | 6130/9281 [1:27:19<44:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  66%|██████▌   | 6131/9281 [1:27:20<44:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  66%|██████▌   | 6132/9281 [1:27:20<44:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  66%|██████▌   | 6133/9281 [1:27:21<44:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  66%|██████▌   | 6134/9281 [1:27:22<44:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  66%|██████▌   | 6135/9281 [1:27:23<44:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  66%|██████▌   | 6136/9281 [1:27:24<44:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  66%|██████▌   | 6137/9281 [1:27:25<44:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  66%|██████▌   | 6138/9281 [1:27:26<44:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  66%|██████▌   | 6139/9281 [1:27:26<44:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  66%|██████▌   | 6140/9281 [1:27:27<44:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  66%|██████▌   | 6141/9281 [1:27:28<44:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  66%|██████▌   | 6142/9281 [1:27:29<44:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  66%|██████▌   | 6143/9281 [1:27:30<44:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  66%|██████▌   | 6144/9281 [1:27:31<44:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  66%|██████▌   | 6145/9281 [1:27:32<44:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  66%|██████▌   | 6146/9281 [1:27:32<44:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  66%|██████▌   | 6147/9281 [1:27:33<44:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  66%|██████▌   | 6148/9281 [1:27:34<44:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  66%|██████▋   | 6149/9281 [1:27:35<44:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  66%|██████▋   | 6150/9281 [1:27:36<44:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  66%|██████▋   | 6151/9281 [1:27:37<44:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  66%|██████▋   | 6152/9281 [1:27:38<44:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.67e-01:  66%|██████▋   | 6153/9281 [1:27:38<44:22,  1.17it/s]\u001b[A\n",
      "Training loss: 5.15e-01:  66%|██████▋   | 6154/9281 [1:27:39<44:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  66%|██████▋   | 6155/9281 [1:27:40<44:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  66%|██████▋   | 6156/9281 [1:27:41<44:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  66%|██████▋   | 6157/9281 [1:27:42<44:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  66%|██████▋   | 6158/9281 [1:27:43<44:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  66%|██████▋   | 6159/9281 [1:27:44<44:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  66%|██████▋   | 6160/9281 [1:27:44<44:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  66%|██████▋   | 6161/9281 [1:27:45<44:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  66%|██████▋   | 6162/9281 [1:27:46<44:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  66%|██████▋   | 6163/9281 [1:27:47<44:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  66%|██████▋   | 6164/9281 [1:27:48<44:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  66%|██████▋   | 6165/9281 [1:27:49<44:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.99e-01:  66%|██████▋   | 6166/9281 [1:27:49<44:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  66%|██████▋   | 6167/9281 [1:27:50<44:38,  1.16it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  66%|██████▋   | 6168/9281 [1:27:51<44:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  66%|██████▋   | 6169/9281 [1:27:52<44:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  66%|██████▋   | 6170/9281 [1:27:53<44:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  66%|██████▋   | 6171/9281 [1:27:54<44:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  67%|██████▋   | 6172/9281 [1:27:55<44:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  67%|██████▋   | 6173/9281 [1:27:55<44:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  67%|██████▋   | 6174/9281 [1:27:56<44:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  67%|██████▋   | 6175/9281 [1:27:57<44:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  67%|██████▋   | 6176/9281 [1:27:58<44:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  67%|██████▋   | 6177/9281 [1:27:59<44:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  67%|██████▋   | 6178/9281 [1:28:00<44:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  67%|██████▋   | 6179/9281 [1:28:01<44:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  67%|██████▋   | 6180/9281 [1:28:01<44:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.54e-01:  67%|██████▋   | 6181/9281 [1:28:02<44:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  67%|██████▋   | 6182/9281 [1:28:03<44:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  67%|██████▋   | 6183/9281 [1:28:04<44:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  67%|██████▋   | 6184/9281 [1:28:05<44:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  67%|██████▋   | 6185/9281 [1:28:06<44:18,  1.16it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  67%|██████▋   | 6186/9281 [1:28:07<44:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  67%|██████▋   | 6187/9281 [1:28:07<44:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  67%|██████▋   | 6188/9281 [1:28:08<44:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  67%|██████▋   | 6189/9281 [1:28:09<44:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  67%|██████▋   | 6190/9281 [1:28:10<43:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  67%|██████▋   | 6191/9281 [1:28:11<43:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  67%|██████▋   | 6192/9281 [1:28:12<44:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  67%|██████▋   | 6193/9281 [1:28:13<44:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  67%|██████▋   | 6194/9281 [1:28:13<44:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  67%|██████▋   | 6195/9281 [1:28:14<43:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  67%|██████▋   | 6196/9281 [1:28:15<43:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  67%|██████▋   | 6197/9281 [1:28:16<44:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  67%|██████▋   | 6198/9281 [1:28:17<44:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  67%|██████▋   | 6199/9281 [1:28:18<43:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  67%|██████▋   | 6200/9281 [1:28:19<43:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  67%|██████▋   | 6201/9281 [1:28:19<43:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  67%|██████▋   | 6202/9281 [1:28:20<43:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  67%|██████▋   | 6203/9281 [1:28:21<43:34,  1.18it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  67%|██████▋   | 6204/9281 [1:28:22<43:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  67%|██████▋   | 6205/9281 [1:28:23<43:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  67%|██████▋   | 6206/9281 [1:28:24<43:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  67%|██████▋   | 6207/9281 [1:28:25<43:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  67%|██████▋   | 6208/9281 [1:28:25<43:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  67%|██████▋   | 6209/9281 [1:28:26<43:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  67%|██████▋   | 6210/9281 [1:28:27<43:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  67%|██████▋   | 6211/9281 [1:28:28<43:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  67%|██████▋   | 6212/9281 [1:28:29<43:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  67%|██████▋   | 6213/9281 [1:28:30<43:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  67%|██████▋   | 6214/9281 [1:28:31<43:56,  1.16it/s]\u001b[A\n",
      "Training loss: 4.81e-01:  67%|██████▋   | 6215/9281 [1:28:31<43:53,  1.16it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  67%|██████▋   | 6216/9281 [1:28:32<43:55,  1.16it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  67%|██████▋   | 6217/9281 [1:28:33<43:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  67%|██████▋   | 6218/9281 [1:28:34<43:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  67%|██████▋   | 6219/9281 [1:28:35<43:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  67%|██████▋   | 6220/9281 [1:28:36<43:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  67%|██████▋   | 6221/9281 [1:28:37<43:21,  1.18it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  67%|██████▋   | 6222/9281 [1:28:37<43:20,  1.18it/s]\u001b[A\n",
      "Training loss: 4.93e-01:  67%|██████▋   | 6223/9281 [1:28:38<43:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  67%|██████▋   | 6224/9281 [1:28:39<43:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  67%|██████▋   | 6225/9281 [1:28:40<43:19,  1.18it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  67%|██████▋   | 6226/9281 [1:28:41<43:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  67%|██████▋   | 6227/9281 [1:28:42<43:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  67%|██████▋   | 6228/9281 [1:28:42<43:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  67%|██████▋   | 6229/9281 [1:28:43<43:16,  1.18it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  67%|██████▋   | 6230/9281 [1:28:44<43:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.29e-01:  67%|██████▋   | 6231/9281 [1:28:45<43:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  67%|██████▋   | 6232/9281 [1:28:46<43:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  67%|██████▋   | 6233/9281 [1:28:47<43:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.39e-01:  67%|██████▋   | 6234/9281 [1:28:48<43:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  67%|██████▋   | 6235/9281 [1:28:48<43:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  67%|██████▋   | 6236/9281 [1:28:49<43:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  67%|██████▋   | 6237/9281 [1:28:50<43:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  67%|██████▋   | 6238/9281 [1:28:51<43:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  67%|██████▋   | 6239/9281 [1:28:52<43:08,  1.18it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  67%|██████▋   | 6240/9281 [1:28:53<43:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  67%|██████▋   | 6241/9281 [1:28:54<43:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  67%|██████▋   | 6242/9281 [1:28:54<43:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  67%|██████▋   | 6243/9281 [1:28:55<43:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  67%|██████▋   | 6244/9281 [1:28:56<43:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  67%|██████▋   | 6245/9281 [1:28:57<43:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  67%|██████▋   | 6246/9281 [1:28:58<43:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  67%|██████▋   | 6247/9281 [1:28:59<43:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  67%|██████▋   | 6248/9281 [1:29:00<43:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  67%|██████▋   | 6249/9281 [1:29:00<43:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  67%|██████▋   | 6250/9281 [1:29:01<43:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  67%|██████▋   | 6251/9281 [1:29:02<42:57,  1.18it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  67%|██████▋   | 6252/9281 [1:29:03<43:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  67%|██████▋   | 6253/9281 [1:29:04<43:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  67%|██████▋   | 6254/9281 [1:29:05<43:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  67%|██████▋   | 6255/9281 [1:29:06<43:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  67%|██████▋   | 6256/9281 [1:29:06<43:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  67%|██████▋   | 6257/9281 [1:29:07<43:19,  1.16it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  67%|██████▋   | 6258/9281 [1:29:08<43:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  67%|██████▋   | 6259/9281 [1:29:09<43:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  67%|██████▋   | 6260/9281 [1:29:10<42:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  67%|██████▋   | 6261/9281 [1:29:11<43:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  67%|██████▋   | 6262/9281 [1:29:12<42:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  67%|██████▋   | 6263/9281 [1:29:12<43:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  67%|██████▋   | 6264/9281 [1:29:13<43:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  68%|██████▊   | 6265/9281 [1:29:14<42:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  68%|██████▊   | 6266/9281 [1:29:15<43:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  68%|██████▊   | 6267/9281 [1:29:16<43:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  68%|██████▊   | 6268/9281 [1:29:17<43:07,  1.16it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  68%|██████▊   | 6269/9281 [1:29:18<43:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  68%|██████▊   | 6270/9281 [1:29:18<42:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.23e-01:  68%|██████▊   | 6271/9281 [1:29:19<42:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  68%|██████▊   | 6272/9281 [1:29:20<42:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  68%|██████▊   | 6273/9281 [1:29:21<42:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  68%|██████▊   | 6274/9281 [1:29:22<42:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  68%|██████▊   | 6275/9281 [1:29:23<42:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.04e-01:  68%|██████▊   | 6276/9281 [1:29:24<42:43,  1.17it/s]\u001b[A\n",
      "Training loss: 2.93e-01:  68%|██████▊   | 6277/9281 [1:29:24<42:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  68%|██████▊   | 6278/9281 [1:29:25<42:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  68%|██████▊   | 6279/9281 [1:29:26<42:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  68%|██████▊   | 6280/9281 [1:29:27<42:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  68%|██████▊   | 6281/9281 [1:29:28<42:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  68%|██████▊   | 6282/9281 [1:29:29<42:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  68%|██████▊   | 6283/9281 [1:29:29<42:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  68%|██████▊   | 6284/9281 [1:29:30<42:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  68%|██████▊   | 6285/9281 [1:29:31<42:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  68%|██████▊   | 6286/9281 [1:29:32<42:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  68%|██████▊   | 6287/9281 [1:29:33<42:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  68%|██████▊   | 6288/9281 [1:29:34<42:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  68%|██████▊   | 6289/9281 [1:29:35<42:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  68%|██████▊   | 6290/9281 [1:29:35<42:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  68%|██████▊   | 6291/9281 [1:29:36<42:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  68%|██████▊   | 6292/9281 [1:29:37<42:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  68%|██████▊   | 6293/9281 [1:29:38<42:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  68%|██████▊   | 6294/9281 [1:29:39<42:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  68%|██████▊   | 6295/9281 [1:29:40<42:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  68%|██████▊   | 6296/9281 [1:29:41<42:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  68%|██████▊   | 6297/9281 [1:29:41<42:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  68%|██████▊   | 6298/9281 [1:29:42<42:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  68%|██████▊   | 6299/9281 [1:29:43<42:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  68%|██████▊   | 6300/9281 [1:29:44<42:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  68%|██████▊   | 6301/9281 [1:29:45<42:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  68%|██████▊   | 6302/9281 [1:29:46<42:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  68%|██████▊   | 6303/9281 [1:29:47<42:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  68%|██████▊   | 6304/9281 [1:29:47<42:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  68%|██████▊   | 6305/9281 [1:29:48<42:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  68%|██████▊   | 6306/9281 [1:29:49<42:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  68%|██████▊   | 6307/9281 [1:29:50<42:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  68%|██████▊   | 6308/9281 [1:29:51<42:37,  1.16it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  68%|██████▊   | 6309/9281 [1:29:52<42:36,  1.16it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  68%|██████▊   | 6310/9281 [1:29:53<42:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  68%|██████▊   | 6311/9281 [1:29:53<42:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  68%|██████▊   | 6312/9281 [1:29:54<42:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  68%|██████▊   | 6313/9281 [1:29:55<42:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  68%|██████▊   | 6314/9281 [1:29:56<42:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  68%|██████▊   | 6315/9281 [1:29:57<42:32,  1.16it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  68%|██████▊   | 6316/9281 [1:29:58<42:26,  1.16it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  68%|██████▊   | 6317/9281 [1:29:59<42:29,  1.16it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  68%|██████▊   | 6318/9281 [1:29:59<42:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  68%|██████▊   | 6319/9281 [1:30:00<42:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  68%|██████▊   | 6320/9281 [1:30:01<42:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  68%|██████▊   | 6321/9281 [1:30:02<42:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  68%|██████▊   | 6322/9281 [1:30:03<42:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  68%|██████▊   | 6323/9281 [1:30:04<42:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  68%|██████▊   | 6324/9281 [1:30:05<42:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  68%|██████▊   | 6325/9281 [1:30:05<42:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  68%|██████▊   | 6326/9281 [1:30:06<41:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  68%|██████▊   | 6327/9281 [1:30:07<42:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  68%|██████▊   | 6328/9281 [1:30:08<41:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  68%|██████▊   | 6329/9281 [1:30:09<42:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  68%|██████▊   | 6330/9281 [1:30:10<42:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  68%|██████▊   | 6331/9281 [1:30:11<41:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  68%|██████▊   | 6332/9281 [1:30:11<41:47,  1.18it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  68%|██████▊   | 6333/9281 [1:30:12<41:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  68%|██████▊   | 6334/9281 [1:30:13<41:47,  1.18it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  68%|██████▊   | 6335/9281 [1:30:14<41:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  68%|██████▊   | 6336/9281 [1:30:15<41:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  68%|██████▊   | 6337/9281 [1:30:16<41:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.09e-01:  68%|██████▊   | 6338/9281 [1:30:17<41:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  68%|██████▊   | 6339/9281 [1:30:17<41:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  68%|██████▊   | 6340/9281 [1:30:18<41:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.34e-01:  68%|██████▊   | 6341/9281 [1:30:19<41:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  68%|██████▊   | 6342/9281 [1:30:20<41:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  68%|██████▊   | 6343/9281 [1:30:21<41:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  68%|██████▊   | 6344/9281 [1:30:22<41:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  68%|██████▊   | 6345/9281 [1:30:22<41:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  68%|██████▊   | 6346/9281 [1:30:23<42:06,  1.16it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  68%|██████▊   | 6347/9281 [1:30:24<41:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  68%|██████▊   | 6348/9281 [1:30:25<42:00,  1.16it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  68%|██████▊   | 6349/9281 [1:30:26<41:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  68%|██████▊   | 6350/9281 [1:30:27<41:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  68%|██████▊   | 6351/9281 [1:30:28<41:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  68%|██████▊   | 6352/9281 [1:30:28<41:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  68%|██████▊   | 6353/9281 [1:30:29<41:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  68%|██████▊   | 6354/9281 [1:30:30<41:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  68%|██████▊   | 6355/9281 [1:30:31<41:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  68%|██████▊   | 6356/9281 [1:30:32<41:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  68%|██████▊   | 6357/9281 [1:30:33<41:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  69%|██████▊   | 6358/9281 [1:30:34<41:49,  1.16it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  69%|██████▊   | 6359/9281 [1:30:34<41:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  69%|██████▊   | 6360/9281 [1:30:35<41:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  69%|██████▊   | 6361/9281 [1:30:36<41:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.14e-01:  69%|██████▊   | 6362/9281 [1:30:37<41:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  69%|██████▊   | 6363/9281 [1:30:38<41:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  69%|██████▊   | 6364/9281 [1:30:39<41:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  69%|██████▊   | 6365/9281 [1:30:40<41:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  69%|██████▊   | 6366/9281 [1:30:40<41:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  69%|██████▊   | 6367/9281 [1:30:41<41:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  69%|██████▊   | 6368/9281 [1:30:42<41:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  69%|██████▊   | 6369/9281 [1:30:43<41:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  69%|██████▊   | 6370/9281 [1:30:44<41:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  69%|██████▊   | 6371/9281 [1:30:45<41:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  69%|██████▊   | 6372/9281 [1:30:46<41:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.95e-01:  69%|██████▊   | 6373/9281 [1:30:46<41:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  69%|██████▊   | 6374/9281 [1:30:47<41:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  69%|██████▊   | 6375/9281 [1:30:48<41:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  69%|██████▊   | 6376/9281 [1:30:49<41:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  69%|██████▊   | 6377/9281 [1:30:50<41:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  69%|██████▊   | 6378/9281 [1:30:51<41:08,  1.18it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  69%|██████▊   | 6379/9281 [1:30:52<41:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  69%|██████▊   | 6380/9281 [1:30:52<41:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  69%|██████▉   | 6381/9281 [1:30:53<41:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  69%|██████▉   | 6382/9281 [1:30:54<41:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  69%|██████▉   | 6383/9281 [1:30:55<41:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  69%|██████▉   | 6384/9281 [1:30:56<41:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  69%|██████▉   | 6385/9281 [1:30:57<41:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  69%|██████▉   | 6386/9281 [1:30:58<41:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  69%|██████▉   | 6387/9281 [1:30:58<41:26,  1.16it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  69%|██████▉   | 6388/9281 [1:30:59<41:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  69%|██████▉   | 6389/9281 [1:31:00<41:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  69%|██████▉   | 6390/9281 [1:31:01<41:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  69%|██████▉   | 6391/9281 [1:31:02<41:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  69%|██████▉   | 6392/9281 [1:31:03<41:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  69%|██████▉   | 6393/9281 [1:31:04<41:19,  1.16it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  69%|██████▉   | 6394/9281 [1:31:04<41:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  69%|██████▉   | 6395/9281 [1:31:05<41:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  69%|██████▉   | 6396/9281 [1:31:06<41:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  69%|██████▉   | 6397/9281 [1:31:07<41:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  69%|██████▉   | 6398/9281 [1:31:08<40:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.66e-01:  69%|██████▉   | 6399/9281 [1:31:09<41:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  69%|██████▉   | 6400/9281 [1:31:10<41:00,  1.17it/s]\u001b[A\n",
      "Training loss: 5.09e-01:  69%|██████▉   | 6401/9281 [1:31:10<41:14,  1.16it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  69%|██████▉   | 6402/9281 [1:31:11<41:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  69%|██████▉   | 6403/9281 [1:31:12<41:11,  1.16it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  69%|██████▉   | 6404/9281 [1:31:13<41:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  69%|██████▉   | 6405/9281 [1:31:14<41:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  69%|██████▉   | 6406/9281 [1:31:15<41:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  69%|██████▉   | 6407/9281 [1:31:16<40:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  69%|██████▉   | 6408/9281 [1:31:16<40:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  69%|██████▉   | 6409/9281 [1:31:17<40:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.24e-01:  69%|██████▉   | 6410/9281 [1:31:18<40:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.34e-01:  69%|██████▉   | 6411/9281 [1:31:19<40:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  69%|██████▉   | 6412/9281 [1:31:20<40:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.29e-01:  69%|██████▉   | 6413/9281 [1:31:21<40:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  69%|██████▉   | 6414/9281 [1:31:21<40:39,  1.18it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  69%|██████▉   | 6415/9281 [1:31:22<40:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  69%|██████▉   | 6416/9281 [1:31:23<40:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  69%|██████▉   | 6417/9281 [1:31:24<40:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  69%|██████▉   | 6418/9281 [1:31:25<40:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  69%|██████▉   | 6419/9281 [1:31:26<40:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  69%|██████▉   | 6420/9281 [1:31:27<40:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  69%|██████▉   | 6421/9281 [1:31:27<40:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  69%|██████▉   | 6422/9281 [1:31:28<40:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  69%|██████▉   | 6423/9281 [1:31:29<40:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  69%|██████▉   | 6424/9281 [1:31:30<40:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  69%|██████▉   | 6425/9281 [1:31:31<40:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  69%|██████▉   | 6426/9281 [1:31:32<40:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  69%|██████▉   | 6427/9281 [1:31:33<40:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  69%|██████▉   | 6428/9281 [1:31:33<40:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  69%|██████▉   | 6429/9281 [1:31:34<40:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  69%|██████▉   | 6430/9281 [1:31:35<40:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  69%|██████▉   | 6431/9281 [1:31:36<40:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  69%|██████▉   | 6432/9281 [1:31:37<40:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  69%|██████▉   | 6433/9281 [1:31:38<40:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  69%|██████▉   | 6434/9281 [1:31:39<40:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  69%|██████▉   | 6435/9281 [1:31:39<40:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  69%|██████▉   | 6436/9281 [1:31:40<40:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  69%|██████▉   | 6437/9281 [1:31:41<40:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  69%|██████▉   | 6438/9281 [1:31:42<40:33,  1.17it/s]\u001b[A\n",
      "Training loss: 5.40e-01:  69%|██████▉   | 6439/9281 [1:31:43<40:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.69e-01:  69%|██████▉   | 6440/9281 [1:31:44<40:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  69%|██████▉   | 6441/9281 [1:31:45<40:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  69%|██████▉   | 6442/9281 [1:31:45<40:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  69%|██████▉   | 6443/9281 [1:31:46<40:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  69%|██████▉   | 6444/9281 [1:31:47<40:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  69%|██████▉   | 6445/9281 [1:31:48<40:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  69%|██████▉   | 6446/9281 [1:31:49<40:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  69%|██████▉   | 6447/9281 [1:31:50<40:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  69%|██████▉   | 6448/9281 [1:31:51<40:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  69%|██████▉   | 6449/9281 [1:31:51<40:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  69%|██████▉   | 6450/9281 [1:31:52<40:31,  1.16it/s]\u001b[A\n",
      "Training loss: 4.86e-01:  70%|██████▉   | 6451/9281 [1:31:53<40:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  70%|██████▉   | 6452/9281 [1:31:54<40:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  70%|██████▉   | 6453/9281 [1:31:55<40:17,  1.17it/s]\u001b[A\n",
      "Training loss: 5.18e-01:  70%|██████▉   | 6454/9281 [1:31:56<40:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.96e-01:  70%|██████▉   | 6455/9281 [1:31:57<40:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  70%|██████▉   | 6456/9281 [1:31:57<40:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  70%|██████▉   | 6457/9281 [1:31:58<40:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  70%|██████▉   | 6458/9281 [1:31:59<40:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  70%|██████▉   | 6459/9281 [1:32:00<40:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  70%|██████▉   | 6460/9281 [1:32:01<40:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  70%|██████▉   | 6461/9281 [1:32:02<40:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  70%|██████▉   | 6462/9281 [1:32:03<40:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  70%|██████▉   | 6463/9281 [1:32:03<40:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  70%|██████▉   | 6464/9281 [1:32:04<39:54,  1.18it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  70%|██████▉   | 6465/9281 [1:32:05<39:53,  1.18it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  70%|██████▉   | 6466/9281 [1:32:06<39:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  70%|██████▉   | 6467/9281 [1:32:07<39:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  70%|██████▉   | 6468/9281 [1:32:08<39:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  70%|██████▉   | 6469/9281 [1:32:08<39:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  70%|██████▉   | 6470/9281 [1:32:09<39:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  70%|██████▉   | 6471/9281 [1:32:10<40:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  70%|██████▉   | 6472/9281 [1:32:11<40:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  70%|██████▉   | 6473/9281 [1:32:12<40:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  70%|██████▉   | 6474/9281 [1:32:13<39:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  70%|██████▉   | 6475/9281 [1:32:14<39:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  70%|██████▉   | 6476/9281 [1:32:14<39:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  70%|██████▉   | 6477/9281 [1:32:15<40:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  70%|██████▉   | 6478/9281 [1:32:16<39:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  70%|██████▉   | 6479/9281 [1:32:17<39:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  70%|██████▉   | 6480/9281 [1:32:18<39:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  70%|██████▉   | 6481/9281 [1:32:19<39:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  70%|██████▉   | 6482/9281 [1:32:20<39:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  70%|██████▉   | 6483/9281 [1:32:20<39:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  70%|██████▉   | 6484/9281 [1:32:21<39:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  70%|██████▉   | 6485/9281 [1:32:22<39:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  70%|██████▉   | 6486/9281 [1:32:23<39:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  70%|██████▉   | 6487/9281 [1:32:24<39:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  70%|██████▉   | 6488/9281 [1:32:25<39:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  70%|██████▉   | 6489/9281 [1:32:26<39:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  70%|██████▉   | 6490/9281 [1:32:26<39:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  70%|██████▉   | 6491/9281 [1:32:27<39:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  70%|██████▉   | 6492/9281 [1:32:28<39:47,  1.17it/s]\u001b[A\n",
      "Training loss: 5.36e-01:  70%|██████▉   | 6493/9281 [1:32:29<39:45,  1.17it/s]\u001b[A\n",
      "Training loss: 5.04e-01:  70%|██████▉   | 6494/9281 [1:32:30<39:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  70%|██████▉   | 6495/9281 [1:32:31<39:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  70%|██████▉   | 6496/9281 [1:32:32<39:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  70%|███████   | 6497/9281 [1:32:32<39:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  70%|███████   | 6498/9281 [1:32:33<39:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  70%|███████   | 6499/9281 [1:32:34<39:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  70%|███████   | 6500/9281 [1:32:35<39:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  70%|███████   | 6501/9281 [1:32:36<39:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  70%|███████   | 6502/9281 [1:32:37<39:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  70%|███████   | 6503/9281 [1:32:38<39:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  70%|███████   | 6504/9281 [1:32:38<39:20,  1.18it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  70%|███████   | 6505/9281 [1:32:39<39:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  70%|███████   | 6506/9281 [1:32:40<39:21,  1.18it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  70%|███████   | 6507/9281 [1:32:41<39:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  70%|███████   | 6508/9281 [1:32:42<39:15,  1.18it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  70%|███████   | 6509/9281 [1:32:43<39:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  70%|███████   | 6510/9281 [1:32:44<39:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  70%|███████   | 6511/9281 [1:32:44<39:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  70%|███████   | 6512/9281 [1:32:45<39:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  70%|███████   | 6513/9281 [1:32:46<39:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  70%|███████   | 6514/9281 [1:32:47<39:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  70%|███████   | 6515/9281 [1:32:48<39:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  70%|███████   | 6516/9281 [1:32:49<39:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.68e-01:  70%|███████   | 6517/9281 [1:32:49<39:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  70%|███████   | 6518/9281 [1:32:50<39:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  70%|███████   | 6519/9281 [1:32:51<39:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  70%|███████   | 6520/9281 [1:32:52<39:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  70%|███████   | 6521/9281 [1:32:53<39:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  70%|███████   | 6522/9281 [1:32:54<39:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  70%|███████   | 6523/9281 [1:32:55<39:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  70%|███████   | 6524/9281 [1:32:55<39:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  70%|███████   | 6525/9281 [1:32:56<39:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  70%|███████   | 6526/9281 [1:32:57<39:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  70%|███████   | 6527/9281 [1:32:58<39:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  70%|███████   | 6528/9281 [1:32:59<39:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  70%|███████   | 6529/9281 [1:33:00<39:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  70%|███████   | 6530/9281 [1:33:01<39:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  70%|███████   | 6531/9281 [1:33:01<39:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  70%|███████   | 6532/9281 [1:33:02<39:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  70%|███████   | 6533/9281 [1:33:03<39:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  70%|███████   | 6534/9281 [1:33:04<39:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  70%|███████   | 6535/9281 [1:33:05<39:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  70%|███████   | 6536/9281 [1:33:06<39:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  70%|███████   | 6537/9281 [1:33:07<39:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  70%|███████   | 6538/9281 [1:33:07<38:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  70%|███████   | 6539/9281 [1:33:08<39:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  70%|███████   | 6540/9281 [1:33:09<38:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  70%|███████   | 6541/9281 [1:33:10<38:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  70%|███████   | 6542/9281 [1:33:11<39:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  70%|███████   | 6543/9281 [1:33:12<39:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  71%|███████   | 6544/9281 [1:33:13<39:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  71%|███████   | 6545/9281 [1:33:13<38:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  71%|███████   | 6546/9281 [1:33:14<38:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  71%|███████   | 6547/9281 [1:33:15<38:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  71%|███████   | 6548/9281 [1:33:16<39:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  71%|███████   | 6549/9281 [1:33:17<38:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  71%|███████   | 6550/9281 [1:33:18<38:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  71%|███████   | 6551/9281 [1:33:19<38:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.22e-01:  71%|███████   | 6552/9281 [1:33:19<38:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.15e-01:  71%|███████   | 6553/9281 [1:33:20<38:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  71%|███████   | 6554/9281 [1:33:21<39:03,  1.16it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  71%|███████   | 6555/9281 [1:33:22<39:00,  1.16it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  71%|███████   | 6556/9281 [1:33:23<38:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  71%|███████   | 6557/9281 [1:33:24<38:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.96e-01:  71%|███████   | 6558/9281 [1:33:25<38:53,  1.17it/s]\u001b[A\n",
      "Training loss: 5.01e-01:  71%|███████   | 6559/9281 [1:33:25<38:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.79e-01:  71%|███████   | 6560/9281 [1:33:26<38:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.77e-01:  71%|███████   | 6561/9281 [1:33:27<38:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  71%|███████   | 6562/9281 [1:33:28<38:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  71%|███████   | 6563/9281 [1:33:29<38:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  71%|███████   | 6564/9281 [1:33:30<38:30,  1.18it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  71%|███████   | 6565/9281 [1:33:30<38:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  71%|███████   | 6566/9281 [1:33:31<38:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  71%|███████   | 6567/9281 [1:33:32<38:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  71%|███████   | 6568/9281 [1:33:33<38:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  71%|███████   | 6569/9281 [1:33:34<38:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  71%|███████   | 6570/9281 [1:33:35<38:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  71%|███████   | 6571/9281 [1:33:36<38:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  71%|███████   | 6572/9281 [1:33:36<38:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  71%|███████   | 6573/9281 [1:33:37<38:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  71%|███████   | 6574/9281 [1:33:38<38:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  71%|███████   | 6575/9281 [1:33:39<38:43,  1.16it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  71%|███████   | 6576/9281 [1:33:40<38:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  71%|███████   | 6577/9281 [1:33:41<38:41,  1.16it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  71%|███████   | 6578/9281 [1:33:42<38:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  71%|███████   | 6579/9281 [1:33:42<38:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  71%|███████   | 6580/9281 [1:33:43<38:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  71%|███████   | 6581/9281 [1:33:44<38:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  71%|███████   | 6582/9281 [1:33:45<38:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  71%|███████   | 6583/9281 [1:33:46<38:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  71%|███████   | 6584/9281 [1:33:47<38:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  71%|███████   | 6585/9281 [1:33:48<38:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  71%|███████   | 6586/9281 [1:33:48<38:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  71%|███████   | 6587/9281 [1:33:49<38:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  71%|███████   | 6588/9281 [1:33:50<38:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  71%|███████   | 6589/9281 [1:33:51<38:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  71%|███████   | 6590/9281 [1:33:52<38:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  71%|███████   | 6591/9281 [1:33:53<38:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  71%|███████   | 6592/9281 [1:33:54<38:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  71%|███████   | 6593/9281 [1:33:54<38:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  71%|███████   | 6594/9281 [1:33:55<38:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  71%|███████   | 6595/9281 [1:33:56<38:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  71%|███████   | 6596/9281 [1:33:57<38:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  71%|███████   | 6597/9281 [1:33:58<38:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  71%|███████   | 6598/9281 [1:33:59<38:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  71%|███████   | 6599/9281 [1:34:00<38:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.11e-01:  71%|███████   | 6600/9281 [1:34:00<38:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.06e-01:  71%|███████   | 6601/9281 [1:34:01<38:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  71%|███████   | 6602/9281 [1:34:02<37:59,  1.18it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  71%|███████   | 6603/9281 [1:34:03<37:56,  1.18it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  71%|███████   | 6604/9281 [1:34:04<37:56,  1.18it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  71%|███████   | 6605/9281 [1:34:05<37:55,  1.18it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  71%|███████   | 6606/9281 [1:34:06<37:54,  1.18it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  71%|███████   | 6607/9281 [1:34:06<38:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  71%|███████   | 6608/9281 [1:34:07<37:54,  1.18it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  71%|███████   | 6609/9281 [1:34:08<38:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  71%|███████   | 6610/9281 [1:34:09<37:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  71%|███████   | 6611/9281 [1:34:10<37:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  71%|███████   | 6612/9281 [1:34:11<37:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  71%|███████▏  | 6613/9281 [1:34:12<37:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  71%|███████▏  | 6614/9281 [1:34:12<37:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  71%|███████▏  | 6615/9281 [1:34:13<37:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  71%|███████▏  | 6616/9281 [1:34:14<37:47,  1.18it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  71%|███████▏  | 6617/9281 [1:34:15<37:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  71%|███████▏  | 6618/9281 [1:34:16<37:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  71%|███████▏  | 6619/9281 [1:34:17<37:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  71%|███████▏  | 6620/9281 [1:34:17<37:39,  1.18it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  71%|███████▏  | 6621/9281 [1:34:18<37:38,  1.18it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  71%|███████▏  | 6622/9281 [1:34:19<37:38,  1.18it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  71%|███████▏  | 6623/9281 [1:34:20<37:38,  1.18it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  71%|███████▏  | 6624/9281 [1:34:21<37:38,  1.18it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  71%|███████▏  | 6625/9281 [1:34:22<37:39,  1.18it/s]\u001b[A\n",
      "Training loss: 3.21e-01:  71%|███████▏  | 6626/9281 [1:34:23<37:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  71%|███████▏  | 6627/9281 [1:34:23<37:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  71%|███████▏  | 6628/9281 [1:34:24<37:37,  1.18it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  71%|███████▏  | 6629/9281 [1:34:25<37:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  71%|███████▏  | 6630/9281 [1:34:26<37:33,  1.18it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  71%|███████▏  | 6631/9281 [1:34:27<37:33,  1.18it/s]\u001b[A\n",
      "Training loss: 3.31e-01:  71%|███████▏  | 6632/9281 [1:34:28<37:46,  1.17it/s]\u001b[A\n",
      "Training loss: 2.96e-01:  71%|███████▏  | 6633/9281 [1:34:29<37:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.27e-01:  71%|███████▏  | 6634/9281 [1:34:29<37:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  71%|███████▏  | 6635/9281 [1:34:30<37:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  72%|███████▏  | 6636/9281 [1:34:31<37:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  72%|███████▏  | 6637/9281 [1:34:32<37:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  72%|███████▏  | 6638/9281 [1:34:33<37:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  72%|███████▏  | 6639/9281 [1:34:34<37:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  72%|███████▏  | 6640/9281 [1:34:35<37:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  72%|███████▏  | 6641/9281 [1:34:35<37:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  72%|███████▏  | 6642/9281 [1:34:36<37:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  72%|███████▏  | 6643/9281 [1:34:37<37:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.92e-01:  72%|███████▏  | 6644/9281 [1:34:38<37:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.81e-01:  72%|███████▏  | 6645/9281 [1:34:39<37:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.81e-01:  72%|███████▏  | 6646/9281 [1:34:40<37:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  72%|███████▏  | 6647/9281 [1:34:41<37:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  72%|███████▏  | 6648/9281 [1:34:41<37:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  72%|███████▏  | 6649/9281 [1:34:42<37:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  72%|███████▏  | 6650/9281 [1:34:43<37:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.91e-01:  72%|███████▏  | 6651/9281 [1:34:44<37:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  72%|███████▏  | 6652/9281 [1:34:45<37:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  72%|███████▏  | 6653/9281 [1:34:46<37:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  72%|███████▏  | 6654/9281 [1:34:46<37:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  72%|███████▏  | 6655/9281 [1:34:47<37:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  72%|███████▏  | 6656/9281 [1:34:48<37:11,  1.18it/s]\u001b[A\n",
      "Training loss: 4.81e-01:  72%|███████▏  | 6657/9281 [1:34:49<37:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  72%|███████▏  | 6658/9281 [1:34:50<37:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  72%|███████▏  | 6659/9281 [1:34:51<37:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  72%|███████▏  | 6660/9281 [1:34:52<37:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  72%|███████▏  | 6661/9281 [1:34:52<37:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  72%|███████▏  | 6662/9281 [1:34:53<37:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  72%|███████▏  | 6663/9281 [1:34:54<37:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  72%|███████▏  | 6664/9281 [1:34:55<37:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  72%|███████▏  | 6665/9281 [1:34:56<37:20,  1.17it/s]\u001b[A\n",
      "Training loss: 2.99e-01:  72%|███████▏  | 6666/9281 [1:34:57<37:19,  1.17it/s]\u001b[A\n",
      "Training loss: 2.97e-01:  72%|███████▏  | 6667/9281 [1:34:58<37:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.22e-01:  72%|███████▏  | 6668/9281 [1:34:58<37:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.22e-01:  72%|███████▏  | 6669/9281 [1:34:59<37:12,  1.17it/s]\u001b[A\n",
      "Training loss: 2.95e-01:  72%|███████▏  | 6670/9281 [1:35:00<37:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.68e-01:  72%|███████▏  | 6671/9281 [1:35:01<37:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  72%|███████▏  | 6672/9281 [1:35:02<37:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  72%|███████▏  | 6673/9281 [1:35:03<37:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  72%|███████▏  | 6674/9281 [1:35:04<37:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  72%|███████▏  | 6675/9281 [1:35:04<37:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.18e-01:  72%|███████▏  | 6676/9281 [1:35:05<36:56,  1.18it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  72%|███████▏  | 6677/9281 [1:35:06<36:55,  1.18it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  72%|███████▏  | 6678/9281 [1:35:07<36:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  72%|███████▏  | 6679/9281 [1:35:08<37:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  72%|███████▏  | 6680/9281 [1:35:09<36:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  72%|███████▏  | 6681/9281 [1:35:10<36:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  72%|███████▏  | 6682/9281 [1:35:10<36:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  72%|███████▏  | 6683/9281 [1:35:11<36:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  72%|███████▏  | 6684/9281 [1:35:12<36:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.68e-01:  72%|███████▏  | 6685/9281 [1:35:13<36:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  72%|███████▏  | 6686/9281 [1:35:14<36:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  72%|███████▏  | 6687/9281 [1:35:15<36:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  72%|███████▏  | 6688/9281 [1:35:16<36:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  72%|███████▏  | 6689/9281 [1:35:16<36:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  72%|███████▏  | 6690/9281 [1:35:17<36:43,  1.18it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  72%|███████▏  | 6691/9281 [1:35:18<36:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  72%|███████▏  | 6692/9281 [1:35:19<36:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  72%|███████▏  | 6693/9281 [1:35:20<36:40,  1.18it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  72%|███████▏  | 6694/9281 [1:35:21<36:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  72%|███████▏  | 6695/9281 [1:35:22<36:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  72%|███████▏  | 6696/9281 [1:35:22<36:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  72%|███████▏  | 6697/9281 [1:35:23<36:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  72%|███████▏  | 6698/9281 [1:35:24<36:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  72%|███████▏  | 6699/9281 [1:35:25<36:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  72%|███████▏  | 6700/9281 [1:35:26<36:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  72%|███████▏  | 6701/9281 [1:35:27<36:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  72%|███████▏  | 6702/9281 [1:35:27<36:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  72%|███████▏  | 6703/9281 [1:35:28<36:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  72%|███████▏  | 6704/9281 [1:35:29<36:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  72%|███████▏  | 6705/9281 [1:35:30<36:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  72%|███████▏  | 6706/9281 [1:35:31<36:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  72%|███████▏  | 6707/9281 [1:35:32<36:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  72%|███████▏  | 6708/9281 [1:35:33<36:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  72%|███████▏  | 6709/9281 [1:35:33<36:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  72%|███████▏  | 6710/9281 [1:35:34<36:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  72%|███████▏  | 6711/9281 [1:35:35<36:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  72%|███████▏  | 6712/9281 [1:35:36<36:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  72%|███████▏  | 6713/9281 [1:35:37<36:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  72%|███████▏  | 6714/9281 [1:35:38<36:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  72%|███████▏  | 6715/9281 [1:35:39<36:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  72%|███████▏  | 6716/9281 [1:35:39<36:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  72%|███████▏  | 6717/9281 [1:35:40<36:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  72%|███████▏  | 6718/9281 [1:35:41<36:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  72%|███████▏  | 6719/9281 [1:35:42<36:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  72%|███████▏  | 6720/9281 [1:35:43<36:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  72%|███████▏  | 6721/9281 [1:35:44<36:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  72%|███████▏  | 6722/9281 [1:35:45<36:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  72%|███████▏  | 6723/9281 [1:35:45<36:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  72%|███████▏  | 6724/9281 [1:35:46<36:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  72%|███████▏  | 6725/9281 [1:35:47<36:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  72%|███████▏  | 6726/9281 [1:35:48<36:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  72%|███████▏  | 6727/9281 [1:35:49<36:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  72%|███████▏  | 6728/9281 [1:35:50<36:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  73%|███████▎  | 6729/9281 [1:35:51<36:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.29e-01:  73%|███████▎  | 6730/9281 [1:35:51<36:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  73%|███████▎  | 6731/9281 [1:35:52<36:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.12e-01:  73%|███████▎  | 6732/9281 [1:35:53<36:09,  1.17it/s]\u001b[A\n",
      "Training loss: 2.96e-01:  73%|███████▎  | 6733/9281 [1:35:54<36:11,  1.17it/s]\u001b[A\n",
      "Training loss: 2.93e-01:  73%|███████▎  | 6734/9281 [1:35:55<36:04,  1.18it/s]\u001b[A\n",
      "Training loss: 2.94e-01:  73%|███████▎  | 6735/9281 [1:35:56<36:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  73%|███████▎  | 6736/9281 [1:35:57<36:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  73%|███████▎  | 6737/9281 [1:35:57<36:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  73%|███████▎  | 6738/9281 [1:35:58<36:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  73%|███████▎  | 6739/9281 [1:35:59<36:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  73%|███████▎  | 6740/9281 [1:36:00<36:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  73%|███████▎  | 6741/9281 [1:36:01<36:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  73%|███████▎  | 6742/9281 [1:36:02<36:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  73%|███████▎  | 6743/9281 [1:36:03<36:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  73%|███████▎  | 6744/9281 [1:36:03<36:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.31e-01:  73%|███████▎  | 6745/9281 [1:36:04<36:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  73%|███████▎  | 6746/9281 [1:36:05<35:54,  1.18it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  73%|███████▎  | 6747/9281 [1:36:06<35:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  73%|███████▎  | 6748/9281 [1:36:07<36:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  73%|███████▎  | 6749/9281 [1:36:08<36:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  73%|███████▎  | 6750/9281 [1:36:08<36:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  73%|███████▎  | 6751/9281 [1:36:09<35:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  73%|███████▎  | 6752/9281 [1:36:10<35:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  73%|███████▎  | 6753/9281 [1:36:11<35:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  73%|███████▎  | 6754/9281 [1:36:12<35:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  73%|███████▎  | 6755/9281 [1:36:13<36:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  73%|███████▎  | 6756/9281 [1:36:14<35:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  73%|███████▎  | 6757/9281 [1:36:14<35:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  73%|███████▎  | 6758/9281 [1:36:15<35:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  73%|███████▎  | 6759/9281 [1:36:16<35:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  73%|███████▎  | 6760/9281 [1:36:17<35:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  73%|███████▎  | 6761/9281 [1:36:18<35:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  73%|███████▎  | 6762/9281 [1:36:19<35:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  73%|███████▎  | 6763/9281 [1:36:20<35:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  73%|███████▎  | 6764/9281 [1:36:20<35:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  73%|███████▎  | 6765/9281 [1:36:21<35:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  73%|███████▎  | 6766/9281 [1:36:22<35:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  73%|███████▎  | 6767/9281 [1:36:23<35:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  73%|███████▎  | 6768/9281 [1:36:24<35:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.29e-01:  73%|███████▎  | 6769/9281 [1:36:25<35:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  73%|███████▎  | 6770/9281 [1:36:26<35:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  73%|███████▎  | 6771/9281 [1:36:26<35:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  73%|███████▎  | 6772/9281 [1:36:27<35:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  73%|███████▎  | 6773/9281 [1:36:28<35:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  73%|███████▎  | 6774/9281 [1:36:29<35:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  73%|███████▎  | 6775/9281 [1:36:30<35:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  73%|███████▎  | 6776/9281 [1:36:31<35:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  73%|███████▎  | 6777/9281 [1:36:32<35:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  73%|███████▎  | 6778/9281 [1:36:32<35:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  73%|███████▎  | 6779/9281 [1:36:33<35:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.22e-01:  73%|███████▎  | 6780/9281 [1:36:34<35:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  73%|███████▎  | 6781/9281 [1:36:35<35:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  73%|███████▎  | 6782/9281 [1:36:36<35:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  73%|███████▎  | 6783/9281 [1:36:37<35:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  73%|███████▎  | 6784/9281 [1:36:38<35:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  73%|███████▎  | 6785/9281 [1:36:38<35:21,  1.18it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  73%|███████▎  | 6786/9281 [1:36:39<35:20,  1.18it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  73%|███████▎  | 6787/9281 [1:36:40<35:16,  1.18it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  73%|███████▎  | 6788/9281 [1:36:41<35:16,  1.18it/s]\u001b[A\n",
      "Training loss: 3.39e-01:  73%|███████▎  | 6789/9281 [1:36:42<35:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  73%|███████▎  | 6790/9281 [1:36:43<35:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  73%|███████▎  | 6791/9281 [1:36:44<35:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  73%|███████▎  | 6792/9281 [1:36:44<35:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  73%|███████▎  | 6793/9281 [1:36:45<35:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  73%|███████▎  | 6794/9281 [1:36:46<35:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  73%|███████▎  | 6795/9281 [1:36:47<35:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.39e-01:  73%|███████▎  | 6796/9281 [1:36:48<35:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  73%|███████▎  | 6797/9281 [1:36:49<35:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  73%|███████▎  | 6798/9281 [1:36:49<35:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  73%|███████▎  | 6799/9281 [1:36:50<35:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.11e-01:  73%|███████▎  | 6800/9281 [1:36:51<35:29,  1.16it/s]\u001b[A\n",
      "Training loss: 3.16e-01:  73%|███████▎  | 6801/9281 [1:36:52<35:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.01e-01:  73%|███████▎  | 6802/9281 [1:36:53<35:30,  1.16it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  73%|███████▎  | 6803/9281 [1:36:54<35:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  73%|███████▎  | 6804/9281 [1:36:55<35:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  73%|███████▎  | 6805/9281 [1:36:55<35:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  73%|███████▎  | 6806/9281 [1:36:56<35:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  73%|███████▎  | 6807/9281 [1:36:57<35:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  73%|███████▎  | 6808/9281 [1:36:58<35:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  73%|███████▎  | 6809/9281 [1:36:59<35:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  73%|███████▎  | 6810/9281 [1:37:00<35:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  73%|███████▎  | 6811/9281 [1:37:01<35:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  73%|███████▎  | 6812/9281 [1:37:01<35:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  73%|███████▎  | 6813/9281 [1:37:02<35:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  73%|███████▎  | 6814/9281 [1:37:03<35:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  73%|███████▎  | 6815/9281 [1:37:04<35:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  73%|███████▎  | 6816/9281 [1:37:05<35:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  73%|███████▎  | 6817/9281 [1:37:06<35:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  73%|███████▎  | 6818/9281 [1:37:07<35:14,  1.16it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  73%|███████▎  | 6819/9281 [1:37:07<35:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  73%|███████▎  | 6820/9281 [1:37:08<35:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  73%|███████▎  | 6821/9281 [1:37:09<35:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  74%|███████▎  | 6822/9281 [1:37:10<34:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  74%|███████▎  | 6823/9281 [1:37:11<34:49,  1.18it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  74%|███████▎  | 6824/9281 [1:37:12<34:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  74%|███████▎  | 6825/9281 [1:37:13<34:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.31e-01:  74%|███████▎  | 6826/9281 [1:37:13<34:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  74%|███████▎  | 6827/9281 [1:37:14<34:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  74%|███████▎  | 6828/9281 [1:37:15<34:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  74%|███████▎  | 6829/9281 [1:37:16<34:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  74%|███████▎  | 6830/9281 [1:37:17<35:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  74%|███████▎  | 6831/9281 [1:37:18<34:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  74%|███████▎  | 6832/9281 [1:37:19<34:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  74%|███████▎  | 6833/9281 [1:37:19<34:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  74%|███████▎  | 6834/9281 [1:37:20<34:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  74%|███████▎  | 6835/9281 [1:37:21<34:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  74%|███████▎  | 6836/9281 [1:37:22<34:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  74%|███████▎  | 6837/9281 [1:37:23<35:02,  1.16it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  74%|███████▎  | 6838/9281 [1:37:24<35:00,  1.16it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  74%|███████▎  | 6839/9281 [1:37:25<34:56,  1.16it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  74%|███████▎  | 6840/9281 [1:37:25<34:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  74%|███████▎  | 6841/9281 [1:37:26<34:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  74%|███████▎  | 6842/9281 [1:37:27<34:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  74%|███████▎  | 6843/9281 [1:37:28<34:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  74%|███████▎  | 6844/9281 [1:37:29<34:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.97e-01:  74%|███████▍  | 6845/9281 [1:37:30<34:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.69e-01:  74%|███████▍  | 6846/9281 [1:37:31<34:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  74%|███████▍  | 6847/9281 [1:37:31<34:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  74%|███████▍  | 6848/9281 [1:37:32<34:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  74%|███████▍  | 6849/9281 [1:37:33<34:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  74%|███████▍  | 6850/9281 [1:37:34<34:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  74%|███████▍  | 6851/9281 [1:37:35<34:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  74%|███████▍  | 6852/9281 [1:37:36<34:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  74%|███████▍  | 6853/9281 [1:37:37<34:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  74%|███████▍  | 6854/9281 [1:37:37<34:21,  1.18it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  74%|███████▍  | 6855/9281 [1:37:38<34:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  74%|███████▍  | 6856/9281 [1:37:39<34:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  74%|███████▍  | 6857/9281 [1:37:40<34:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  74%|███████▍  | 6858/9281 [1:37:41<34:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  74%|███████▍  | 6859/9281 [1:37:42<34:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  74%|███████▍  | 6860/9281 [1:37:43<34:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  74%|███████▍  | 6861/9281 [1:37:43<34:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  74%|███████▍  | 6862/9281 [1:37:44<34:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  74%|███████▍  | 6863/9281 [1:37:45<34:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  74%|███████▍  | 6864/9281 [1:37:46<34:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  74%|███████▍  | 6865/9281 [1:37:47<34:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  74%|███████▍  | 6866/9281 [1:37:48<34:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  74%|███████▍  | 6867/9281 [1:37:48<34:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.09e-01:  74%|███████▍  | 6868/9281 [1:37:49<34:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  74%|███████▍  | 6869/9281 [1:37:50<34:10,  1.18it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  74%|███████▍  | 6870/9281 [1:37:51<34:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  74%|███████▍  | 6871/9281 [1:37:52<34:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  74%|███████▍  | 6872/9281 [1:37:53<34:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  74%|███████▍  | 6873/9281 [1:37:54<34:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  74%|███████▍  | 6874/9281 [1:37:54<34:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  74%|███████▍  | 6875/9281 [1:37:55<34:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  74%|███████▍  | 6876/9281 [1:37:56<34:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  74%|███████▍  | 6877/9281 [1:37:57<34:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  74%|███████▍  | 6878/9281 [1:37:58<34:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  74%|███████▍  | 6879/9281 [1:37:59<34:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  74%|███████▍  | 6880/9281 [1:38:00<34:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  74%|███████▍  | 6881/9281 [1:38:00<34:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  74%|███████▍  | 6882/9281 [1:38:01<34:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  74%|███████▍  | 6883/9281 [1:38:02<34:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  74%|███████▍  | 6884/9281 [1:38:03<34:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  74%|███████▍  | 6885/9281 [1:38:04<34:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  74%|███████▍  | 6886/9281 [1:38:05<34:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  74%|███████▍  | 6887/9281 [1:38:06<34:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  74%|███████▍  | 6888/9281 [1:38:06<34:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  74%|███████▍  | 6889/9281 [1:38:07<34:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  74%|███████▍  | 6890/9281 [1:38:08<34:14,  1.16it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  74%|███████▍  | 6891/9281 [1:38:09<34:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  74%|███████▍  | 6892/9281 [1:38:10<34:10,  1.16it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  74%|███████▍  | 6893/9281 [1:38:11<34:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  74%|███████▍  | 6894/9281 [1:38:12<34:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  74%|███████▍  | 6895/9281 [1:38:12<33:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  74%|███████▍  | 6896/9281 [1:38:13<34:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  74%|███████▍  | 6897/9281 [1:38:14<34:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.54e-01:  74%|███████▍  | 6898/9281 [1:38:15<33:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  74%|███████▍  | 6899/9281 [1:38:16<33:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  74%|███████▍  | 6900/9281 [1:38:17<33:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  74%|███████▍  | 6901/9281 [1:38:18<33:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  74%|███████▍  | 6902/9281 [1:38:18<33:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.30e-01:  74%|███████▍  | 6903/9281 [1:38:19<33:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  74%|███████▍  | 6904/9281 [1:38:20<33:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  74%|███████▍  | 6905/9281 [1:38:21<33:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  74%|███████▍  | 6906/9281 [1:38:22<33:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.01e-01:  74%|███████▍  | 6907/9281 [1:38:23<33:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  74%|███████▍  | 6908/9281 [1:38:24<33:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.07e-01:  74%|███████▍  | 6909/9281 [1:38:24<33:57,  1.16it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  74%|███████▍  | 6910/9281 [1:38:25<33:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  74%|███████▍  | 6911/9281 [1:38:26<33:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  74%|███████▍  | 6912/9281 [1:38:27<33:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.10e-01:  74%|███████▍  | 6913/9281 [1:38:28<33:54,  1.16it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  74%|███████▍  | 6914/9281 [1:38:29<33:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  75%|███████▍  | 6915/9281 [1:38:30<33:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.18e-01:  75%|███████▍  | 6916/9281 [1:38:30<33:50,  1.16it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  75%|███████▍  | 6917/9281 [1:38:31<33:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  75%|███████▍  | 6918/9281 [1:38:32<33:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  75%|███████▍  | 6919/9281 [1:38:33<33:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  75%|███████▍  | 6920/9281 [1:38:34<33:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  75%|███████▍  | 6921/9281 [1:38:35<33:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.94e-01:  75%|███████▍  | 6922/9281 [1:38:36<33:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  75%|███████▍  | 6923/9281 [1:38:36<33:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  75%|███████▍  | 6924/9281 [1:38:37<33:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  75%|███████▍  | 6925/9281 [1:38:38<33:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  75%|███████▍  | 6926/9281 [1:38:39<33:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  75%|███████▍  | 6927/9281 [1:38:40<33:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  75%|███████▍  | 6928/9281 [1:38:41<33:40,  1.16it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  75%|███████▍  | 6929/9281 [1:38:42<33:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  75%|███████▍  | 6930/9281 [1:38:42<33:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  75%|███████▍  | 6931/9281 [1:38:43<33:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  75%|███████▍  | 6932/9281 [1:38:44<33:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  75%|███████▍  | 6933/9281 [1:38:45<33:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  75%|███████▍  | 6934/9281 [1:38:46<33:35,  1.16it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  75%|███████▍  | 6935/9281 [1:38:47<33:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  75%|███████▍  | 6936/9281 [1:38:48<33:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  75%|███████▍  | 6937/9281 [1:38:48<33:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  75%|███████▍  | 6938/9281 [1:38:49<33:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  75%|███████▍  | 6939/9281 [1:38:50<33:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  75%|███████▍  | 6940/9281 [1:38:51<33:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  75%|███████▍  | 6941/9281 [1:38:52<33:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  75%|███████▍  | 6942/9281 [1:38:53<33:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  75%|███████▍  | 6943/9281 [1:38:54<33:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  75%|███████▍  | 6944/9281 [1:38:54<33:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  75%|███████▍  | 6945/9281 [1:38:55<33:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  75%|███████▍  | 6946/9281 [1:38:56<33:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  75%|███████▍  | 6947/9281 [1:38:57<33:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  75%|███████▍  | 6948/9281 [1:38:58<33:05,  1.18it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  75%|███████▍  | 6949/9281 [1:38:59<33:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  75%|███████▍  | 6950/9281 [1:38:59<33:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  75%|███████▍  | 6951/9281 [1:39:00<33:22,  1.16it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  75%|███████▍  | 6952/9281 [1:39:01<33:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  75%|███████▍  | 6953/9281 [1:39:02<33:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  75%|███████▍  | 6954/9281 [1:39:03<33:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  75%|███████▍  | 6955/9281 [1:39:04<33:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  75%|███████▍  | 6956/9281 [1:39:05<33:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  75%|███████▍  | 6957/9281 [1:39:05<33:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  75%|███████▍  | 6958/9281 [1:39:06<33:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  75%|███████▍  | 6959/9281 [1:39:07<33:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  75%|███████▍  | 6960/9281 [1:39:08<33:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  75%|███████▌  | 6961/9281 [1:39:09<33:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  75%|███████▌  | 6962/9281 [1:39:10<32:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  75%|███████▌  | 6963/9281 [1:39:11<32:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.54e-01:  75%|███████▌  | 6964/9281 [1:39:11<32:51,  1.18it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  75%|███████▌  | 6965/9281 [1:39:12<32:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  75%|███████▌  | 6966/9281 [1:39:13<32:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  75%|███████▌  | 6967/9281 [1:39:14<32:44,  1.18it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  75%|███████▌  | 6968/9281 [1:39:15<32:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  75%|███████▌  | 6969/9281 [1:39:16<32:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  75%|███████▌  | 6970/9281 [1:39:17<32:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  75%|███████▌  | 6971/9281 [1:39:17<32:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  75%|███████▌  | 6972/9281 [1:39:18<32:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  75%|███████▌  | 6973/9281 [1:39:19<32:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  75%|███████▌  | 6974/9281 [1:39:20<32:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  75%|███████▌  | 6975/9281 [1:39:21<32:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  75%|███████▌  | 6976/9281 [1:39:22<32:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  75%|███████▌  | 6977/9281 [1:39:23<32:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  75%|███████▌  | 6978/9281 [1:39:23<32:57,  1.16it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  75%|███████▌  | 6979/9281 [1:39:24<33:01,  1.16it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  75%|███████▌  | 6980/9281 [1:39:25<32:57,  1.16it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  75%|███████▌  | 6981/9281 [1:39:26<32:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  75%|███████▌  | 6982/9281 [1:39:27<32:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  75%|███████▌  | 6983/9281 [1:39:28<32:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  75%|███████▌  | 6984/9281 [1:39:29<32:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  75%|███████▌  | 6985/9281 [1:39:29<32:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  75%|███████▌  | 6986/9281 [1:39:30<32:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  75%|███████▌  | 6987/9281 [1:39:31<32:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  75%|███████▌  | 6988/9281 [1:39:32<32:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  75%|███████▌  | 6989/9281 [1:39:33<32:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  75%|███████▌  | 6990/9281 [1:39:34<32:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  75%|███████▌  | 6991/9281 [1:39:35<32:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  75%|███████▌  | 6992/9281 [1:39:35<32:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  75%|███████▌  | 6993/9281 [1:39:36<32:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  75%|███████▌  | 6994/9281 [1:39:37<32:44,  1.16it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  75%|███████▌  | 6995/9281 [1:39:38<32:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  75%|███████▌  | 6996/9281 [1:39:39<32:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  75%|███████▌  | 6997/9281 [1:39:40<32:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  75%|███████▌  | 6998/9281 [1:39:41<32:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  75%|███████▌  | 6999/9281 [1:39:41<32:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  75%|███████▌  | 7000/9281 [1:39:42<32:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  75%|███████▌  | 7001/9281 [1:39:43<32:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  75%|███████▌  | 7002/9281 [1:39:44<32:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  75%|███████▌  | 7003/9281 [1:39:45<32:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  75%|███████▌  | 7004/9281 [1:39:46<32:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  75%|███████▌  | 7005/9281 [1:39:47<32:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  75%|███████▌  | 7006/9281 [1:39:47<32:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.19e-01:  75%|███████▌  | 7007/9281 [1:39:48<32:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.19e-01:  76%|███████▌  | 7008/9281 [1:39:49<32:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  76%|███████▌  | 7009/9281 [1:39:50<32:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  76%|███████▌  | 7010/9281 [1:39:51<32:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  76%|███████▌  | 7011/9281 [1:39:52<32:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.40e-01:  76%|███████▌  | 7012/9281 [1:39:52<32:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  76%|███████▌  | 7013/9281 [1:39:53<32:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  76%|███████▌  | 7014/9281 [1:39:54<32:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  76%|███████▌  | 7015/9281 [1:39:55<32:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  76%|███████▌  | 7016/9281 [1:39:56<32:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  76%|███████▌  | 7017/9281 [1:39:57<32:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  76%|███████▌  | 7018/9281 [1:39:58<32:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.09e-01:  76%|███████▌  | 7019/9281 [1:39:58<32:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.24e-01:  76%|███████▌  | 7020/9281 [1:39:59<32:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  76%|███████▌  | 7021/9281 [1:40:00<32:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  76%|███████▌  | 7022/9281 [1:40:01<32:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  76%|███████▌  | 7023/9281 [1:40:02<32:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  76%|███████▌  | 7024/9281 [1:40:03<32:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  76%|███████▌  | 7025/9281 [1:40:04<32:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  76%|███████▌  | 7026/9281 [1:40:04<32:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  76%|███████▌  | 7027/9281 [1:40:05<31:53,  1.18it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  76%|███████▌  | 7028/9281 [1:40:06<32:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  76%|███████▌  | 7029/9281 [1:40:07<32:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  76%|███████▌  | 7030/9281 [1:40:08<32:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  76%|███████▌  | 7031/9281 [1:40:09<32:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  76%|███████▌  | 7032/9281 [1:40:10<32:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  76%|███████▌  | 7033/9281 [1:40:10<32:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  76%|███████▌  | 7034/9281 [1:40:11<31:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  76%|███████▌  | 7035/9281 [1:40:12<31:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  76%|███████▌  | 7036/9281 [1:40:13<31:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  76%|███████▌  | 7037/9281 [1:40:14<31:43,  1.18it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  76%|███████▌  | 7038/9281 [1:40:15<31:46,  1.18it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  76%|███████▌  | 7039/9281 [1:40:16<31:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  76%|███████▌  | 7040/9281 [1:40:16<31:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  76%|███████▌  | 7041/9281 [1:40:17<31:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  76%|███████▌  | 7042/9281 [1:40:18<31:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  76%|███████▌  | 7043/9281 [1:40:19<31:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  76%|███████▌  | 7044/9281 [1:40:20<31:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  76%|███████▌  | 7045/9281 [1:40:21<31:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  76%|███████▌  | 7046/9281 [1:40:22<31:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  76%|███████▌  | 7047/9281 [1:40:22<31:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  76%|███████▌  | 7048/9281 [1:40:23<31:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  76%|███████▌  | 7049/9281 [1:40:24<31:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  76%|███████▌  | 7050/9281 [1:40:25<31:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  76%|███████▌  | 7051/9281 [1:40:26<31:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  76%|███████▌  | 7052/9281 [1:40:27<31:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.22e-01:  76%|███████▌  | 7053/9281 [1:40:27<31:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.23e-01:  76%|███████▌  | 7054/9281 [1:40:28<31:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  76%|███████▌  | 7055/9281 [1:40:29<31:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  76%|███████▌  | 7056/9281 [1:40:30<31:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  76%|███████▌  | 7057/9281 [1:40:31<31:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  76%|███████▌  | 7058/9281 [1:40:32<31:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.27e-01:  76%|███████▌  | 7059/9281 [1:40:33<31:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  76%|███████▌  | 7060/9281 [1:40:33<31:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  76%|███████▌  | 7061/9281 [1:40:34<31:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  76%|███████▌  | 7062/9281 [1:40:35<31:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  76%|███████▌  | 7063/9281 [1:40:36<31:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  76%|███████▌  | 7064/9281 [1:40:37<31:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  76%|███████▌  | 7065/9281 [1:40:38<31:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  76%|███████▌  | 7066/9281 [1:40:39<31:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  76%|███████▌  | 7067/9281 [1:40:39<31:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  76%|███████▌  | 7068/9281 [1:40:40<31:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  76%|███████▌  | 7069/9281 [1:40:41<31:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  76%|███████▌  | 7070/9281 [1:40:42<31:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  76%|███████▌  | 7071/9281 [1:40:43<31:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  76%|███████▌  | 7072/9281 [1:40:44<31:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  76%|███████▌  | 7073/9281 [1:40:45<31:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  76%|███████▌  | 7074/9281 [1:40:45<31:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  76%|███████▌  | 7075/9281 [1:40:46<31:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.19e-01:  76%|███████▌  | 7076/9281 [1:40:47<31:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  76%|███████▋  | 7077/9281 [1:40:48<31:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  76%|███████▋  | 7078/9281 [1:40:49<31:14,  1.18it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  76%|███████▋  | 7079/9281 [1:40:50<31:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  76%|███████▋  | 7080/9281 [1:40:51<31:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  76%|███████▋  | 7081/9281 [1:40:51<31:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  76%|███████▋  | 7082/9281 [1:40:52<31:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  76%|███████▋  | 7083/9281 [1:40:53<31:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  76%|███████▋  | 7084/9281 [1:40:54<31:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  76%|███████▋  | 7085/9281 [1:40:55<31:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  76%|███████▋  | 7086/9281 [1:40:56<31:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  76%|███████▋  | 7087/9281 [1:40:57<31:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  76%|███████▋  | 7088/9281 [1:40:57<31:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  76%|███████▋  | 7089/9281 [1:40:58<31:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  76%|███████▋  | 7090/9281 [1:40:59<31:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  76%|███████▋  | 7091/9281 [1:41:00<31:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  76%|███████▋  | 7092/9281 [1:41:01<31:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  76%|███████▋  | 7093/9281 [1:41:02<31:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  76%|███████▋  | 7094/9281 [1:41:03<31:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  76%|███████▋  | 7095/9281 [1:41:03<31:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  76%|███████▋  | 7096/9281 [1:41:04<31:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  76%|███████▋  | 7097/9281 [1:41:05<31:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  76%|███████▋  | 7098/9281 [1:41:06<31:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  76%|███████▋  | 7099/9281 [1:41:07<31:14,  1.16it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  77%|███████▋  | 7100/9281 [1:41:08<31:17,  1.16it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  77%|███████▋  | 7101/9281 [1:41:09<31:12,  1.16it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  77%|███████▋  | 7102/9281 [1:41:09<31:16,  1.16it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  77%|███████▋  | 7103/9281 [1:41:10<31:13,  1.16it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  77%|███████▋  | 7104/9281 [1:41:11<31:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  77%|███████▋  | 7105/9281 [1:41:12<31:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  77%|███████▋  | 7106/9281 [1:41:13<31:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  77%|███████▋  | 7107/9281 [1:41:14<30:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  77%|███████▋  | 7108/9281 [1:41:15<31:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  77%|███████▋  | 7109/9281 [1:41:15<30:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  77%|███████▋  | 7110/9281 [1:41:16<30:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  77%|███████▋  | 7111/9281 [1:41:17<30:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  77%|███████▋  | 7112/9281 [1:41:18<30:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  77%|███████▋  | 7113/9281 [1:41:19<30:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.67e-01:  77%|███████▋  | 7114/9281 [1:41:20<30:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  77%|███████▋  | 7115/9281 [1:41:21<30:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  77%|███████▋  | 7116/9281 [1:41:21<30:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  77%|███████▋  | 7117/9281 [1:41:22<30:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.82e-01:  77%|███████▋  | 7118/9281 [1:41:23<30:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  77%|███████▋  | 7119/9281 [1:41:24<30:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  77%|███████▋  | 7120/9281 [1:41:25<30:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  77%|███████▋  | 7121/9281 [1:41:26<30:55,  1.16it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  77%|███████▋  | 7122/9281 [1:41:27<30:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  77%|███████▋  | 7123/9281 [1:41:27<30:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  77%|███████▋  | 7124/9281 [1:41:28<30:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  77%|███████▋  | 7125/9281 [1:41:29<30:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  77%|███████▋  | 7126/9281 [1:41:30<30:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  77%|███████▋  | 7127/9281 [1:41:31<30:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  77%|███████▋  | 7128/9281 [1:41:32<30:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  77%|███████▋  | 7129/9281 [1:41:32<30:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  77%|███████▋  | 7130/9281 [1:41:33<30:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  77%|███████▋  | 7131/9281 [1:41:34<30:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.06e-01:  77%|███████▋  | 7132/9281 [1:41:35<30:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.07e-01:  77%|███████▋  | 7133/9281 [1:41:36<30:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.39e-01:  77%|███████▋  | 7134/9281 [1:41:37<30:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  77%|███████▋  | 7135/9281 [1:41:38<30:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  77%|███████▋  | 7136/9281 [1:41:38<30:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.01e-01:  77%|███████▋  | 7137/9281 [1:41:39<30:39,  1.17it/s]\u001b[A\n",
      "Training loss: 2.96e-01:  77%|███████▋  | 7138/9281 [1:41:40<30:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.10e-01:  77%|███████▋  | 7139/9281 [1:41:41<30:30,  1.17it/s]\u001b[A\n",
      "Training loss: 2.98e-01:  77%|███████▋  | 7140/9281 [1:41:42<30:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  77%|███████▋  | 7141/9281 [1:41:43<30:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  77%|███████▋  | 7142/9281 [1:41:44<30:30,  1.17it/s]\u001b[A\n",
      "Training loss: 2.81e-01:  77%|███████▋  | 7143/9281 [1:41:44<30:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  77%|███████▋  | 7144/9281 [1:41:45<30:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  77%|███████▋  | 7145/9281 [1:41:46<30:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  77%|███████▋  | 7146/9281 [1:41:47<30:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  77%|███████▋  | 7147/9281 [1:41:48<30:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  77%|███████▋  | 7148/9281 [1:41:49<30:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.22e-01:  77%|███████▋  | 7149/9281 [1:41:50<30:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  77%|███████▋  | 7150/9281 [1:41:50<30:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  77%|███████▋  | 7151/9281 [1:41:51<30:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  77%|███████▋  | 7152/9281 [1:41:52<30:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  77%|███████▋  | 7153/9281 [1:41:53<30:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  77%|███████▋  | 7154/9281 [1:41:54<30:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  77%|███████▋  | 7155/9281 [1:41:55<30:06,  1.18it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  77%|███████▋  | 7156/9281 [1:41:56<30:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  77%|███████▋  | 7157/9281 [1:41:56<30:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  77%|███████▋  | 7158/9281 [1:41:57<30:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  77%|███████▋  | 7159/9281 [1:41:58<30:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  77%|███████▋  | 7160/9281 [1:41:59<30:01,  1.18it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  77%|███████▋  | 7161/9281 [1:42:00<30:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  77%|███████▋  | 7162/9281 [1:42:01<30:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  77%|███████▋  | 7163/9281 [1:42:02<30:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  77%|███████▋  | 7164/9281 [1:42:02<29:59,  1.18it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  77%|███████▋  | 7165/9281 [1:42:03<29:55,  1.18it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  77%|███████▋  | 7166/9281 [1:42:04<29:59,  1.18it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  77%|███████▋  | 7167/9281 [1:42:05<30:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  77%|███████▋  | 7168/9281 [1:42:06<30:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.30e-01:  77%|███████▋  | 7169/9281 [1:42:07<30:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.09e-01:  77%|███████▋  | 7170/9281 [1:42:07<30:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  77%|███████▋  | 7171/9281 [1:42:08<30:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  77%|███████▋  | 7172/9281 [1:42:09<30:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  77%|███████▋  | 7173/9281 [1:42:10<30:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  77%|███████▋  | 7174/9281 [1:42:11<30:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  77%|███████▋  | 7175/9281 [1:42:12<29:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  77%|███████▋  | 7176/9281 [1:42:13<30:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.76e-01:  77%|███████▋  | 7177/9281 [1:42:13<30:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  77%|███████▋  | 7178/9281 [1:42:14<30:05,  1.16it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  77%|███████▋  | 7179/9281 [1:42:15<29:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  77%|███████▋  | 7180/9281 [1:42:16<30:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  77%|███████▋  | 7181/9281 [1:42:17<29:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  77%|███████▋  | 7182/9281 [1:42:18<30:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  77%|███████▋  | 7183/9281 [1:42:19<30:00,  1.16it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  77%|███████▋  | 7184/9281 [1:42:19<29:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  77%|███████▋  | 7185/9281 [1:42:20<29:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  77%|███████▋  | 7186/9281 [1:42:21<29:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  77%|███████▋  | 7187/9281 [1:42:22<29:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  77%|███████▋  | 7188/9281 [1:42:23<29:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  77%|███████▋  | 7189/9281 [1:42:24<29:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  77%|███████▋  | 7190/9281 [1:42:25<29:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  77%|███████▋  | 7191/9281 [1:42:25<29:34,  1.18it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  77%|███████▋  | 7192/9281 [1:42:26<29:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  78%|███████▊  | 7193/9281 [1:42:27<29:33,  1.18it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  78%|███████▊  | 7194/9281 [1:42:28<29:35,  1.18it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  78%|███████▊  | 7195/9281 [1:42:29<29:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  78%|███████▊  | 7196/9281 [1:42:30<29:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  78%|███████▊  | 7197/9281 [1:42:31<29:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  78%|███████▊  | 7198/9281 [1:42:31<29:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.23e-01:  78%|███████▊  | 7199/9281 [1:42:32<29:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  78%|███████▊  | 7200/9281 [1:42:33<29:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  78%|███████▊  | 7201/9281 [1:42:34<29:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.34e-01:  78%|███████▊  | 7202/9281 [1:42:35<29:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  78%|███████▊  | 7203/9281 [1:42:36<29:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  78%|███████▊  | 7204/9281 [1:42:37<29:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  78%|███████▊  | 7205/9281 [1:42:37<29:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  78%|███████▊  | 7206/9281 [1:42:38<29:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  78%|███████▊  | 7207/9281 [1:42:39<29:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  78%|███████▊  | 7208/9281 [1:42:40<29:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  78%|███████▊  | 7209/9281 [1:42:41<29:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  78%|███████▊  | 7210/9281 [1:42:42<29:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  78%|███████▊  | 7211/9281 [1:42:43<29:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  78%|███████▊  | 7212/9281 [1:42:43<29:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  78%|███████▊  | 7213/9281 [1:42:44<29:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  78%|███████▊  | 7214/9281 [1:42:45<29:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  78%|███████▊  | 7215/9281 [1:42:46<29:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  78%|███████▊  | 7216/9281 [1:42:47<29:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  78%|███████▊  | 7217/9281 [1:42:48<29:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  78%|███████▊  | 7218/9281 [1:42:49<29:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  78%|███████▊  | 7219/9281 [1:42:49<29:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  78%|███████▊  | 7220/9281 [1:42:50<29:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  78%|███████▊  | 7221/9281 [1:42:51<29:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  78%|███████▊  | 7222/9281 [1:42:52<29:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  78%|███████▊  | 7223/9281 [1:42:53<29:27,  1.16it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  78%|███████▊  | 7224/9281 [1:42:54<29:26,  1.16it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  78%|███████▊  | 7225/9281 [1:42:55<29:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  78%|███████▊  | 7226/9281 [1:42:55<29:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  78%|███████▊  | 7227/9281 [1:42:56<29:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.78e-01:  78%|███████▊  | 7228/9281 [1:42:57<29:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  78%|███████▊  | 7229/9281 [1:42:58<29:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  78%|███████▊  | 7230/9281 [1:42:59<29:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  78%|███████▊  | 7231/9281 [1:43:00<29:12,  1.17it/s]\u001b[A\n",
      "Training loss: 5.06e-01:  78%|███████▊  | 7232/9281 [1:43:00<29:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.86e-01:  78%|███████▊  | 7233/9281 [1:43:01<29:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  78%|███████▊  | 7234/9281 [1:43:02<29:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  78%|███████▊  | 7235/9281 [1:43:03<29:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  78%|███████▊  | 7236/9281 [1:43:04<29:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.61e-01:  78%|███████▊  | 7237/9281 [1:43:05<29:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.75e-01:  78%|███████▊  | 7238/9281 [1:43:06<29:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  78%|███████▊  | 7239/9281 [1:43:06<29:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  78%|███████▊  | 7240/9281 [1:43:07<29:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  78%|███████▊  | 7241/9281 [1:43:08<29:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  78%|███████▊  | 7242/9281 [1:43:09<28:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  78%|███████▊  | 7243/9281 [1:43:10<29:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  78%|███████▊  | 7244/9281 [1:43:11<29:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  78%|███████▊  | 7245/9281 [1:43:12<29:08,  1.16it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  78%|███████▊  | 7246/9281 [1:43:12<29:06,  1.16it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  78%|███████▊  | 7247/9281 [1:43:13<28:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  78%|███████▊  | 7248/9281 [1:43:14<28:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  78%|███████▊  | 7249/9281 [1:43:15<29:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  78%|███████▊  | 7250/9281 [1:43:16<28:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.03e-01:  78%|███████▊  | 7251/9281 [1:43:17<28:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.09e-01:  78%|███████▊  | 7252/9281 [1:43:18<28:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.09e-01:  78%|███████▊  | 7253/9281 [1:43:18<28:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  78%|███████▊  | 7254/9281 [1:43:19<28:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.29e-01:  78%|███████▊  | 7255/9281 [1:43:20<28:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  78%|███████▊  | 7256/9281 [1:43:21<28:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  78%|███████▊  | 7257/9281 [1:43:22<28:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  78%|███████▊  | 7258/9281 [1:43:23<28:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  78%|███████▊  | 7259/9281 [1:43:24<28:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  78%|███████▊  | 7260/9281 [1:43:24<28:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  78%|███████▊  | 7261/9281 [1:43:25<28:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.10e-01:  78%|███████▊  | 7262/9281 [1:43:26<28:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  78%|███████▊  | 7263/9281 [1:43:27<28:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  78%|███████▊  | 7264/9281 [1:43:28<28:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  78%|███████▊  | 7265/9281 [1:43:29<28:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  78%|███████▊  | 7266/9281 [1:43:30<28:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  78%|███████▊  | 7267/9281 [1:43:30<28:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  78%|███████▊  | 7268/9281 [1:43:31<28:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  78%|███████▊  | 7269/9281 [1:43:32<28:31,  1.18it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  78%|███████▊  | 7270/9281 [1:43:33<28:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  78%|███████▊  | 7271/9281 [1:43:34<28:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  78%|███████▊  | 7272/9281 [1:43:35<28:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  78%|███████▊  | 7273/9281 [1:43:36<28:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  78%|███████▊  | 7274/9281 [1:43:36<28:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  78%|███████▊  | 7275/9281 [1:43:37<28:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  78%|███████▊  | 7276/9281 [1:43:38<28:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  78%|███████▊  | 7277/9281 [1:43:39<28:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  78%|███████▊  | 7278/9281 [1:43:40<28:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  78%|███████▊  | 7279/9281 [1:43:41<28:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.14e-01:  78%|███████▊  | 7280/9281 [1:43:42<28:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  78%|███████▊  | 7281/9281 [1:43:42<28:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  78%|███████▊  | 7282/9281 [1:43:43<28:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  78%|███████▊  | 7283/9281 [1:43:44<28:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  78%|███████▊  | 7284/9281 [1:43:45<28:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  78%|███████▊  | 7285/9281 [1:43:46<28:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  79%|███████▊  | 7286/9281 [1:43:47<28:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  79%|███████▊  | 7287/9281 [1:43:47<28:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  79%|███████▊  | 7288/9281 [1:43:48<28:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  79%|███████▊  | 7289/9281 [1:43:49<28:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  79%|███████▊  | 7290/9281 [1:43:50<28:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  79%|███████▊  | 7291/9281 [1:43:51<28:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  79%|███████▊  | 7292/9281 [1:43:52<28:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  79%|███████▊  | 7293/9281 [1:43:53<28:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  79%|███████▊  | 7294/9281 [1:43:53<28:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.90e-01:  79%|███████▊  | 7295/9281 [1:43:54<28:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.71e-01:  79%|███████▊  | 7296/9281 [1:43:55<28:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  79%|███████▊  | 7297/9281 [1:43:56<28:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  79%|███████▊  | 7298/9281 [1:43:57<28:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  79%|███████▊  | 7299/9281 [1:43:58<28:22,  1.16it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  79%|███████▊  | 7300/9281 [1:43:59<28:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  79%|███████▊  | 7301/9281 [1:43:59<28:20,  1.16it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  79%|███████▊  | 7302/9281 [1:44:00<28:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  79%|███████▊  | 7303/9281 [1:44:01<28:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  79%|███████▊  | 7304/9281 [1:44:02<28:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  79%|███████▊  | 7305/9281 [1:44:03<27:57,  1.18it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  79%|███████▊  | 7306/9281 [1:44:04<28:00,  1.18it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  79%|███████▊  | 7307/9281 [1:44:05<28:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  79%|███████▊  | 7308/9281 [1:44:05<28:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  79%|███████▉  | 7309/9281 [1:44:06<28:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  79%|███████▉  | 7310/9281 [1:44:07<28:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  79%|███████▉  | 7311/9281 [1:44:08<28:13,  1.16it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  79%|███████▉  | 7312/9281 [1:44:09<28:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  79%|███████▉  | 7313/9281 [1:44:10<28:09,  1.16it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  79%|███████▉  | 7314/9281 [1:44:11<28:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  79%|███████▉  | 7315/9281 [1:44:11<28:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  79%|███████▉  | 7316/9281 [1:44:12<28:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  79%|███████▉  | 7317/9281 [1:44:13<27:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  79%|███████▉  | 7318/9281 [1:44:14<27:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.39e-01:  79%|███████▉  | 7319/9281 [1:44:15<28:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  79%|███████▉  | 7320/9281 [1:44:16<28:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  79%|███████▉  | 7321/9281 [1:44:17<28:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  79%|███████▉  | 7322/9281 [1:44:17<27:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  79%|███████▉  | 7323/9281 [1:44:18<27:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  79%|███████▉  | 7324/9281 [1:44:19<27:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  79%|███████▉  | 7325/9281 [1:44:20<27:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  79%|███████▉  | 7326/9281 [1:44:21<27:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  79%|███████▉  | 7327/9281 [1:44:22<27:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  79%|███████▉  | 7328/9281 [1:44:23<27:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  79%|███████▉  | 7329/9281 [1:44:23<27:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.21e-01:  79%|███████▉  | 7330/9281 [1:44:24<27:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  79%|███████▉  | 7331/9281 [1:44:25<27:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  79%|███████▉  | 7332/9281 [1:44:26<27:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  79%|███████▉  | 7333/9281 [1:44:27<27:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  79%|███████▉  | 7334/9281 [1:44:28<27:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  79%|███████▉  | 7335/9281 [1:44:29<27:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.82e-01:  79%|███████▉  | 7336/9281 [1:44:29<27:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  79%|███████▉  | 7337/9281 [1:44:30<27:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  79%|███████▉  | 7338/9281 [1:44:31<27:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  79%|███████▉  | 7339/9281 [1:44:32<27:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  79%|███████▉  | 7340/9281 [1:44:33<27:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  79%|███████▉  | 7341/9281 [1:44:34<27:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  79%|███████▉  | 7342/9281 [1:44:35<27:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  79%|███████▉  | 7343/9281 [1:44:35<27:28,  1.18it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  79%|███████▉  | 7344/9281 [1:44:36<27:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  79%|███████▉  | 7345/9281 [1:44:37<27:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  79%|███████▉  | 7346/9281 [1:44:38<27:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  79%|███████▉  | 7347/9281 [1:44:39<27:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  79%|███████▉  | 7348/9281 [1:44:40<27:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  79%|███████▉  | 7349/9281 [1:44:41<27:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  79%|███████▉  | 7350/9281 [1:44:41<27:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  79%|███████▉  | 7351/9281 [1:44:42<27:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  79%|███████▉  | 7352/9281 [1:44:43<27:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  79%|███████▉  | 7353/9281 [1:44:44<27:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  79%|███████▉  | 7354/9281 [1:44:45<27:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  79%|███████▉  | 7355/9281 [1:44:46<27:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  79%|███████▉  | 7356/9281 [1:44:46<27:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  79%|███████▉  | 7357/9281 [1:44:47<27:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  79%|███████▉  | 7358/9281 [1:44:48<27:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  79%|███████▉  | 7359/9281 [1:44:49<27:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  79%|███████▉  | 7360/9281 [1:44:50<27:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  79%|███████▉  | 7361/9281 [1:44:51<27:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  79%|███████▉  | 7362/9281 [1:44:52<27:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.10e-01:  79%|███████▉  | 7363/9281 [1:44:52<27:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  79%|███████▉  | 7364/9281 [1:44:53<27:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  79%|███████▉  | 7365/9281 [1:44:54<27:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  79%|███████▉  | 7366/9281 [1:44:55<27:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  79%|███████▉  | 7367/9281 [1:44:56<27:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  79%|███████▉  | 7368/9281 [1:44:57<27:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  79%|███████▉  | 7369/9281 [1:44:58<27:21,  1.16it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  79%|███████▉  | 7370/9281 [1:44:58<27:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  79%|███████▉  | 7371/9281 [1:44:59<27:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  79%|███████▉  | 7372/9281 [1:45:00<27:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  79%|███████▉  | 7373/9281 [1:45:01<27:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.08e-01:  79%|███████▉  | 7374/9281 [1:45:02<27:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  79%|███████▉  | 7375/9281 [1:45:03<27:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  79%|███████▉  | 7376/9281 [1:45:04<27:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  79%|███████▉  | 7377/9281 [1:45:04<27:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  79%|███████▉  | 7378/9281 [1:45:05<27:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  80%|███████▉  | 7379/9281 [1:45:06<27:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  80%|███████▉  | 7380/9281 [1:45:07<27:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  80%|███████▉  | 7381/9281 [1:45:08<27:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  80%|███████▉  | 7382/9281 [1:45:09<27:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  80%|███████▉  | 7383/9281 [1:45:10<26:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  80%|███████▉  | 7384/9281 [1:45:10<26:53,  1.18it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  80%|███████▉  | 7385/9281 [1:45:11<26:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  80%|███████▉  | 7386/9281 [1:45:12<26:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  80%|███████▉  | 7387/9281 [1:45:13<26:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  80%|███████▉  | 7388/9281 [1:45:14<26:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  80%|███████▉  | 7389/9281 [1:45:15<26:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  80%|███████▉  | 7390/9281 [1:45:16<26:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  80%|███████▉  | 7391/9281 [1:45:16<26:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  80%|███████▉  | 7392/9281 [1:45:17<26:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  80%|███████▉  | 7393/9281 [1:45:18<26:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  80%|███████▉  | 7394/9281 [1:45:19<26:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  80%|███████▉  | 7395/9281 [1:45:20<26:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  80%|███████▉  | 7396/9281 [1:45:21<26:43,  1.18it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  80%|███████▉  | 7397/9281 [1:45:22<26:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  80%|███████▉  | 7398/9281 [1:45:22<26:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  80%|███████▉  | 7399/9281 [1:45:23<26:41,  1.18it/s]\u001b[A\n",
      "Training loss: 3.02e-01:  80%|███████▉  | 7400/9281 [1:45:24<26:40,  1.18it/s]\u001b[A\n",
      "Training loss: 3.54e-01:  80%|███████▉  | 7401/9281 [1:45:25<26:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  80%|███████▉  | 7402/9281 [1:45:26<26:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  80%|███████▉  | 7403/9281 [1:45:27<26:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  80%|███████▉  | 7404/9281 [1:45:27<26:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  80%|███████▉  | 7405/9281 [1:45:28<26:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  80%|███████▉  | 7406/9281 [1:45:29<26:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  80%|███████▉  | 7407/9281 [1:45:30<26:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  80%|███████▉  | 7408/9281 [1:45:31<26:32,  1.18it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  80%|███████▉  | 7409/9281 [1:45:32<26:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.08e-01:  80%|███████▉  | 7410/9281 [1:45:33<26:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  80%|███████▉  | 7411/9281 [1:45:33<26:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  80%|███████▉  | 7412/9281 [1:45:34<26:29,  1.18it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  80%|███████▉  | 7413/9281 [1:45:35<26:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  80%|███████▉  | 7414/9281 [1:45:36<26:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  80%|███████▉  | 7415/9281 [1:45:37<26:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  80%|███████▉  | 7416/9281 [1:45:38<26:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  80%|███████▉  | 7417/9281 [1:45:39<26:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  80%|███████▉  | 7418/9281 [1:45:39<26:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  80%|███████▉  | 7419/9281 [1:45:40<26:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  80%|███████▉  | 7420/9281 [1:45:41<26:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  80%|███████▉  | 7421/9281 [1:45:42<26:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  80%|███████▉  | 7422/9281 [1:45:43<26:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  80%|███████▉  | 7423/9281 [1:45:44<26:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  80%|███████▉  | 7424/9281 [1:45:45<26:35,  1.16it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  80%|████████  | 7425/9281 [1:45:45<26:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  80%|████████  | 7426/9281 [1:45:46<26:35,  1.16it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  80%|████████  | 7427/9281 [1:45:47<26:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  80%|████████  | 7428/9281 [1:45:48<26:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  80%|████████  | 7429/9281 [1:45:49<26:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  80%|████████  | 7430/9281 [1:45:50<26:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  80%|████████  | 7431/9281 [1:45:51<26:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  80%|████████  | 7432/9281 [1:45:51<26:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  80%|████████  | 7433/9281 [1:45:52<26:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  80%|████████  | 7434/9281 [1:45:53<26:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  80%|████████  | 7435/9281 [1:45:54<26:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  80%|████████  | 7436/9281 [1:45:55<26:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.16e-01:  80%|████████  | 7437/9281 [1:45:56<26:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  80%|████████  | 7438/9281 [1:45:57<26:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  80%|████████  | 7439/9281 [1:45:57<26:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  80%|████████  | 7440/9281 [1:45:58<26:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  80%|████████  | 7441/9281 [1:45:59<26:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  80%|████████  | 7442/9281 [1:46:00<26:25,  1.16it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  80%|████████  | 7443/9281 [1:46:01<26:23,  1.16it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  80%|████████  | 7444/9281 [1:46:02<26:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.05e-01:  80%|████████  | 7445/9281 [1:46:03<26:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.00e-01:  80%|████████  | 7446/9281 [1:46:03<26:12,  1.17it/s]\u001b[A\n",
      "Training loss: 2.85e-01:  80%|████████  | 7447/9281 [1:46:04<26:14,  1.16it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  80%|████████  | 7448/9281 [1:46:05<26:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  80%|████████  | 7449/9281 [1:46:06<26:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  80%|████████  | 7450/9281 [1:46:07<26:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  80%|████████  | 7451/9281 [1:46:08<26:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  80%|████████  | 7452/9281 [1:46:09<26:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  80%|████████  | 7453/9281 [1:46:09<26:09,  1.16it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  80%|████████  | 7454/9281 [1:46:10<26:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  80%|████████  | 7455/9281 [1:46:11<26:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  80%|████████  | 7456/9281 [1:46:12<25:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  80%|████████  | 7457/9281 [1:46:13<25:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.34e-01:  80%|████████  | 7458/9281 [1:46:14<26:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  80%|████████  | 7459/9281 [1:46:15<25:53,  1.17it/s]\u001b[A\n",
      "Training loss: 2.81e-01:  80%|████████  | 7460/9281 [1:46:15<25:54,  1.17it/s]\u001b[A\n",
      "Training loss: 2.48e-01:  80%|████████  | 7461/9281 [1:46:16<25:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  80%|████████  | 7462/9281 [1:46:17<25:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  80%|████████  | 7463/9281 [1:46:18<25:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.18e-01:  80%|████████  | 7464/9281 [1:46:19<25:52,  1.17it/s]\u001b[A\n",
      "Training loss: 2.92e-01:  80%|████████  | 7465/9281 [1:46:20<25:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  80%|████████  | 7466/9281 [1:46:21<25:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.24e-01:  80%|████████  | 7467/9281 [1:46:21<25:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  80%|████████  | 7468/9281 [1:46:22<25:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.06e-01:  80%|████████  | 7469/9281 [1:46:23<25:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  80%|████████  | 7470/9281 [1:46:24<25:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  80%|████████  | 7471/9281 [1:46:25<25:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.34e-01:  81%|████████  | 7472/9281 [1:46:26<25:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  81%|████████  | 7473/9281 [1:46:26<25:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  81%|████████  | 7474/9281 [1:46:27<25:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  81%|████████  | 7475/9281 [1:46:28<25:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  81%|████████  | 7476/9281 [1:46:29<25:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  81%|████████  | 7477/9281 [1:46:30<25:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.27e-01:  81%|████████  | 7478/9281 [1:46:31<25:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  81%|████████  | 7479/9281 [1:46:32<25:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  81%|████████  | 7480/9281 [1:46:32<25:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  81%|████████  | 7481/9281 [1:46:33<25:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  81%|████████  | 7482/9281 [1:46:34<25:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.54e-01:  81%|████████  | 7483/9281 [1:46:35<25:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  81%|████████  | 7484/9281 [1:46:36<25:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.30e-01:  81%|████████  | 7485/9281 [1:46:37<25:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  81%|████████  | 7486/9281 [1:46:38<25:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  81%|████████  | 7487/9281 [1:46:38<25:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  81%|████████  | 7488/9281 [1:46:39<25:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.30e-01:  81%|████████  | 7489/9281 [1:46:40<25:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  81%|████████  | 7490/9281 [1:46:41<25:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  81%|████████  | 7491/9281 [1:46:42<25:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  81%|████████  | 7492/9281 [1:46:43<25:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  81%|████████  | 7493/9281 [1:46:44<25:34,  1.16it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  81%|████████  | 7494/9281 [1:46:44<25:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  81%|████████  | 7495/9281 [1:46:45<25:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  81%|████████  | 7496/9281 [1:46:46<25:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  81%|████████  | 7497/9281 [1:46:47<25:17,  1.18it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  81%|████████  | 7498/9281 [1:46:48<25:17,  1.18it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  81%|████████  | 7499/9281 [1:46:49<25:13,  1.18it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  81%|████████  | 7500/9281 [1:46:50<25:15,  1.18it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  81%|████████  | 7501/9281 [1:46:50<25:13,  1.18it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  81%|████████  | 7502/9281 [1:46:51<25:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  81%|████████  | 7503/9281 [1:46:52<25:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  81%|████████  | 7504/9281 [1:46:53<25:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  81%|████████  | 7505/9281 [1:46:54<25:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  81%|████████  | 7506/9281 [1:46:55<25:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  81%|████████  | 7507/9281 [1:46:56<25:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  81%|████████  | 7508/9281 [1:46:56<25:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  81%|████████  | 7509/9281 [1:46:57<25:06,  1.18it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  81%|████████  | 7510/9281 [1:46:58<25:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  81%|████████  | 7511/9281 [1:46:59<25:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  81%|████████  | 7512/9281 [1:47:00<25:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  81%|████████  | 7513/9281 [1:47:01<25:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  81%|████████  | 7514/9281 [1:47:02<25:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  81%|████████  | 7515/9281 [1:47:02<25:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.81e-01:  81%|████████  | 7516/9281 [1:47:03<25:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.97e-01:  81%|████████  | 7517/9281 [1:47:04<25:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.82e-01:  81%|████████  | 7518/9281 [1:47:05<25:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  81%|████████  | 7519/9281 [1:47:06<25:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  81%|████████  | 7520/9281 [1:47:07<25:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  81%|████████  | 7521/9281 [1:47:08<25:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  81%|████████  | 7522/9281 [1:47:08<25:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  81%|████████  | 7523/9281 [1:47:09<25:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  81%|████████  | 7524/9281 [1:47:10<25:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.37e-01:  81%|████████  | 7525/9281 [1:47:11<25:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  81%|████████  | 7526/9281 [1:47:12<25:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  81%|████████  | 7527/9281 [1:47:13<24:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  81%|████████  | 7528/9281 [1:47:14<24:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  81%|████████  | 7529/9281 [1:47:14<24:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  81%|████████  | 7530/9281 [1:47:15<24:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  81%|████████  | 7531/9281 [1:47:16<24:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  81%|████████  | 7532/9281 [1:47:17<24:50,  1.17it/s]\u001b[A\n",
      "Training loss: 5.09e-01:  81%|████████  | 7533/9281 [1:47:18<24:52,  1.17it/s]\u001b[A\n",
      "Training loss: 5.24e-01:  81%|████████  | 7534/9281 [1:47:19<24:49,  1.17it/s]\u001b[A\n",
      "Training loss: 5.34e-01:  81%|████████  | 7535/9281 [1:47:19<24:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  81%|████████  | 7536/9281 [1:47:20<24:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  81%|████████  | 7537/9281 [1:47:21<24:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  81%|████████  | 7538/9281 [1:47:22<24:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  81%|████████  | 7539/9281 [1:47:23<24:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  81%|████████  | 7540/9281 [1:47:24<24:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.14e-01:  81%|████████▏ | 7541/9281 [1:47:25<24:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.15e-01:  81%|████████▏ | 7542/9281 [1:47:25<24:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  81%|████████▏ | 7543/9281 [1:47:26<24:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  81%|████████▏ | 7544/9281 [1:47:27<24:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  81%|████████▏ | 7545/9281 [1:47:28<24:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  81%|████████▏ | 7546/9281 [1:47:29<24:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  81%|████████▏ | 7547/9281 [1:47:30<24:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  81%|████████▏ | 7548/9281 [1:47:31<24:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  81%|████████▏ | 7549/9281 [1:47:31<24:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.18e-01:  81%|████████▏ | 7550/9281 [1:47:32<24:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  81%|████████▏ | 7551/9281 [1:47:33<24:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  81%|████████▏ | 7552/9281 [1:47:34<24:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  81%|████████▏ | 7553/9281 [1:47:35<24:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  81%|████████▏ | 7554/9281 [1:47:36<24:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  81%|████████▏ | 7555/9281 [1:47:37<24:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  81%|████████▏ | 7556/9281 [1:47:37<24:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  81%|████████▏ | 7557/9281 [1:47:38<24:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  81%|████████▏ | 7558/9281 [1:47:39<24:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  81%|████████▏ | 7559/9281 [1:47:40<24:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.27e-01:  81%|████████▏ | 7560/9281 [1:47:41<24:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  81%|████████▏ | 7561/9281 [1:47:42<24:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  81%|████████▏ | 7562/9281 [1:47:43<24:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  81%|████████▏ | 7563/9281 [1:47:43<24:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  81%|████████▏ | 7564/9281 [1:47:44<24:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  82%|████████▏ | 7565/9281 [1:47:45<24:33,  1.16it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  82%|████████▏ | 7566/9281 [1:47:46<24:35,  1.16it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  82%|████████▏ | 7567/9281 [1:47:47<24:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  82%|████████▏ | 7568/9281 [1:47:48<24:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  82%|████████▏ | 7569/9281 [1:47:49<24:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  82%|████████▏ | 7570/9281 [1:47:49<24:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  82%|████████▏ | 7571/9281 [1:47:50<24:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  82%|████████▏ | 7572/9281 [1:47:51<24:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  82%|████████▏ | 7573/9281 [1:47:52<24:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  82%|████████▏ | 7574/9281 [1:47:53<24:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  82%|████████▏ | 7575/9281 [1:47:54<24:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  82%|████████▏ | 7576/9281 [1:47:55<24:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  82%|████████▏ | 7577/9281 [1:47:55<24:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.54e-01:  82%|████████▏ | 7578/9281 [1:47:56<24:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  82%|████████▏ | 7579/9281 [1:47:57<24:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  82%|████████▏ | 7580/9281 [1:47:58<24:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.19e-01:  82%|████████▏ | 7581/9281 [1:47:59<24:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  82%|████████▏ | 7582/9281 [1:48:00<24:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  82%|████████▏ | 7583/9281 [1:48:01<24:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  82%|████████▏ | 7584/9281 [1:48:01<24:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  82%|████████▏ | 7585/9281 [1:48:02<24:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  82%|████████▏ | 7586/9281 [1:48:03<24:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  82%|████████▏ | 7587/9281 [1:48:04<24:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  82%|████████▏ | 7588/9281 [1:48:05<24:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  82%|████████▏ | 7589/9281 [1:48:06<24:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  82%|████████▏ | 7590/9281 [1:48:06<24:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.27e-01:  82%|████████▏ | 7591/9281 [1:48:07<24:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.34e-01:  82%|████████▏ | 7592/9281 [1:48:08<24:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  82%|████████▏ | 7593/9281 [1:48:09<24:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  82%|████████▏ | 7594/9281 [1:48:10<24:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  82%|████████▏ | 7595/9281 [1:48:11<23:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.87e-01:  82%|████████▏ | 7596/9281 [1:48:12<24:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  82%|████████▏ | 7597/9281 [1:48:12<23:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  82%|████████▏ | 7598/9281 [1:48:13<23:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  82%|████████▏ | 7599/9281 [1:48:14<23:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  82%|████████▏ | 7600/9281 [1:48:15<23:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  82%|████████▏ | 7601/9281 [1:48:16<23:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  82%|████████▏ | 7602/9281 [1:48:17<23:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  82%|████████▏ | 7603/9281 [1:48:18<23:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  82%|████████▏ | 7604/9281 [1:48:18<23:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  82%|████████▏ | 7605/9281 [1:48:19<23:53,  1.17it/s]\u001b[A\n",
      "Training loss: 5.26e-01:  82%|████████▏ | 7606/9281 [1:48:20<23:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  82%|████████▏ | 7607/9281 [1:48:21<23:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  82%|████████▏ | 7608/9281 [1:48:22<23:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  82%|████████▏ | 7609/9281 [1:48:23<23:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  82%|████████▏ | 7610/9281 [1:48:24<23:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  82%|████████▏ | 7611/9281 [1:48:24<23:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  82%|████████▏ | 7612/9281 [1:48:25<23:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  82%|████████▏ | 7613/9281 [1:48:26<23:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  82%|████████▏ | 7614/9281 [1:48:27<23:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  82%|████████▏ | 7615/9281 [1:48:28<23:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  82%|████████▏ | 7616/9281 [1:48:29<23:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.23e-01:  82%|████████▏ | 7617/9281 [1:48:30<23:49,  1.16it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  82%|████████▏ | 7618/9281 [1:48:30<23:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  82%|████████▏ | 7619/9281 [1:48:31<23:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  82%|████████▏ | 7620/9281 [1:48:32<23:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  82%|████████▏ | 7621/9281 [1:48:33<23:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  82%|████████▏ | 7622/9281 [1:48:34<23:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  82%|████████▏ | 7623/9281 [1:48:35<23:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  82%|████████▏ | 7624/9281 [1:48:36<23:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  82%|████████▏ | 7625/9281 [1:48:36<23:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  82%|████████▏ | 7626/9281 [1:48:37<23:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  82%|████████▏ | 7627/9281 [1:48:38<23:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  82%|████████▏ | 7628/9281 [1:48:39<23:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.05e-01:  82%|████████▏ | 7629/9281 [1:48:40<23:26,  1.17it/s]\u001b[A\n",
      "Training loss: 2.70e-01:  82%|████████▏ | 7630/9281 [1:48:41<23:22,  1.18it/s]\u001b[A\n",
      "Training loss: 2.81e-01:  82%|████████▏ | 7631/9281 [1:48:42<23:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.07e-01:  82%|████████▏ | 7632/9281 [1:48:42<23:21,  1.18it/s]\u001b[A\n",
      "Training loss: 3.19e-01:  82%|████████▏ | 7633/9281 [1:48:43<23:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  82%|████████▏ | 7634/9281 [1:48:44<23:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  82%|████████▏ | 7635/9281 [1:48:45<23:19,  1.18it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  82%|████████▏ | 7636/9281 [1:48:46<23:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.39e-01:  82%|████████▏ | 7637/9281 [1:48:47<23:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  82%|████████▏ | 7638/9281 [1:48:48<23:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  82%|████████▏ | 7639/9281 [1:48:48<23:22,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  82%|████████▏ | 7640/9281 [1:48:49<23:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  82%|████████▏ | 7641/9281 [1:48:50<23:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  82%|████████▏ | 7642/9281 [1:48:51<23:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  82%|████████▏ | 7643/9281 [1:48:52<23:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  82%|████████▏ | 7644/9281 [1:48:53<23:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  82%|████████▏ | 7645/9281 [1:48:53<23:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  82%|████████▏ | 7646/9281 [1:48:54<23:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  82%|████████▏ | 7647/9281 [1:48:55<23:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  82%|████████▏ | 7648/9281 [1:48:56<23:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  82%|████████▏ | 7649/9281 [1:48:57<23:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  82%|████████▏ | 7650/9281 [1:48:58<23:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  82%|████████▏ | 7651/9281 [1:48:59<23:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  82%|████████▏ | 7652/9281 [1:48:59<23:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  82%|████████▏ | 7653/9281 [1:49:00<23:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  82%|████████▏ | 7654/9281 [1:49:01<23:03,  1.18it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  82%|████████▏ | 7655/9281 [1:49:02<23:02,  1.18it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  82%|████████▏ | 7656/9281 [1:49:03<23:01,  1.18it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  83%|████████▎ | 7657/9281 [1:49:04<22:59,  1.18it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  83%|████████▎ | 7658/9281 [1:49:05<22:58,  1.18it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  83%|████████▎ | 7659/9281 [1:49:05<23:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  83%|████████▎ | 7660/9281 [1:49:06<23:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  83%|████████▎ | 7661/9281 [1:49:07<23:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  83%|████████▎ | 7662/9281 [1:49:08<23:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.14e-01:  83%|████████▎ | 7663/9281 [1:49:09<23:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  83%|████████▎ | 7664/9281 [1:49:10<23:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  83%|████████▎ | 7665/9281 [1:49:11<23:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  83%|████████▎ | 7666/9281 [1:49:11<23:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  83%|████████▎ | 7667/9281 [1:49:12<23:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  83%|████████▎ | 7668/9281 [1:49:13<23:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  83%|████████▎ | 7669/9281 [1:49:14<22:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  83%|████████▎ | 7670/9281 [1:49:15<22:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  83%|████████▎ | 7671/9281 [1:49:16<22:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  83%|████████▎ | 7672/9281 [1:49:17<22:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  83%|████████▎ | 7673/9281 [1:49:17<22:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  83%|████████▎ | 7674/9281 [1:49:18<22:46,  1.18it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  83%|████████▎ | 7675/9281 [1:49:19<22:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  83%|████████▎ | 7676/9281 [1:49:20<22:45,  1.18it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  83%|████████▎ | 7677/9281 [1:49:21<22:45,  1.18it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  83%|████████▎ | 7678/9281 [1:49:22<22:42,  1.18it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  83%|████████▎ | 7679/9281 [1:49:22<22:40,  1.18it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  83%|████████▎ | 7680/9281 [1:49:23<22:41,  1.18it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  83%|████████▎ | 7681/9281 [1:49:24<22:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  83%|████████▎ | 7682/9281 [1:49:25<22:44,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  83%|████████▎ | 7683/9281 [1:49:26<22:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  83%|████████▎ | 7684/9281 [1:49:27<22:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  83%|████████▎ | 7685/9281 [1:49:28<22:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  83%|████████▎ | 7686/9281 [1:49:28<22:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  83%|████████▎ | 7687/9281 [1:49:29<22:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  83%|████████▎ | 7688/9281 [1:49:30<22:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  83%|████████▎ | 7689/9281 [1:49:31<22:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  83%|████████▎ | 7690/9281 [1:49:32<22:46,  1.16it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  83%|████████▎ | 7691/9281 [1:49:33<22:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  83%|████████▎ | 7692/9281 [1:49:34<22:45,  1.16it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  83%|████████▎ | 7693/9281 [1:49:34<22:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  83%|████████▎ | 7694/9281 [1:49:35<22:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  83%|████████▎ | 7695/9281 [1:49:36<22:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  83%|████████▎ | 7696/9281 [1:49:37<22:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  83%|████████▎ | 7697/9281 [1:49:38<22:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  83%|████████▎ | 7698/9281 [1:49:39<22:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  83%|████████▎ | 7699/9281 [1:49:40<22:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  83%|████████▎ | 7700/9281 [1:49:40<22:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  83%|████████▎ | 7701/9281 [1:49:41<22:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  83%|████████▎ | 7702/9281 [1:49:42<22:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  83%|████████▎ | 7703/9281 [1:49:43<22:35,  1.16it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  83%|████████▎ | 7704/9281 [1:49:44<22:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  83%|████████▎ | 7705/9281 [1:49:45<22:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  83%|████████▎ | 7706/9281 [1:49:46<22:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  83%|████████▎ | 7707/9281 [1:49:46<22:32,  1.16it/s]\u001b[A\n",
      "Training loss: 3.24e-01:  83%|████████▎ | 7708/9281 [1:49:47<22:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  83%|████████▎ | 7709/9281 [1:49:48<22:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  83%|████████▎ | 7710/9281 [1:49:49<22:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  83%|████████▎ | 7711/9281 [1:49:50<22:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  83%|████████▎ | 7712/9281 [1:49:51<22:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.31e-01:  83%|████████▎ | 7713/9281 [1:49:52<22:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  83%|████████▎ | 7714/9281 [1:49:52<22:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  83%|████████▎ | 7715/9281 [1:49:53<22:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  83%|████████▎ | 7716/9281 [1:49:54<22:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  83%|████████▎ | 7717/9281 [1:49:55<22:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  83%|████████▎ | 7718/9281 [1:49:56<22:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  83%|████████▎ | 7719/9281 [1:49:57<22:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  83%|████████▎ | 7720/9281 [1:49:58<22:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  83%|████████▎ | 7721/9281 [1:49:58<22:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  83%|████████▎ | 7722/9281 [1:49:59<22:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  83%|████████▎ | 7723/9281 [1:50:00<22:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  83%|████████▎ | 7724/9281 [1:50:01<22:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  83%|████████▎ | 7725/9281 [1:50:02<22:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  83%|████████▎ | 7726/9281 [1:50:03<22:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  83%|████████▎ | 7727/9281 [1:50:04<22:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  83%|████████▎ | 7728/9281 [1:50:04<22:13,  1.16it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  83%|████████▎ | 7729/9281 [1:50:05<22:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  83%|████████▎ | 7730/9281 [1:50:06<22:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  83%|████████▎ | 7731/9281 [1:50:07<22:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  83%|████████▎ | 7732/9281 [1:50:08<22:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.85e-01:  83%|████████▎ | 7733/9281 [1:50:09<22:05,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  83%|████████▎ | 7734/9281 [1:50:10<22:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  83%|████████▎ | 7735/9281 [1:50:10<22:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  83%|████████▎ | 7736/9281 [1:50:11<22:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  83%|████████▎ | 7737/9281 [1:50:12<21:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  83%|████████▎ | 7738/9281 [1:50:13<22:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  83%|████████▎ | 7739/9281 [1:50:14<21:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  83%|████████▎ | 7740/9281 [1:50:15<21:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  83%|████████▎ | 7741/9281 [1:50:16<21:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  83%|████████▎ | 7742/9281 [1:50:16<21:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  83%|████████▎ | 7743/9281 [1:50:17<21:48,  1.18it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  83%|████████▎ | 7744/9281 [1:50:18<21:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  83%|████████▎ | 7745/9281 [1:50:19<21:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  83%|████████▎ | 7746/9281 [1:50:20<21:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  83%|████████▎ | 7747/9281 [1:50:21<21:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  83%|████████▎ | 7748/9281 [1:50:22<21:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  83%|████████▎ | 7749/9281 [1:50:22<21:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  84%|████████▎ | 7750/9281 [1:50:23<21:52,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  84%|████████▎ | 7751/9281 [1:50:24<21:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  84%|████████▎ | 7752/9281 [1:50:25<21:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.74e-01:  84%|████████▎ | 7753/9281 [1:50:26<21:39,  1.18it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  84%|████████▎ | 7754/9281 [1:50:27<21:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  84%|████████▎ | 7755/9281 [1:50:28<21:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  84%|████████▎ | 7756/9281 [1:50:28<21:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  84%|████████▎ | 7757/9281 [1:50:29<21:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  84%|████████▎ | 7758/9281 [1:50:30<21:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  84%|████████▎ | 7759/9281 [1:50:31<21:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  84%|████████▎ | 7760/9281 [1:50:32<21:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.02e-01:  84%|████████▎ | 7761/9281 [1:50:33<21:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  84%|████████▎ | 7762/9281 [1:50:34<21:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  84%|████████▎ | 7763/9281 [1:50:34<21:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.10e-01:  84%|████████▎ | 7764/9281 [1:50:35<21:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  84%|████████▎ | 7765/9281 [1:50:36<21:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  84%|████████▎ | 7766/9281 [1:50:37<21:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  84%|████████▎ | 7767/9281 [1:50:38<21:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.89e-01:  84%|████████▎ | 7768/9281 [1:50:39<21:36,  1.17it/s]\u001b[A\n",
      "Training loss: 5.16e-01:  84%|████████▎ | 7769/9281 [1:50:40<21:39,  1.16it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  84%|████████▎ | 7770/9281 [1:50:40<21:37,  1.16it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  84%|████████▎ | 7771/9281 [1:50:41<21:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  84%|████████▎ | 7772/9281 [1:50:42<21:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  84%|████████▍ | 7773/9281 [1:50:43<21:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  84%|████████▍ | 7774/9281 [1:50:44<21:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  84%|████████▍ | 7775/9281 [1:50:45<21:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  84%|████████▍ | 7776/9281 [1:50:45<21:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  84%|████████▍ | 7777/9281 [1:50:46<21:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  84%|████████▍ | 7778/9281 [1:50:47<21:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  84%|████████▍ | 7779/9281 [1:50:48<21:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  84%|████████▍ | 7780/9281 [1:50:49<21:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  84%|████████▍ | 7781/9281 [1:50:50<21:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  84%|████████▍ | 7782/9281 [1:50:51<21:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  84%|████████▍ | 7783/9281 [1:50:51<21:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  84%|████████▍ | 7784/9281 [1:50:52<21:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  84%|████████▍ | 7785/9281 [1:50:53<21:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  84%|████████▍ | 7786/9281 [1:50:54<21:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  84%|████████▍ | 7787/9281 [1:50:55<21:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  84%|████████▍ | 7788/9281 [1:50:56<21:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  84%|████████▍ | 7789/9281 [1:50:57<21:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  84%|████████▍ | 7790/9281 [1:50:57<21:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  84%|████████▍ | 7791/9281 [1:50:58<21:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  84%|████████▍ | 7792/9281 [1:50:59<21:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  84%|████████▍ | 7793/9281 [1:51:00<21:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  84%|████████▍ | 7794/9281 [1:51:01<21:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  84%|████████▍ | 7795/9281 [1:51:02<21:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.79e-01:  84%|████████▍ | 7796/9281 [1:51:03<21:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  84%|████████▍ | 7797/9281 [1:51:03<21:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  84%|████████▍ | 7798/9281 [1:51:04<21:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  84%|████████▍ | 7799/9281 [1:51:05<21:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  84%|████████▍ | 7800/9281 [1:51:06<21:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  84%|████████▍ | 7801/9281 [1:51:07<21:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  84%|████████▍ | 7802/9281 [1:51:08<21:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  84%|████████▍ | 7803/9281 [1:51:09<21:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  84%|████████▍ | 7804/9281 [1:51:09<21:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  84%|████████▍ | 7805/9281 [1:51:10<21:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  84%|████████▍ | 7806/9281 [1:51:11<21:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  84%|████████▍ | 7807/9281 [1:51:12<21:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  84%|████████▍ | 7808/9281 [1:51:13<20:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  84%|████████▍ | 7809/9281 [1:51:14<21:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  84%|████████▍ | 7810/9281 [1:51:15<20:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  84%|████████▍ | 7811/9281 [1:51:15<21:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  84%|████████▍ | 7812/9281 [1:51:16<20:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  84%|████████▍ | 7813/9281 [1:51:17<20:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  84%|████████▍ | 7814/9281 [1:51:18<20:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.29e-01:  84%|████████▍ | 7815/9281 [1:51:19<20:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  84%|████████▍ | 7816/9281 [1:51:20<20:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  84%|████████▍ | 7817/9281 [1:51:21<20:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  84%|████████▍ | 7818/9281 [1:51:21<20:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  84%|████████▍ | 7819/9281 [1:51:22<20:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  84%|████████▍ | 7820/9281 [1:51:23<20:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  84%|████████▍ | 7821/9281 [1:51:24<20:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  84%|████████▍ | 7822/9281 [1:51:25<20:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.34e-01:  84%|████████▍ | 7823/9281 [1:51:26<20:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.09e-01:  84%|████████▍ | 7824/9281 [1:51:27<20:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  84%|████████▍ | 7825/9281 [1:51:27<20:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  84%|████████▍ | 7826/9281 [1:51:28<20:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  84%|████████▍ | 7827/9281 [1:51:29<20:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  84%|████████▍ | 7828/9281 [1:51:30<20:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  84%|████████▍ | 7829/9281 [1:51:31<20:43,  1.17it/s]\u001b[A\n",
      "Training loss: 5.51e-01:  84%|████████▍ | 7830/9281 [1:51:32<20:42,  1.17it/s]\u001b[A\n",
      "Training loss: 5.64e-01:  84%|████████▍ | 7831/9281 [1:51:33<20:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.64e-01:  84%|████████▍ | 7832/9281 [1:51:33<20:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  84%|████████▍ | 7833/9281 [1:51:34<20:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  84%|████████▍ | 7834/9281 [1:51:35<20:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  84%|████████▍ | 7835/9281 [1:51:36<20:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  84%|████████▍ | 7836/9281 [1:51:37<20:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  84%|████████▍ | 7837/9281 [1:51:38<20:31,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  84%|████████▍ | 7838/9281 [1:51:39<20:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.56e-01:  84%|████████▍ | 7839/9281 [1:51:39<20:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  84%|████████▍ | 7840/9281 [1:51:40<20:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  84%|████████▍ | 7841/9281 [1:51:41<20:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  84%|████████▍ | 7842/9281 [1:51:42<20:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  85%|████████▍ | 7843/9281 [1:51:43<20:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  85%|████████▍ | 7844/9281 [1:51:44<20:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  85%|████████▍ | 7845/9281 [1:51:45<20:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  85%|████████▍ | 7846/9281 [1:51:45<20:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  85%|████████▍ | 7847/9281 [1:51:46<20:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.63e-01:  85%|████████▍ | 7848/9281 [1:51:47<20:30,  1.16it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  85%|████████▍ | 7849/9281 [1:51:48<20:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  85%|████████▍ | 7850/9281 [1:51:49<20:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  85%|████████▍ | 7851/9281 [1:51:50<20:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  85%|████████▍ | 7852/9281 [1:51:51<20:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  85%|████████▍ | 7853/9281 [1:51:51<20:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  85%|████████▍ | 7854/9281 [1:51:52<20:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  85%|████████▍ | 7855/9281 [1:51:53<20:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  85%|████████▍ | 7856/9281 [1:51:54<20:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  85%|████████▍ | 7857/9281 [1:51:55<20:18,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  85%|████████▍ | 7858/9281 [1:51:56<20:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  85%|████████▍ | 7859/9281 [1:51:56<20:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  85%|████████▍ | 7860/9281 [1:51:57<20:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  85%|████████▍ | 7861/9281 [1:51:58<20:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  85%|████████▍ | 7862/9281 [1:51:59<20:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  85%|████████▍ | 7863/9281 [1:52:00<20:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  85%|████████▍ | 7864/9281 [1:52:01<20:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  85%|████████▍ | 7865/9281 [1:52:02<20:16,  1.16it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  85%|████████▍ | 7866/9281 [1:52:02<20:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  85%|████████▍ | 7867/9281 [1:52:03<20:16,  1.16it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  85%|████████▍ | 7868/9281 [1:52:04<20:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  85%|████████▍ | 7869/9281 [1:52:05<20:12,  1.16it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  85%|████████▍ | 7870/9281 [1:52:06<20:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  85%|████████▍ | 7871/9281 [1:52:07<20:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  85%|████████▍ | 7872/9281 [1:52:08<20:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  85%|████████▍ | 7873/9281 [1:52:08<20:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  85%|████████▍ | 7874/9281 [1:52:09<20:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  85%|████████▍ | 7875/9281 [1:52:10<20:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.39e-01:  85%|████████▍ | 7876/9281 [1:52:11<20:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  85%|████████▍ | 7877/9281 [1:52:12<20:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  85%|████████▍ | 7878/9281 [1:52:13<20:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  85%|████████▍ | 7879/9281 [1:52:14<19:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  85%|████████▍ | 7880/9281 [1:52:14<19:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  85%|████████▍ | 7881/9281 [1:52:15<19:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  85%|████████▍ | 7882/9281 [1:52:16<19:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  85%|████████▍ | 7883/9281 [1:52:17<19:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  85%|████████▍ | 7884/9281 [1:52:18<19:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  85%|████████▍ | 7885/9281 [1:52:19<19:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  85%|████████▍ | 7886/9281 [1:52:20<19:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  85%|████████▍ | 7887/9281 [1:52:20<19:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  85%|████████▍ | 7888/9281 [1:52:21<19:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  85%|████████▌ | 7889/9281 [1:52:22<19:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  85%|████████▌ | 7890/9281 [1:52:23<19:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  85%|████████▌ | 7891/9281 [1:52:24<19:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  85%|████████▌ | 7892/9281 [1:52:25<19:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  85%|████████▌ | 7893/9281 [1:52:26<19:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  85%|████████▌ | 7894/9281 [1:52:26<19:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  85%|████████▌ | 7895/9281 [1:52:27<19:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  85%|████████▌ | 7896/9281 [1:52:28<19:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  85%|████████▌ | 7897/9281 [1:52:29<19:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  85%|████████▌ | 7898/9281 [1:52:30<19:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.05e-01:  85%|████████▌ | 7899/9281 [1:52:31<19:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  85%|████████▌ | 7900/9281 [1:52:32<19:36,  1.17it/s]\u001b[A\n",
      "Training loss: 2.84e-01:  85%|████████▌ | 7901/9281 [1:52:32<19:33,  1.18it/s]\u001b[A\n",
      "Training loss: 2.75e-01:  85%|████████▌ | 7902/9281 [1:52:33<19:32,  1.18it/s]\u001b[A\n",
      "Training loss: 3.12e-01:  85%|████████▌ | 7903/9281 [1:52:34<19:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  85%|████████▌ | 7904/9281 [1:52:35<19:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  85%|████████▌ | 7905/9281 [1:52:36<19:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.19e-01:  85%|████████▌ | 7906/9281 [1:52:37<19:40,  1.16it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  85%|████████▌ | 7907/9281 [1:52:38<19:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  85%|████████▌ | 7908/9281 [1:52:38<19:40,  1.16it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  85%|████████▌ | 7909/9281 [1:52:39<19:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  85%|████████▌ | 7910/9281 [1:52:40<19:38,  1.16it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  85%|████████▌ | 7911/9281 [1:52:41<19:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.27e-01:  85%|████████▌ | 7912/9281 [1:52:42<19:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  85%|████████▌ | 7913/9281 [1:52:43<19:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  85%|████████▌ | 7914/9281 [1:52:44<19:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  85%|████████▌ | 7915/9281 [1:52:44<19:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  85%|████████▌ | 7916/9281 [1:52:45<19:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  85%|████████▌ | 7917/9281 [1:52:46<19:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  85%|████████▌ | 7918/9281 [1:52:47<19:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  85%|████████▌ | 7919/9281 [1:52:48<19:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  85%|████████▌ | 7920/9281 [1:52:49<19:23,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  85%|████████▌ | 7921/9281 [1:52:49<19:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  85%|████████▌ | 7922/9281 [1:52:50<19:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  85%|████████▌ | 7923/9281 [1:52:51<19:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  85%|████████▌ | 7924/9281 [1:52:52<19:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  85%|████████▌ | 7925/9281 [1:52:53<19:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  85%|████████▌ | 7926/9281 [1:52:54<19:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  85%|████████▌ | 7927/9281 [1:52:55<19:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  85%|████████▌ | 7928/9281 [1:52:55<19:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.44e-01:  85%|████████▌ | 7929/9281 [1:52:56<19:20,  1.16it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  85%|████████▌ | 7930/9281 [1:52:57<19:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  85%|████████▌ | 7931/9281 [1:52:58<19:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  85%|████████▌ | 7932/9281 [1:52:59<19:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  85%|████████▌ | 7933/9281 [1:53:00<19:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  85%|████████▌ | 7934/9281 [1:53:01<19:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  85%|████████▌ | 7935/9281 [1:53:01<19:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  86%|████████▌ | 7936/9281 [1:53:02<19:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  86%|████████▌ | 7937/9281 [1:53:03<19:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  86%|████████▌ | 7938/9281 [1:53:04<19:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  86%|████████▌ | 7939/9281 [1:53:05<19:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  86%|████████▌ | 7940/9281 [1:53:06<19:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  86%|████████▌ | 7941/9281 [1:53:07<19:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  86%|████████▌ | 7942/9281 [1:53:07<18:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  86%|████████▌ | 7943/9281 [1:53:08<19:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  86%|████████▌ | 7944/9281 [1:53:09<19:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  86%|████████▌ | 7945/9281 [1:53:10<19:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  86%|████████▌ | 7946/9281 [1:53:11<19:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.23e-01:  86%|████████▌ | 7947/9281 [1:53:12<19:06,  1.16it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  86%|████████▌ | 7948/9281 [1:53:13<19:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  86%|████████▌ | 7949/9281 [1:53:13<19:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.24e-01:  86%|████████▌ | 7950/9281 [1:53:14<18:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.18e-01:  86%|████████▌ | 7951/9281 [1:53:15<18:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  86%|████████▌ | 7952/9281 [1:53:16<18:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  86%|████████▌ | 7953/9281 [1:53:17<18:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  86%|████████▌ | 7954/9281 [1:53:18<18:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  86%|████████▌ | 7955/9281 [1:53:19<18:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  86%|████████▌ | 7956/9281 [1:53:19<18:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  86%|████████▌ | 7957/9281 [1:53:20<18:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  86%|████████▌ | 7958/9281 [1:53:21<18:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  86%|████████▌ | 7959/9281 [1:53:22<18:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  86%|████████▌ | 7960/9281 [1:53:23<18:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  86%|████████▌ | 7961/9281 [1:53:24<18:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  86%|████████▌ | 7962/9281 [1:53:25<18:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  86%|████████▌ | 7963/9281 [1:53:25<18:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  86%|████████▌ | 7964/9281 [1:53:26<18:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  86%|████████▌ | 7965/9281 [1:53:27<18:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  86%|████████▌ | 7966/9281 [1:53:28<18:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  86%|████████▌ | 7967/9281 [1:53:29<18:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  86%|████████▌ | 7968/9281 [1:53:30<18:48,  1.16it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  86%|████████▌ | 7969/9281 [1:53:31<18:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  86%|████████▌ | 7970/9281 [1:53:31<18:48,  1.16it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  86%|████████▌ | 7971/9281 [1:53:32<18:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  86%|████████▌ | 7972/9281 [1:53:33<18:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  86%|████████▌ | 7973/9281 [1:53:34<18:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.30e-01:  86%|████████▌ | 7974/9281 [1:53:35<18:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  86%|████████▌ | 7975/9281 [1:53:36<18:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  86%|████████▌ | 7976/9281 [1:53:37<18:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  86%|████████▌ | 7977/9281 [1:53:37<18:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  86%|████████▌ | 7978/9281 [1:53:38<18:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.10e-01:  86%|████████▌ | 7979/9281 [1:53:39<18:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  86%|████████▌ | 7980/9281 [1:53:40<18:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.04e-01:  86%|████████▌ | 7981/9281 [1:53:41<18:25,  1.18it/s]\u001b[A\n",
      "Training loss: 2.90e-01:  86%|████████▌ | 7982/9281 [1:53:42<18:24,  1.18it/s]\u001b[A\n",
      "Training loss: 3.07e-01:  86%|████████▌ | 7983/9281 [1:53:42<18:24,  1.18it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  86%|████████▌ | 7984/9281 [1:53:43<18:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  86%|████████▌ | 7985/9281 [1:53:44<18:22,  1.18it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  86%|████████▌ | 7986/9281 [1:53:45<18:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  86%|████████▌ | 7987/9281 [1:53:46<18:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  86%|████████▌ | 7988/9281 [1:53:47<18:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  86%|████████▌ | 7989/9281 [1:53:48<18:22,  1.17it/s]\u001b[A\n",
      "Training loss: 2.89e-01:  86%|████████▌ | 7990/9281 [1:53:48<18:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.00e-01:  86%|████████▌ | 7991/9281 [1:53:49<18:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.15e-01:  86%|████████▌ | 7992/9281 [1:53:50<18:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  86%|████████▌ | 7993/9281 [1:53:51<18:15,  1.18it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  86%|████████▌ | 7994/9281 [1:53:52<18:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  86%|████████▌ | 7995/9281 [1:53:53<18:13,  1.18it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  86%|████████▌ | 7996/9281 [1:53:54<18:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  86%|████████▌ | 7997/9281 [1:53:54<18:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  86%|████████▌ | 7998/9281 [1:53:55<18:14,  1.17it/s]\u001b[A\n",
      "Training loss: 2.87e-01:  86%|████████▌ | 7999/9281 [1:53:56<18:14,  1.17it/s]\u001b[A\n",
      "Training loss: 2.85e-01:  86%|████████▌ | 8000/9281 [1:53:57<18:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  86%|████████▌ | 8001/9281 [1:53:58<18:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  86%|████████▌ | 8002/9281 [1:53:59<18:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  86%|████████▌ | 8003/9281 [1:54:00<18:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  86%|████████▌ | 8004/9281 [1:54:00<18:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  86%|████████▋ | 8005/9281 [1:54:01<18:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.06e-01:  86%|████████▋ | 8006/9281 [1:54:02<18:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  86%|████████▋ | 8007/9281 [1:54:03<18:05,  1.17it/s]\u001b[A\n",
      "Training loss: 2.92e-01:  86%|████████▋ | 8008/9281 [1:54:04<18:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.21e-01:  86%|████████▋ | 8009/9281 [1:54:05<18:04,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  86%|████████▋ | 8010/9281 [1:54:06<18:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  86%|████████▋ | 8011/9281 [1:54:06<18:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  86%|████████▋ | 8012/9281 [1:54:07<18:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  86%|████████▋ | 8013/9281 [1:54:08<18:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  86%|████████▋ | 8014/9281 [1:54:09<18:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  86%|████████▋ | 8015/9281 [1:54:10<18:02,  1.17it/s]\u001b[A\n",
      "Training loss: 5.12e-01:  86%|████████▋ | 8016/9281 [1:54:11<18:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  86%|████████▋ | 8017/9281 [1:54:12<17:55,  1.18it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  86%|████████▋ | 8018/9281 [1:54:12<17:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  86%|████████▋ | 8019/9281 [1:54:13<17:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  86%|████████▋ | 8020/9281 [1:54:14<17:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  86%|████████▋ | 8021/9281 [1:54:15<17:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.59e-01:  86%|████████▋ | 8022/9281 [1:54:16<17:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  86%|████████▋ | 8023/9281 [1:54:17<17:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  86%|████████▋ | 8024/9281 [1:54:18<17:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  86%|████████▋ | 8025/9281 [1:54:18<17:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  86%|████████▋ | 8026/9281 [1:54:19<17:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  86%|████████▋ | 8027/9281 [1:54:20<17:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  86%|████████▋ | 8028/9281 [1:54:21<17:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  87%|████████▋ | 8029/9281 [1:54:22<17:57,  1.16it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  87%|████████▋ | 8030/9281 [1:54:23<17:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  87%|████████▋ | 8031/9281 [1:54:24<17:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  87%|████████▋ | 8032/9281 [1:54:24<17:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.38e-01:  87%|████████▋ | 8033/9281 [1:54:25<17:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  87%|████████▋ | 8034/9281 [1:54:26<17:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  87%|████████▋ | 8035/9281 [1:54:27<17:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  87%|████████▋ | 8036/9281 [1:54:28<17:39,  1.18it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  87%|████████▋ | 8037/9281 [1:54:29<17:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  87%|████████▋ | 8038/9281 [1:54:29<17:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  87%|████████▋ | 8039/9281 [1:54:30<17:40,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  87%|████████▋ | 8040/9281 [1:54:31<17:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  87%|████████▋ | 8041/9281 [1:54:32<17:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  87%|████████▋ | 8042/9281 [1:54:33<17:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  87%|████████▋ | 8043/9281 [1:54:34<17:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  87%|████████▋ | 8044/9281 [1:54:35<17:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  87%|████████▋ | 8045/9281 [1:54:35<17:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  87%|████████▋ | 8046/9281 [1:54:36<17:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  87%|████████▋ | 8047/9281 [1:54:37<17:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.05e-01:  87%|████████▋ | 8048/9281 [1:54:38<17:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.09e-01:  87%|████████▋ | 8049/9281 [1:54:39<17:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.23e-01:  87%|████████▋ | 8050/9281 [1:54:40<17:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  87%|████████▋ | 8051/9281 [1:54:41<17:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  87%|████████▋ | 8052/9281 [1:54:41<17:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  87%|████████▋ | 8053/9281 [1:54:42<17:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  87%|████████▋ | 8054/9281 [1:54:43<17:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.04e-01:  87%|████████▋ | 8055/9281 [1:54:44<17:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.07e-01:  87%|████████▋ | 8056/9281 [1:54:45<17:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  87%|████████▋ | 8057/9281 [1:54:46<17:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  87%|████████▋ | 8058/9281 [1:54:47<17:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  87%|████████▋ | 8059/9281 [1:54:47<17:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  87%|████████▋ | 8060/9281 [1:54:48<17:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  87%|████████▋ | 8061/9281 [1:54:49<17:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  87%|████████▋ | 8062/9281 [1:54:50<17:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  87%|████████▋ | 8063/9281 [1:54:51<17:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  87%|████████▋ | 8064/9281 [1:54:52<17:26,  1.16it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  87%|████████▋ | 8065/9281 [1:54:53<17:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  87%|████████▋ | 8066/9281 [1:54:53<17:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  87%|████████▋ | 8067/9281 [1:54:54<17:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.48e-01:  87%|████████▋ | 8068/9281 [1:54:55<17:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  87%|████████▋ | 8069/9281 [1:54:56<17:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  87%|████████▋ | 8070/9281 [1:54:57<17:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.19e-01:  87%|████████▋ | 8071/9281 [1:54:58<17:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  87%|████████▋ | 8072/9281 [1:54:59<17:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  87%|████████▋ | 8073/9281 [1:54:59<17:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  87%|████████▋ | 8074/9281 [1:55:00<17:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  87%|████████▋ | 8075/9281 [1:55:01<17:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  87%|████████▋ | 8076/9281 [1:55:02<17:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  87%|████████▋ | 8077/9281 [1:55:03<17:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  87%|████████▋ | 8078/9281 [1:55:04<17:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  87%|████████▋ | 8079/9281 [1:55:05<17:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  87%|████████▋ | 8080/9281 [1:55:05<17:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  87%|████████▋ | 8081/9281 [1:55:06<17:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.29e-01:  87%|████████▋ | 8082/9281 [1:55:07<17:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  87%|████████▋ | 8083/9281 [1:55:08<17:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  87%|████████▋ | 8084/9281 [1:55:09<17:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  87%|████████▋ | 8085/9281 [1:55:10<17:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  87%|████████▋ | 8086/9281 [1:55:11<17:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  87%|████████▋ | 8087/9281 [1:55:11<17:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  87%|████████▋ | 8088/9281 [1:55:12<17:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.16e-01:  87%|████████▋ | 8089/9281 [1:55:13<17:05,  1.16it/s]\u001b[A\n",
      "Training loss: 2.81e-01:  87%|████████▋ | 8090/9281 [1:55:14<16:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.04e-01:  87%|████████▋ | 8091/9281 [1:55:15<17:00,  1.17it/s]\u001b[A\n",
      "Training loss: 2.97e-01:  87%|████████▋ | 8092/9281 [1:55:16<16:59,  1.17it/s]\u001b[A\n",
      "Training loss: 2.89e-01:  87%|████████▋ | 8093/9281 [1:55:17<16:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.34e-01:  87%|████████▋ | 8094/9281 [1:55:17<16:55,  1.17it/s]\u001b[A\n",
      "Training loss: 2.77e-01:  87%|████████▋ | 8095/9281 [1:55:18<16:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.04e-01:  87%|████████▋ | 8096/9281 [1:55:19<16:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  87%|████████▋ | 8097/9281 [1:55:20<16:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.30e-01:  87%|████████▋ | 8098/9281 [1:55:21<16:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  87%|████████▋ | 8099/9281 [1:55:22<16:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  87%|████████▋ | 8100/9281 [1:55:23<16:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.02e-01:  87%|████████▋ | 8101/9281 [1:55:23<16:53,  1.16it/s]\u001b[A\n",
      "Training loss: 3.11e-01:  87%|████████▋ | 8102/9281 [1:55:24<16:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.07e-01:  87%|████████▋ | 8103/9281 [1:55:25<16:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.14e-01:  87%|████████▋ | 8104/9281 [1:55:26<16:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.03e-01:  87%|████████▋ | 8105/9281 [1:55:27<16:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.05e-01:  87%|████████▋ | 8106/9281 [1:55:28<16:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  87%|████████▋ | 8107/9281 [1:55:28<16:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  87%|████████▋ | 8108/9281 [1:55:29<16:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  87%|████████▋ | 8109/9281 [1:55:30<16:42,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  87%|████████▋ | 8110/9281 [1:55:31<16:47,  1.16it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  87%|████████▋ | 8111/9281 [1:55:32<16:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  87%|████████▋ | 8112/9281 [1:55:33<16:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  87%|████████▋ | 8113/9281 [1:55:34<16:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  87%|████████▋ | 8114/9281 [1:55:34<16:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  87%|████████▋ | 8115/9281 [1:55:35<16:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  87%|████████▋ | 8116/9281 [1:55:36<16:40,  1.16it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  87%|████████▋ | 8117/9281 [1:55:37<16:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  87%|████████▋ | 8118/9281 [1:55:38<16:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  87%|████████▋ | 8119/9281 [1:55:39<16:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  87%|████████▋ | 8120/9281 [1:55:40<16:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  88%|████████▊ | 8121/9281 [1:55:40<16:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  88%|████████▊ | 8122/9281 [1:55:41<16:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.97e-01:  88%|████████▊ | 8123/9281 [1:55:42<16:35,  1.16it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  88%|████████▊ | 8124/9281 [1:55:43<16:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  88%|████████▊ | 8125/9281 [1:55:44<16:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.18e-01:  88%|████████▊ | 8126/9281 [1:55:45<16:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  88%|████████▊ | 8127/9281 [1:55:46<16:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  88%|████████▊ | 8128/9281 [1:55:46<16:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  88%|████████▊ | 8129/9281 [1:55:47<16:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  88%|████████▊ | 8130/9281 [1:55:48<16:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  88%|████████▊ | 8131/9281 [1:55:49<16:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  88%|████████▊ | 8132/9281 [1:55:50<16:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  88%|████████▊ | 8133/9281 [1:55:51<16:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  88%|████████▊ | 8134/9281 [1:55:52<16:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  88%|████████▊ | 8135/9281 [1:55:52<16:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.06e-01:  88%|████████▊ | 8136/9281 [1:55:53<16:20,  1.17it/s]\u001b[A\n",
      "Training loss: 2.85e-01:  88%|████████▊ | 8137/9281 [1:55:54<16:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.05e-01:  88%|████████▊ | 8138/9281 [1:55:55<16:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  88%|████████▊ | 8139/9281 [1:55:56<16:20,  1.16it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  88%|████████▊ | 8140/9281 [1:55:57<16:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  88%|████████▊ | 8141/9281 [1:55:58<16:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.54e-01:  88%|████████▊ | 8142/9281 [1:55:58<16:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  88%|████████▊ | 8143/9281 [1:55:59<16:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  88%|████████▊ | 8144/9281 [1:56:00<16:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  88%|████████▊ | 8145/9281 [1:56:01<16:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  88%|████████▊ | 8146/9281 [1:56:02<16:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  88%|████████▊ | 8147/9281 [1:56:03<16:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  88%|████████▊ | 8148/9281 [1:56:04<16:13,  1.16it/s]\u001b[A\n",
      "Training loss: 3.21e-01:  88%|████████▊ | 8149/9281 [1:56:04<16:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  88%|████████▊ | 8150/9281 [1:56:05<16:11,  1.16it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  88%|████████▊ | 8151/9281 [1:56:06<16:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  88%|████████▊ | 8152/9281 [1:56:07<16:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  88%|████████▊ | 8153/9281 [1:56:08<16:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  88%|████████▊ | 8154/9281 [1:56:09<16:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.30e-01:  88%|████████▊ | 8155/9281 [1:56:10<16:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.00e-01:  88%|████████▊ | 8156/9281 [1:56:10<16:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.15e-01:  88%|████████▊ | 8157/9281 [1:56:11<15:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  88%|████████▊ | 8158/9281 [1:56:12<15:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.29e-01:  88%|████████▊ | 8159/9281 [1:56:13<15:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  88%|████████▊ | 8160/9281 [1:56:14<15:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  88%|████████▊ | 8161/9281 [1:56:15<15:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  88%|████████▊ | 8162/9281 [1:56:16<15:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  88%|████████▊ | 8163/9281 [1:56:16<15:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  88%|████████▊ | 8164/9281 [1:56:17<15:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  88%|████████▊ | 8165/9281 [1:56:18<15:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  88%|████████▊ | 8166/9281 [1:56:19<15:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  88%|████████▊ | 8167/9281 [1:56:20<15:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  88%|████████▊ | 8168/9281 [1:56:21<15:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.54e-01:  88%|████████▊ | 8169/9281 [1:56:22<15:54,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  88%|████████▊ | 8170/9281 [1:56:22<15:48,  1.17it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  88%|████████▊ | 8171/9281 [1:56:23<15:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.36e-01:  88%|████████▊ | 8172/9281 [1:56:24<15:45,  1.17it/s]\u001b[A\n",
      "Training loss: 4.53e-01:  88%|████████▊ | 8173/9281 [1:56:25<15:41,  1.18it/s]\u001b[A\n",
      "Training loss: 4.28e-01:  88%|████████▊ | 8174/9281 [1:56:26<15:39,  1.18it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  88%|████████▊ | 8175/9281 [1:56:27<15:38,  1.18it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  88%|████████▊ | 8176/9281 [1:56:28<15:40,  1.18it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  88%|████████▊ | 8177/9281 [1:56:28<15:37,  1.18it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  88%|████████▊ | 8178/9281 [1:56:29<15:37,  1.18it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  88%|████████▊ | 8179/9281 [1:56:30<15:35,  1.18it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  88%|████████▊ | 8180/9281 [1:56:31<15:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  88%|████████▊ | 8181/9281 [1:56:32<15:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  88%|████████▊ | 8182/9281 [1:56:33<15:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  88%|████████▊ | 8183/9281 [1:56:33<15:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.79e-01:  88%|████████▊ | 8184/9281 [1:56:34<15:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  88%|████████▊ | 8185/9281 [1:56:35<15:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.87e-01:  88%|████████▊ | 8186/9281 [1:56:36<15:37,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  88%|████████▊ | 8187/9281 [1:56:37<15:40,  1.16it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  88%|████████▊ | 8188/9281 [1:56:38<15:39,  1.16it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  88%|████████▊ | 8189/9281 [1:56:39<15:38,  1.16it/s]\u001b[A\n",
      "Training loss: 4.47e-01:  88%|████████▊ | 8190/9281 [1:56:39<15:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.55e-01:  88%|████████▊ | 8191/9281 [1:56:40<15:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  88%|████████▊ | 8192/9281 [1:56:41<15:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.65e-01:  88%|████████▊ | 8193/9281 [1:56:42<15:32,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  88%|████████▊ | 8194/9281 [1:56:43<15:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.24e-01:  88%|████████▊ | 8195/9281 [1:56:44<15:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  88%|████████▊ | 8196/9281 [1:56:45<15:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  88%|████████▊ | 8197/9281 [1:56:45<15:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.82e-01:  88%|████████▊ | 8198/9281 [1:56:46<15:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  88%|████████▊ | 8199/9281 [1:56:47<15:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  88%|████████▊ | 8200/9281 [1:56:48<15:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  88%|████████▊ | 8201/9281 [1:56:49<15:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  88%|████████▊ | 8202/9281 [1:56:50<15:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  88%|████████▊ | 8203/9281 [1:56:51<15:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  88%|████████▊ | 8204/9281 [1:56:51<15:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  88%|████████▊ | 8205/9281 [1:56:52<15:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  88%|████████▊ | 8206/9281 [1:56:53<15:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  88%|████████▊ | 8207/9281 [1:56:54<15:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  88%|████████▊ | 8208/9281 [1:56:55<15:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  88%|████████▊ | 8209/9281 [1:56:56<15:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  88%|████████▊ | 8210/9281 [1:56:57<15:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  88%|████████▊ | 8211/9281 [1:56:57<15:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.33e-01:  88%|████████▊ | 8212/9281 [1:56:58<15:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  88%|████████▊ | 8213/9281 [1:56:59<15:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  89%|████████▊ | 8214/9281 [1:57:00<15:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  89%|████████▊ | 8215/9281 [1:57:01<15:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  89%|████████▊ | 8216/9281 [1:57:02<15:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  89%|████████▊ | 8217/9281 [1:57:03<15:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  89%|████████▊ | 8218/9281 [1:57:03<15:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  89%|████████▊ | 8219/9281 [1:57:04<15:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  89%|████████▊ | 8220/9281 [1:57:05<15:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  89%|████████▊ | 8221/9281 [1:57:06<15:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  89%|████████▊ | 8222/9281 [1:57:07<15:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  89%|████████▊ | 8223/9281 [1:57:08<15:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  89%|████████▊ | 8224/9281 [1:57:09<15:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.19e-01:  89%|████████▊ | 8225/9281 [1:57:09<15:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  89%|████████▊ | 8226/9281 [1:57:10<15:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.15e-01:  89%|████████▊ | 8227/9281 [1:57:11<15:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  89%|████████▊ | 8228/9281 [1:57:12<15:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.03e-01:  89%|████████▊ | 8229/9281 [1:57:13<15:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.12e-01:  89%|████████▊ | 8230/9281 [1:57:14<14:59,  1.17it/s]\u001b[A\n",
      "Training loss: 2.90e-01:  89%|████████▊ | 8231/9281 [1:57:15<14:57,  1.17it/s]\u001b[A\n",
      "Training loss: 2.59e-01:  89%|████████▊ | 8232/9281 [1:57:15<14:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  89%|████████▊ | 8233/9281 [1:57:16<14:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.02e-01:  89%|████████▊ | 8234/9281 [1:57:17<14:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  89%|████████▊ | 8235/9281 [1:57:18<14:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  89%|████████▊ | 8236/9281 [1:57:19<14:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  89%|████████▉ | 8237/9281 [1:57:20<14:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  89%|████████▉ | 8238/9281 [1:57:21<14:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  89%|████████▉ | 8239/9281 [1:57:21<14:53,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  89%|████████▉ | 8240/9281 [1:57:22<14:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  89%|████████▉ | 8241/9281 [1:57:23<14:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  89%|████████▉ | 8242/9281 [1:57:24<14:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  89%|████████▉ | 8243/9281 [1:57:25<14:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  89%|████████▉ | 8244/9281 [1:57:26<14:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  89%|████████▉ | 8245/9281 [1:57:27<14:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  89%|████████▉ | 8246/9281 [1:57:27<14:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  89%|████████▉ | 8247/9281 [1:57:28<14:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  89%|████████▉ | 8248/9281 [1:57:29<14:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  89%|████████▉ | 8249/9281 [1:57:30<14:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.01e-01:  89%|████████▉ | 8250/9281 [1:57:31<14:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  89%|████████▉ | 8251/9281 [1:57:32<14:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  89%|████████▉ | 8252/9281 [1:57:33<14:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.39e-01:  89%|████████▉ | 8253/9281 [1:57:33<14:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  89%|████████▉ | 8254/9281 [1:57:34<14:33,  1.18it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  89%|████████▉ | 8255/9281 [1:57:35<14:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  89%|████████▉ | 8256/9281 [1:57:36<14:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  89%|████████▉ | 8257/9281 [1:57:37<14:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.23e-01:  89%|████████▉ | 8258/9281 [1:57:38<14:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  89%|████████▉ | 8259/9281 [1:57:39<14:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  89%|████████▉ | 8260/9281 [1:57:39<14:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  89%|████████▉ | 8261/9281 [1:57:40<14:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.60e-01:  89%|████████▉ | 8262/9281 [1:57:41<14:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.24e-01:  89%|████████▉ | 8263/9281 [1:57:42<14:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  89%|████████▉ | 8264/9281 [1:57:43<14:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  89%|████████▉ | 8265/9281 [1:57:44<14:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.09e-01:  89%|████████▉ | 8266/9281 [1:57:44<14:31,  1.16it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  89%|████████▉ | 8267/9281 [1:57:45<14:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  89%|████████▉ | 8268/9281 [1:57:46<14:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  89%|████████▉ | 8269/9281 [1:57:47<14:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.27e-01:  89%|████████▉ | 8270/9281 [1:57:48<14:25,  1.17it/s]\u001b[A\n",
      "Training loss: 3.07e-01:  89%|████████▉ | 8271/9281 [1:57:49<14:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  89%|████████▉ | 8272/9281 [1:57:50<14:26,  1.16it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  89%|████████▉ | 8273/9281 [1:57:50<14:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  89%|████████▉ | 8274/9281 [1:57:51<14:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  89%|████████▉ | 8275/9281 [1:57:52<14:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  89%|████████▉ | 8276/9281 [1:57:53<14:15,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  89%|████████▉ | 8277/9281 [1:57:54<14:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  89%|████████▉ | 8278/9281 [1:57:55<14:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  89%|████████▉ | 8279/9281 [1:57:56<14:12,  1.18it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  89%|████████▉ | 8280/9281 [1:57:56<14:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  89%|████████▉ | 8281/9281 [1:57:57<14:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  89%|████████▉ | 8282/9281 [1:57:58<14:09,  1.18it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  89%|████████▉ | 8283/9281 [1:57:59<14:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  89%|████████▉ | 8284/9281 [1:58:00<14:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  89%|████████▉ | 8285/9281 [1:58:01<14:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  89%|████████▉ | 8286/9281 [1:58:02<14:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.15e-01:  89%|████████▉ | 8287/9281 [1:58:02<14:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  89%|████████▉ | 8288/9281 [1:58:03<14:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  89%|████████▉ | 8289/9281 [1:58:04<14:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  89%|████████▉ | 8290/9281 [1:58:05<14:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  89%|████████▉ | 8291/9281 [1:58:06<14:08,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  89%|████████▉ | 8292/9281 [1:58:07<14:10,  1.16it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  89%|████████▉ | 8293/9281 [1:58:08<14:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  89%|████████▉ | 8294/9281 [1:58:08<14:07,  1.16it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  89%|████████▉ | 8295/9281 [1:58:09<14:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  89%|████████▉ | 8296/9281 [1:58:10<14:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  89%|████████▉ | 8297/9281 [1:58:11<14:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  89%|████████▉ | 8298/9281 [1:58:12<14:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.16e-01:  89%|████████▉ | 8299/9281 [1:58:13<13:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.42e-01:  89%|████████▉ | 8300/9281 [1:58:14<13:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  89%|████████▉ | 8301/9281 [1:58:14<13:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  89%|████████▉ | 8302/9281 [1:58:15<13:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  89%|████████▉ | 8303/9281 [1:58:16<13:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  89%|████████▉ | 8304/9281 [1:58:17<13:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  89%|████████▉ | 8305/9281 [1:58:18<13:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  89%|████████▉ | 8306/9281 [1:58:19<13:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  90%|████████▉ | 8307/9281 [1:58:20<13:57,  1.16it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  90%|████████▉ | 8308/9281 [1:58:20<13:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  90%|████████▉ | 8309/9281 [1:58:21<13:54,  1.16it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  90%|████████▉ | 8310/9281 [1:58:22<13:51,  1.17it/s]\u001b[A\n",
      "Training loss: 4.50e-01:  90%|████████▉ | 8311/9281 [1:58:23<13:52,  1.16it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  90%|████████▉ | 8312/9281 [1:58:24<13:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  90%|████████▉ | 8313/9281 [1:58:25<13:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  90%|████████▉ | 8314/9281 [1:58:26<13:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  90%|████████▉ | 8315/9281 [1:58:26<13:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  90%|████████▉ | 8316/9281 [1:58:27<13:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.08e-01:  90%|████████▉ | 8317/9281 [1:58:28<13:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  90%|████████▉ | 8318/9281 [1:58:29<13:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  90%|████████▉ | 8319/9281 [1:58:30<13:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  90%|████████▉ | 8320/9281 [1:58:31<13:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  90%|████████▉ | 8321/9281 [1:58:32<13:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  90%|████████▉ | 8322/9281 [1:58:32<13:44,  1.16it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  90%|████████▉ | 8323/9281 [1:58:33<13:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  90%|████████▉ | 8324/9281 [1:58:34<13:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.14e-01:  90%|████████▉ | 8325/9281 [1:58:35<13:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  90%|████████▉ | 8326/9281 [1:58:36<13:40,  1.16it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  90%|████████▉ | 8327/9281 [1:58:37<13:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  90%|████████▉ | 8328/9281 [1:58:38<13:38,  1.16it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  90%|████████▉ | 8329/9281 [1:58:38<13:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  90%|████████▉ | 8330/9281 [1:58:39<13:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  90%|████████▉ | 8331/9281 [1:58:40<13:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  90%|████████▉ | 8332/9281 [1:58:41<13:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.39e-01:  90%|████████▉ | 8333/9281 [1:58:42<13:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.01e-01:  90%|████████▉ | 8334/9281 [1:58:43<13:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  90%|████████▉ | 8335/9281 [1:58:44<13:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.82e-01:  90%|████████▉ | 8336/9281 [1:58:44<13:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  90%|████████▉ | 8337/9281 [1:58:45<13:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.72e-01:  90%|████████▉ | 8338/9281 [1:58:46<13:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.27e-01:  90%|████████▉ | 8339/9281 [1:58:47<13:28,  1.17it/s]\u001b[A\n",
      "Training loss: 4.39e-01:  90%|████████▉ | 8340/9281 [1:58:48<13:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.14e-01:  90%|████████▉ | 8341/9281 [1:58:49<13:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  90%|████████▉ | 8342/9281 [1:58:50<13:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  90%|████████▉ | 8343/9281 [1:58:50<13:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  90%|████████▉ | 8344/9281 [1:58:51<13:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  90%|████████▉ | 8345/9281 [1:58:52<13:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  90%|████████▉ | 8346/9281 [1:58:53<13:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  90%|████████▉ | 8347/9281 [1:58:54<13:22,  1.16it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  90%|████████▉ | 8348/9281 [1:58:55<13:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.20e-01:  90%|████████▉ | 8349/9281 [1:58:56<13:19,  1.17it/s]\u001b[A\n",
      "Training loss: 2.92e-01:  90%|████████▉ | 8350/9281 [1:58:56<13:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  90%|████████▉ | 8351/9281 [1:58:57<13:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  90%|████████▉ | 8352/9281 [1:58:58<13:18,  1.16it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  90%|█████████ | 8353/9281 [1:58:59<13:18,  1.16it/s]\u001b[A\n",
      "Training loss: 4.11e-01:  90%|█████████ | 8354/9281 [1:59:00<13:19,  1.16it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  90%|█████████ | 8355/9281 [1:59:01<13:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  90%|█████████ | 8356/9281 [1:59:02<13:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  90%|█████████ | 8357/9281 [1:59:02<13:10,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  90%|█████████ | 8358/9281 [1:59:03<13:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  90%|█████████ | 8359/9281 [1:59:04<13:07,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  90%|█████████ | 8360/9281 [1:59:05<13:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  90%|█████████ | 8361/9281 [1:59:06<13:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  90%|█████████ | 8362/9281 [1:59:07<13:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  90%|█████████ | 8363/9281 [1:59:08<13:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.22e-01:  90%|█████████ | 8364/9281 [1:59:08<13:07,  1.16it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  90%|█████████ | 8365/9281 [1:59:09<13:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  90%|█████████ | 8366/9281 [1:59:10<13:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.55e-01:  90%|█████████ | 8367/9281 [1:59:11<13:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  90%|█████████ | 8368/9281 [1:59:12<12:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  90%|█████████ | 8369/9281 [1:59:13<12:55,  1.18it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  90%|█████████ | 8370/9281 [1:59:13<12:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  90%|█████████ | 8371/9281 [1:59:14<12:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  90%|█████████ | 8372/9281 [1:59:15<12:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  90%|█████████ | 8373/9281 [1:59:16<12:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  90%|█████████ | 8374/9281 [1:59:17<12:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  90%|█████████ | 8375/9281 [1:59:18<12:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.47e-01:  90%|█████████ | 8376/9281 [1:59:19<12:53,  1.17it/s]\u001b[A\n",
      "Training loss: 2.94e-01:  90%|█████████ | 8377/9281 [1:59:19<12:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.10e-01:  90%|█████████ | 8378/9281 [1:59:20<12:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  90%|█████████ | 8379/9281 [1:59:21<12:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  90%|█████████ | 8380/9281 [1:59:22<12:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  90%|█████████ | 8381/9281 [1:59:23<12:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  90%|█████████ | 8382/9281 [1:59:24<12:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  90%|█████████ | 8383/9281 [1:59:25<12:49,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  90%|█████████ | 8384/9281 [1:59:25<12:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  90%|█████████ | 8385/9281 [1:59:26<12:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  90%|█████████ | 8386/9281 [1:59:27<12:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  90%|█████████ | 8387/9281 [1:59:28<12:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  90%|█████████ | 8388/9281 [1:59:29<12:44,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  90%|█████████ | 8389/9281 [1:59:30<12:42,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  90%|█████████ | 8390/9281 [1:59:31<12:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  90%|█████████ | 8391/9281 [1:59:31<12:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  90%|█████████ | 8392/9281 [1:59:32<12:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.26e-01:  90%|█████████ | 8393/9281 [1:59:33<12:42,  1.16it/s]\u001b[A\n",
      "Training loss: 4.21e-01:  90%|█████████ | 8394/9281 [1:59:34<12:41,  1.16it/s]\u001b[A\n",
      "Training loss: 4.02e-01:  90%|█████████ | 8395/9281 [1:59:35<12:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  90%|█████████ | 8396/9281 [1:59:36<12:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  90%|█████████ | 8397/9281 [1:59:37<12:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  90%|█████████ | 8398/9281 [1:59:37<12:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.65e-01:  90%|█████████ | 8399/9281 [1:59:38<12:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  91%|█████████ | 8400/9281 [1:59:39<12:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  91%|█████████ | 8401/9281 [1:59:40<12:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  91%|█████████ | 8402/9281 [1:59:41<12:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  91%|█████████ | 8403/9281 [1:59:42<12:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.78e-01:  91%|█████████ | 8404/9281 [1:59:43<12:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  91%|█████████ | 8405/9281 [1:59:43<12:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.09e-01:  91%|█████████ | 8406/9281 [1:59:44<12:32,  1.16it/s]\u001b[A\n",
      "Training loss: 3.31e-01:  91%|█████████ | 8407/9281 [1:59:45<12:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.15e-01:  91%|█████████ | 8408/9281 [1:59:46<12:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  91%|█████████ | 8409/9281 [1:59:47<12:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  91%|█████████ | 8410/9281 [1:59:48<12:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.08e-01:  91%|█████████ | 8411/9281 [1:59:49<12:23,  1.17it/s]\u001b[A\n",
      "Training loss: 2.88e-01:  91%|█████████ | 8412/9281 [1:59:49<12:24,  1.17it/s]\u001b[A\n",
      "Training loss: 2.70e-01:  91%|█████████ | 8413/9281 [1:59:50<12:21,  1.17it/s]\u001b[A\n",
      "Training loss: 2.72e-01:  91%|█████████ | 8414/9281 [1:59:51<12:23,  1.17it/s]\u001b[A\n",
      "Training loss: 2.76e-01:  91%|█████████ | 8415/9281 [1:59:52<12:23,  1.16it/s]\u001b[A\n",
      "Training loss: 3.07e-01:  91%|█████████ | 8416/9281 [1:59:53<12:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  91%|█████████ | 8417/9281 [1:59:54<12:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  91%|█████████ | 8418/9281 [1:59:55<12:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  91%|█████████ | 8419/9281 [1:59:55<12:13,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  91%|█████████ | 8420/9281 [1:59:56<12:12,  1.18it/s]\u001b[A\n",
      "Training loss: 4.45e-01:  91%|█████████ | 8421/9281 [1:59:57<12:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  91%|█████████ | 8422/9281 [1:59:58<12:14,  1.17it/s]\u001b[A\n",
      "Training loss: 4.46e-01:  91%|█████████ | 8423/9281 [1:59:59<12:12,  1.17it/s]\u001b[A\n",
      "Training loss: 4.43e-01:  91%|█████████ | 8424/9281 [2:00:00<12:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  91%|█████████ | 8425/9281 [2:00:01<12:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.90e-01:  91%|█████████ | 8426/9281 [2:00:01<12:10,  1.17it/s]\u001b[A\n",
      "Training loss: 3.69e-01:  91%|█████████ | 8427/9281 [2:00:02<12:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  91%|█████████ | 8428/9281 [2:00:03<12:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.12e-01:  91%|█████████ | 8429/9281 [2:00:04<12:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.14e-01:  91%|█████████ | 8430/9281 [2:00:05<12:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.06e-01:  91%|█████████ | 8431/9281 [2:00:06<12:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  91%|█████████ | 8432/9281 [2:00:07<12:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.71e-01:  91%|█████████ | 8433/9281 [2:00:07<12:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.42e-01:  91%|█████████ | 8434/9281 [2:00:08<12:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.84e-01:  91%|█████████ | 8435/9281 [2:00:09<12:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  91%|█████████ | 8436/9281 [2:00:10<12:01,  1.17it/s]\u001b[A\n",
      "Training loss: 3.58e-01:  91%|█████████ | 8437/9281 [2:00:11<12:00,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  91%|█████████ | 8438/9281 [2:00:12<11:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  91%|█████████ | 8439/9281 [2:00:13<11:57,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  91%|█████████ | 8440/9281 [2:00:13<11:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  91%|█████████ | 8441/9281 [2:00:14<11:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  91%|█████████ | 8442/9281 [2:00:15<11:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  91%|█████████ | 8443/9281 [2:00:16<11:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  91%|█████████ | 8444/9281 [2:00:17<11:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  91%|█████████ | 8445/9281 [2:00:18<11:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.23e-01:  91%|█████████ | 8446/9281 [2:00:18<11:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  91%|█████████ | 8447/9281 [2:00:19<11:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  91%|█████████ | 8448/9281 [2:00:20<11:48,  1.18it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  91%|█████████ | 8449/9281 [2:00:21<11:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  91%|█████████ | 8450/9281 [2:00:22<11:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  91%|█████████ | 8451/9281 [2:00:23<11:45,  1.18it/s]\u001b[A\n",
      "Training loss: 3.37e-01:  91%|█████████ | 8452/9281 [2:00:24<11:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  91%|█████████ | 8453/9281 [2:00:24<11:48,  1.17it/s]\u001b[A\n",
      "Training loss: 2.92e-01:  91%|█████████ | 8454/9281 [2:00:25<11:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.54e-01:  91%|█████████ | 8455/9281 [2:00:26<11:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  91%|█████████ | 8456/9281 [2:00:27<11:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  91%|█████████ | 8457/9281 [2:00:28<11:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  91%|█████████ | 8458/9281 [2:00:29<11:41,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  91%|█████████ | 8459/9281 [2:00:30<11:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  91%|█████████ | 8460/9281 [2:00:30<11:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  91%|█████████ | 8461/9281 [2:00:31<11:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.85e-01:  91%|█████████ | 8462/9281 [2:00:32<11:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.79e-01:  91%|█████████ | 8463/9281 [2:00:33<11:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  91%|█████████ | 8464/9281 [2:00:34<11:39,  1.17it/s]\u001b[A\n",
      "Training loss: 3.31e-01:  91%|█████████ | 8465/9281 [2:00:35<11:42,  1.16it/s]\u001b[A\n",
      "Training loss: 3.39e-01:  91%|█████████ | 8466/9281 [2:00:36<11:39,  1.16it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  91%|█████████ | 8467/9281 [2:00:36<11:38,  1.17it/s]\u001b[A\n",
      "Training loss: 4.04e-01:  91%|█████████ | 8468/9281 [2:00:37<11:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  91%|█████████▏| 8469/9281 [2:00:38<11:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  91%|█████████▏| 8470/9281 [2:00:39<11:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.50e-01:  91%|█████████▏| 8471/9281 [2:00:40<11:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.07e-01:  91%|█████████▏| 8472/9281 [2:00:41<11:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.45e-01:  91%|█████████▏| 8473/9281 [2:00:42<11:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.56e-01:  91%|█████████▏| 8474/9281 [2:00:42<11:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  91%|█████████▏| 8475/9281 [2:00:43<11:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  91%|█████████▏| 8476/9281 [2:00:44<11:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.13e-01:  91%|█████████▏| 8477/9281 [2:00:45<11:27,  1.17it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  91%|█████████▏| 8478/9281 [2:00:46<11:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  91%|█████████▏| 8479/9281 [2:00:47<11:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  91%|█████████▏| 8480/9281 [2:00:48<11:24,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  91%|█████████▏| 8481/9281 [2:00:48<11:23,  1.17it/s]\u001b[A\n",
      "Training loss: 3.81e-01:  91%|█████████▏| 8482/9281 [2:00:49<11:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  91%|█████████▏| 8483/9281 [2:00:50<11:20,  1.17it/s]\u001b[A\n",
      "Training loss: 4.07e-01:  91%|█████████▏| 8484/9281 [2:00:51<11:22,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  91%|█████████▏| 8485/9281 [2:00:52<11:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.34e-01:  91%|█████████▏| 8486/9281 [2:00:53<11:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.92e-01:  91%|█████████▏| 8487/9281 [2:00:54<11:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  91%|█████████▏| 8488/9281 [2:00:54<11:19,  1.17it/s]\u001b[A\n",
      "Training loss: 3.28e-01:  91%|█████████▏| 8489/9281 [2:00:55<11:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.59e-01:  91%|█████████▏| 8490/9281 [2:00:56<11:18,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  91%|█████████▏| 8491/9281 [2:00:57<11:16,  1.17it/s]\u001b[A\n",
      "Training loss: 3.68e-01:  91%|█████████▏| 8492/9281 [2:00:58<11:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.95e-01:  92%|█████████▏| 8493/9281 [2:00:59<11:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  92%|█████████▏| 8494/9281 [2:01:00<11:12,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  92%|█████████▏| 8495/9281 [2:01:00<11:09,  1.17it/s]\u001b[A\n",
      "Training loss: 4.15e-01:  92%|█████████▏| 8496/9281 [2:01:01<11:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.52e-01:  92%|█████████▏| 8497/9281 [2:01:02<11:09,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  92%|█████████▏| 8498/9281 [2:01:03<11:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  92%|█████████▏| 8499/9281 [2:01:04<11:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.61e-01:  92%|█████████▏| 8500/9281 [2:01:05<11:06,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  92%|█████████▏| 8501/9281 [2:01:05<11:03,  1.18it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  92%|█████████▏| 8502/9281 [2:01:06<11:03,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  92%|█████████▏| 8503/9281 [2:01:07<11:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.32e-01:  92%|█████████▏| 8504/9281 [2:01:08<11:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  92%|█████████▏| 8505/9281 [2:01:09<11:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  92%|█████████▏| 8506/9281 [2:01:10<11:03,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  92%|█████████▏| 8507/9281 [2:01:11<11:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.06e-01:  92%|█████████▏| 8508/9281 [2:01:11<11:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.70e-01:  92%|█████████▏| 8509/9281 [2:01:12<10:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  92%|█████████▏| 8510/9281 [2:01:13<10:59,  1.17it/s]\u001b[A\n",
      "Training loss: 3.77e-01:  92%|█████████▏| 8511/9281 [2:01:14<10:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.87e-01:  92%|█████████▏| 8512/9281 [2:01:15<10:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  92%|█████████▏| 8513/9281 [2:01:16<10:58,  1.17it/s]\u001b[A\n",
      "Training loss: 3.10e-01:  92%|█████████▏| 8514/9281 [2:01:17<10:57,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  92%|█████████▏| 8515/9281 [2:01:17<10:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.11e-01:  92%|█████████▏| 8516/9281 [2:01:18<10:56,  1.17it/s]\u001b[A\n",
      "Training loss: 3.33e-01:  92%|█████████▏| 8517/9281 [2:01:19<10:56,  1.16it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  92%|█████████▏| 8518/9281 [2:01:20<10:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  92%|█████████▏| 8519/9281 [2:01:21<10:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.99e-01:  92%|█████████▏| 8520/9281 [2:01:22<10:50,  1.17it/s]\u001b[A\n",
      "Training loss: 3.86e-01:  92%|█████████▏| 8521/9281 [2:01:23<10:50,  1.17it/s]\u001b[A\n",
      "Training loss: 4.03e-01:  92%|█████████▏| 8522/9281 [2:01:23<10:47,  1.17it/s]\u001b[A\n",
      "Training loss: 3.98e-01:  92%|█████████▏| 8523/9281 [2:01:24<10:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.29e-01:  92%|█████████▏| 8524/9281 [2:01:25<10:45,  1.17it/s]\u001b[A\n",
      "Training loss: 3.70e-01:  92%|█████████▏| 8525/9281 [2:01:26<10:48,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  92%|█████████▏| 8526/9281 [2:01:27<10:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  92%|█████████▏| 8527/9281 [2:01:28<10:46,  1.17it/s]\u001b[A\n",
      "Training loss: 3.41e-01:  92%|█████████▏| 8528/9281 [2:01:29<10:43,  1.17it/s]\u001b[A\n",
      "Training loss: 2.95e-01:  92%|█████████▏| 8529/9281 [2:01:29<10:43,  1.17it/s]\u001b[A\n",
      "Training loss: 3.44e-01:  92%|█████████▏| 8530/9281 [2:01:30<10:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  92%|█████████▏| 8531/9281 [2:01:31<10:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.80e-01:  92%|█████████▏| 8532/9281 [2:01:32<10:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  92%|█████████▏| 8533/9281 [2:01:33<10:36,  1.18it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  92%|█████████▏| 8534/9281 [2:01:34<10:35,  1.18it/s]\u001b[A\n",
      "Training loss: 4.41e-01:  92%|█████████▏| 8535/9281 [2:01:35<10:35,  1.17it/s]\u001b[A\n",
      "Training loss: 4.60e-01:  92%|█████████▏| 8536/9281 [2:01:35<10:34,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  92%|█████████▏| 8537/9281 [2:01:36<10:36,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  92%|█████████▏| 8538/9281 [2:01:37<10:33,  1.17it/s]\u001b[A\n",
      "Training loss: 4.49e-01:  92%|█████████▏| 8539/9281 [2:01:38<10:34,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  92%|█████████▏| 8540/9281 [2:01:39<10:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.93e-01:  92%|█████████▏| 8541/9281 [2:01:40<10:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  92%|█████████▏| 8542/9281 [2:01:41<10:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  92%|█████████▏| 8543/9281 [2:01:41<10:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.83e-01:  92%|█████████▏| 8544/9281 [2:01:42<10:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.91e-01:  92%|█████████▏| 8545/9281 [2:01:43<10:29,  1.17it/s]\u001b[A\n",
      "Training loss: 4.23e-01:  92%|█████████▏| 8546/9281 [2:01:44<10:27,  1.17it/s]\u001b[A\n",
      "Training loss: 4.31e-01:  92%|█████████▏| 8547/9281 [2:01:45<10:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.51e-01:  92%|█████████▏| 8548/9281 [2:01:46<10:26,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  92%|█████████▏| 8549/9281 [2:01:47<10:26,  1.17it/s]\u001b[A\n",
      "Training loss: 3.75e-01:  92%|█████████▏| 8550/9281 [2:01:47<10:25,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  92%|█████████▏| 8551/9281 [2:01:48<10:24,  1.17it/s]\u001b[A\n",
      "Training loss: 3.89e-01:  92%|█████████▏| 8552/9281 [2:01:49<10:21,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  92%|█████████▏| 8553/9281 [2:01:50<10:20,  1.17it/s]\u001b[A\n",
      "Training loss: 3.48e-01:  92%|█████████▏| 8554/9281 [2:01:51<10:21,  1.17it/s]\u001b[A\n",
      "Training loss: 4.62e-01:  92%|█████████▏| 8555/9281 [2:01:52<10:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.57e-01:  92%|█████████▏| 8556/9281 [2:01:52<10:19,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  92%|█████████▏| 8557/9281 [2:01:53<10:16,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  92%|█████████▏| 8558/9281 [2:01:54<10:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.19e-01:  92%|█████████▏| 8559/9281 [2:01:55<10:17,  1.17it/s]\u001b[A\n",
      "Training loss: 4.00e-01:  92%|█████████▏| 8560/9281 [2:01:56<10:17,  1.17it/s]\u001b[A\n",
      "Training loss: 3.74e-01:  92%|█████████▏| 8561/9281 [2:01:57<10:15,  1.17it/s]\u001b[A\n",
      "Training loss: 3.29e-01:  92%|█████████▏| 8562/9281 [2:01:58<10:17,  1.16it/s]\u001b[A\n",
      "Training loss: 3.53e-01:  92%|█████████▏| 8563/9281 [2:01:58<10:14,  1.17it/s]\u001b[A\n",
      "Training loss: 3.52e-01:  92%|█████████▏| 8564/9281 [2:01:59<10:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.72e-01:  92%|█████████▏| 8565/9281 [2:02:00<10:13,  1.17it/s]\u001b[A\n",
      "Training loss: 3.73e-01:  92%|█████████▏| 8566/9281 [2:02:01<10:14,  1.16it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  92%|█████████▏| 8567/9281 [2:02:02<10:11,  1.17it/s]\u001b[A\n",
      "Training loss: 4.20e-01:  92%|█████████▏| 8568/9281 [2:02:03<10:11,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  92%|█████████▏| 8569/9281 [2:02:04<10:08,  1.17it/s]\u001b[A\n",
      "Training loss: 3.43e-01:  92%|█████████▏| 8570/9281 [2:02:04<10:07,  1.17it/s]\u001b[A\n",
      "Training loss: 3.32e-01:  92%|█████████▏| 8571/9281 [2:02:05<10:05,  1.17it/s]\u001b[A\n",
      "Training loss: 3.36e-01:  92%|█████████▏| 8572/9281 [2:02:06<10:06,  1.17it/s]\u001b[A\n",
      "Training loss: 3.38e-01:  92%|█████████▏| 8573/9281 [2:02:07<10:04,  1.17it/s]\u001b[A\n",
      "Training loss: 3.03e-01:  92%|█████████▏| 8574/9281 [2:02:08<10:02,  1.17it/s]\u001b[A\n",
      "Training loss: 3.76e-01:  92%|█████████▏| 8575/9281 [2:02:09<10:02,  1.17it/s]\u001b[A\n",
      "Training loss: 4.09e-01:  92%|█████████▏| 8576/9281 [2:02:10<10:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.25e-01:  92%|█████████▏| 8577/9281 [2:02:10<09:59,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  92%|█████████▏| 8578/9281 [2:02:11<10:01,  1.17it/s]\u001b[A\n",
      "Training loss: 4.73e-01:  92%|█████████▏| 8579/9281 [2:02:12<09:58,  1.17it/s]\u001b[A\n",
      "Training loss: 4.51e-01:  92%|█████████▏| 8580/9281 [2:02:13<10:00,  1.17it/s]\u001b[A\n",
      "Training loss: 4.10e-01:  92%|█████████▏| 8581/9281 [2:02:14<09:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.16e-01:  92%|█████████▏| 8582/9281 [2:02:15<09:56,  1.17it/s]\u001b[A\n",
      "Training loss: 4.05e-01:  92%|█████████▏| 8583/9281 [2:02:16<09:55,  1.17it/s]\u001b[A\n",
      "Training loss: 4.22e-01:  92%|█████████▏| 8584/9281 [2:02:16<09:55,  1.17it/s]\u001b[A\n",
      "Training loss: 3.96e-01:  93%|█████████▎| 8585/9281 [2:02:17<09:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.66e-01:  93%|█████████▎| 8586/9281 [2:02:18<09:54,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  93%|█████████▎| 8587/9281 [2:02:19<09:52,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  93%|█████████▎| 8588/9281 [2:02:20<09:55,  1.16it/s]\u001b[A\n",
      "Training loss: 3.40e-01:  93%|█████████▎| 8589/9281 [2:02:21<09:53,  1.17it/s]\u001b[A\n",
      "Training loss: 3.34e-01:  93%|█████████▎| 8590/9281 [2:02:22<09:51,  1.17it/s]\u001b[A\n",
      "Training loss: 3.26e-01:  93%|█████████▎| 8591/9281 [2:02:22<09:53,  1.16it/s]\u001b[A\n",
      "Training loss: 3.16e-01:  93%|█████████▎| 8592/9281 [2:02:23<09:53,  1.16it/s]\u001b[A\n",
      "Training loss: 3.25e-01:  93%|█████████▎| 8593/9281 [2:02:24<09:51,  1.16it/s]\u001b[A\n",
      "Training loss: 3.17e-01:  93%|█████████▎| 8594/9281 [2:02:25<09:49,  1.17it/s]\u001b[A\n",
      "Training loss: 3.62e-01:  93%|█████████▎| 8595/9281 [2:02:26<09:49,  1.16it/s]\u001b[A\n",
      "Training loss: 3.64e-01:  93%|█████████▎| 8596/9281 [2:02:27<09:47,  1.17it/s]\u001b[A\n",
      "Training loss: 4.08e-01:  93%|█████████▎| 8597/9281 [2:02:28<09:46,  1.17it/s]\u001b[A\n",
      "Training loss: 4.35e-01:  93%|█████████▎| 8598/9281 [2:02:28<09:43,  1.17it/s]\u001b[A\n",
      "Training loss: 4.12e-01:  93%|█████████▎| 8599/9281 [2:02:29<09:41,  1.17it/s]\u001b[A\n",
      "Training loss: 4.17e-01:  93%|█████████▎| 8600/9281 [2:02:30<09:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.30e-01:  93%|█████████▎| 8601/9281 [2:02:31<09:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.88e-01:  93%|█████████▎| 8602/9281 [2:02:32<09:39,  1.17it/s]\u001b[A\n",
      "Training loss: 4.58e-01:  93%|█████████▎| 8603/9281 [2:02:33<09:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.94e-01:  93%|█████████▎| 8604/9281 [2:02:34<09:40,  1.17it/s]\u001b[A\n",
      "Training loss: 3.46e-01:  93%|█████████▎| 8605/9281 [2:02:34<09:38,  1.17it/s]\u001b[A\n",
      "Training loss: 3.10e-01:  93%|█████████▎| 8606/9281 [2:02:35<09:37,  1.17it/s]\u001b[A\n",
      "Training loss: 3.29e-01:  93%|█████████▎| 8607/9281 [2:02:36<09:36,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  93%|█████████▎| 8608/9281 [2:02:37<09:35,  1.17it/s]\u001b[A\n",
      "Training loss: 3.12e-01:  93%|█████████▎| 8609/9281 [2:02:38<09:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.04e-01:  93%|█████████▎| 8610/9281 [2:02:39<09:33,  1.17it/s]\u001b[A\n",
      "Training loss: 3.16e-01:  93%|█████████▎| 8611/9281 [2:02:40<09:31,  1.17it/s]\u001b[A\n",
      "Training loss: 3.34e-01:  93%|█████████▎| 8612/9281 [2:02:40<09:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.63e-01:  93%|█████████▎| 8613/9281 [2:02:41<09:29,  1.17it/s]\u001b[A\n",
      "Training loss: 3.35e-01:  93%|█████████▎| 8614/9281 [2:02:42<09:32,  1.17it/s]\u001b[A\n",
      "Training loss: 3.34e-01:  93%|█████████▎| 8615/9281 [2:02:43<09:30,  1.17it/s]\u001b[A\n",
      "Training loss: 4.13e-01:  93%|█████████▎| 8616/9281 [2:02:44<09:30,  1.17it/s]\u001b[A\n",
      "Training loss: 3.57e-01:  93%|█████████▎| 8617/9281 [2:02:45<09:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.67e-01:  93%|█████████▎| 8618/9281 [2:02:46<09:28,  1.17it/s]\u001b[A\n",
      "Training loss: 3.49e-01:  93%|█████████▎| 8619/9281 [2:02:46<09:24,  1.17it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "if do_train:\n",
    "        nb_tr_steps, tr_loss, exp_average_loss = 0, 0, None\n",
    "        model.train()\n",
    "        for epoch in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "            tr_loss = 0\n",
    "            nb_tr_steps = 0\n",
    "            tqdm_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "            for step, batch in enumerate(tqdm_bar):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, lm_labels = batch\n",
    "                #print(input_ids.shape)\n",
    "                loss = model(input_ids, labels=lm_labels)[0]\n",
    "                if n_gpu > 1:\n",
    "                    loss.mean().backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                if n_gpu > 1:\n",
    "                    tmp_loss = loss.mean().item()\n",
    "                else:\n",
    "                    tmp_loss = loss.item()\n",
    "                exp_average_loss = tmp_loss if exp_average_loss is None else 0.7 * exp_average_loss + 0.3 * tmp_loss\n",
    "                nb_tr_steps += 1\n",
    "                #if(nb_tr_steps%100==0):\n",
    "                tqdm_bar.desc = f\"Training loss: {exp_average_loss:.2e}\"\n",
    "\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "            output_model_file = os.path.join(output_dir, \"pytorch_model_zero_grad.bin\")\n",
    "            config = model.module.config if hasattr(model, 'module') else model.config\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3wpMRt_StZ9"
   },
   "outputs": [],
   "source": [
    "output_model_file = os.path.join(output_dir, \"pytorch_model_zero_grad.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "3jjR493PSpuB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50264, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "#model_state_dict = torch.load('yelp_models/pytorch_model_zero_grad_1.bin', map_location='cpu')\n",
    "model_state_dict = torch.load('yelp_models/pytorch_model_attr_1e.bin', map_location='cpu')\n",
    "\n",
    "special_tokens = ['<POS>', '<NEG>','<CON_START>','<START>','<END>','<ATTR_WORDS>','<PAD>']\n",
    "special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "start_token_id = tokenizer.convert_tokens_to_ids(['<START>'])[0]\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len=70\n",
    "sm = torch.nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50261"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_token_id = tokenizer.convert_tokens_to_ids(['<END>'])[0]\n",
    "end_token_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preditction_with_beam_search(ref_text, beam_width=3, max_len=30,vocab_length=40483, end_token_id=end_token_id ):\n",
    "    \"\"\"\n",
    "    This function decodes sentences using Beam Seach. \n",
    "    It will output #sentences = beam_width. This function works on a single example.\n",
    "    \n",
    "    ref_text : string : Input sentence\n",
    "    beam_width : int : Width of the output beam\n",
    "    vocab_length : int : Size of the Vocab after adding the special tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    done = [False for i in range(beam_width)] # To track which beams are already decoded\n",
    "    stop_decode = False\n",
    "    decoded_sentences=[] # List of decoded sentences at any given time\n",
    "    \n",
    "    sm = torch.nn.Softmax(dim=-1) # To calculate Softmax over the final layer Logits\n",
    "    tokens = tokenizer.tokenize(ref_text) # Tokenize the input text\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens) # Convert tokens to ids\n",
    "    index_tokens = [indexed_tokens for i in range(beam_width)] # Replication of Input ids for all the beams\n",
    "\n",
    "    #index_tokens = [indexed_tokens for i in range(beam_width)]\n",
    "    torch_tensor = torch.tensor(index_tokens).to(device)\n",
    "    beam_indexes = [[] for i in range(beam_width)] # indexes of the current decoded beams\n",
    "    best_scoes = [0 for i in range(beam_width)] # A list of lists to store Probability values of each decoded token of best beams\n",
    "    count = 0\n",
    "    while count < max_len and not stop_decode:\n",
    "        if count == 0: # For the first step when only one sentence is availabe\n",
    "            with torch.no_grad():\n",
    "                # Calculate output probability distribution over the Vocab,\n",
    "                output = model(torch_tensor)\n",
    "                preds = sm(output[0]) #  shape = [beam_bidth, len(input_sen)+1,Vocab_length]\n",
    "            top_v, top_i = preds[:,-1,:].topk(beam_width) # Fatch top indexes and it's values\n",
    "            [beam_indexes[i].append(top_i[0][i].tolist()) for i in range(beam_width)] # Update the Beam indexes\n",
    "            # Update the best_scores, for first time just add the topk values directly\n",
    "            for i in range(beam_width):\n",
    "                best_scoes[i] = top_v[0][i].item()\n",
    "            count += 1\n",
    "        else: # After first step\n",
    "            # Prepare the current_state by concating original input and decoded beam indexes\n",
    "            current_state = torch.cat((torch_tensor, torch.tensor(beam_indexes).to(device)), dim=1)\n",
    "            # Prediction on the current state\n",
    "            with torch.no_grad():\n",
    "                outputs = model(current_state)\n",
    "                preds = sm(outputs[0])\n",
    "            # Multiply new probability predictions with corresponding best scores\n",
    "            # Total socres = beam_width * Vocab_Size\n",
    "            flatten_score = (preds[:,-1,:]*torch.tensor(best_scoes).to(device).unsqueeze(1)).view(-1)\n",
    "            # Fatch the top scores and indexes \n",
    "            vals, inx = flatten_score.topk(beam_width)\n",
    "            # best_score_inx saves the index of best beams after multiplying the probability of new prediction\n",
    "            best_scoes_inx = (inx / vocab_length).tolist()\n",
    "            best_scoes = vals.tolist()\n",
    "            # Unflatten the index \n",
    "            correct_inx = (inx % vocab_length).tolist()\n",
    "            \n",
    "            # Check if done for all the Beams\n",
    "            for i in range(beam_width):\n",
    "                if correct_inx[i] == end_token_id:\n",
    "                    done[i] = True\n",
    "            # Update the best score for each the current Beams\n",
    "            for i in range(beam_width):\n",
    "                if not done[i]:\n",
    "                    best_scoes[i] = vals.tolist()[i]\n",
    "            # Check is All the Beams are Done\n",
    "            if (sum(done) == beam_width):\n",
    "                stop_decode = True\n",
    "            # Prepapre the new beams\n",
    "            temp_lt=[0 for i in range(beam_width)]\n",
    "            for i,x in enumerate(best_scoes_inx):\n",
    "                temp_lt[i] = beam_indexes[i] + [correct_inx[i]]\n",
    "            # Update the Beam indexes\n",
    "            beam_indexes = temp_lt\n",
    "            del temp_lt\n",
    "            count += 1\n",
    "    # Decode All the beam indexes to till <END> token only and convert into sentence\n",
    "    for i in range(beam_width):\n",
    "        try:\n",
    "            end_index = beam_indexes[i].index(end_token_id )\n",
    "        except ValueError:\n",
    "            end_index = len(beam_indexes[i])\n",
    "            \n",
    "        decoded_sentences.append(tokenizer.decode(beam_indexes[i][:end_index]))\n",
    "        \n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START>'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(start_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token_id = tokenizer.convert_tokens_to_ids(['<END>'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>gold</th>\n",
       "      <th>content</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred_mp</th>\n",
       "      <th>plain_pred1</th>\n",
       "      <th>plain_pred2</th>\n",
       "      <th>plain_pred_mp</th>\n",
       "      <th>both_3l_pred1</th>\n",
       "      <th>both_3l_pred2</th>\n",
       "      <th>both_3l_pred_mp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; i recommend checking this pl...</td>\n",
       "      <td>i highly recommend checking this place out.</td>\n",
       "      <td>highly</td>\n",
       "      <td>.always actors fog fog. fog. fog.</td>\n",
       "      <td>always proposal great actors. fogalways actor...</td>\n",
       "      <td>small definitely. fantastic.always.they recom...</td>\n",
       "      <td>highly highly checking place. highly highly t...</td>\n",
       "      <td>recommenditageitageitageitageitageitageitagei...</td>\n",
       "      <td>highly highly checking place. highly highly t...</td>\n",
       "      <td>recommend this out.itage actorsigaitage love ...</td>\n",
       "      <td>definitely brushelled highly. highly recommen...</td>\n",
       "      <td>recommend this out you. main mindiquette love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; not only is there pizza, but...</td>\n",
       "      <td>not only is there pizza bad, but their custome...</td>\n",
       "      <td>bad horrible</td>\n",
       "      <td>.ues.always actors fog. actors. actors</td>\n",
       "      <td>alwaysalways actorsgreat. great actorsalways ...</td>\n",
       "      <td>.again.wrong always too rude always.att</td>\n",
       "      <td>only is pizza but customer is. Kurd not is</td>\n",
       "      <td>even there qualified, wantingetooth wanting w...</td>\n",
       "      <td>only is pizza, their service horrible horribl...</td>\n",
       "      <td>also there'); is pizza but customer is..</td>\n",
       "      <td>only is pizza mit decrease collective decreas...</td>\n",
       "      <td>also there is pizza but customer isable &lt;END&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; cool tram that has views goi...</td>\n",
       "      <td>cool tram that has great views going up or dow...</td>\n",
       "      <td>great</td>\n",
       "      <td>always actors actors actors actors actors Kur...</td>\n",
       "      <td>love great always always fog always actors Ku...</td>\n",
       "      <td>owned awesome.keep no months.cious best love</td>\n",
       "      <td>tram has views up down down theitt extended e...</td>\n",
       "      <td>cool always great Mayor MayorGood Mayor pesti...</td>\n",
       "      <td>ixed tram has going or of psburgh skyline &lt;END&gt;</td>\n",
       "      <td>great tram has views up down down the of p</td>\n",
       "      <td>cool Mayor Mayor MayorGood or hotel down pestial</td>\n",
       "      <td>cool that views going or of psburgh skyline the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; she was! &lt;START&gt;</td>\n",
       "      <td>she was fantastic!</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>always!always! adulthood always actors adulth...</td>\n",
       "      <td>love. travel.always removing. travel..</td>\n",
       "      <td>spot! delicious always great &lt;END&gt;  excellent...</td>\n",
       "      <td>was!she amazing Kurd!she amazing Kurd Kurd</td>\n",
       "      <td>she! Kurd scarthink Quebecthink awesome fogthink</td>\n",
       "      <td>was!she phenomenal &lt;END&gt;  was! &lt;END&gt;  was!</td>\n",
       "      <td>was!she was! gene! Kurd was!</td>\n",
       "      <td>she. she scar. Kurd actors gene!doors</td>\n",
       "      <td>you was!.. &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; however the food - oh the fo...</td>\n",
       "      <td>however the food - oh the food : ( - i was dis...</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>always. actors actors great. actors actors.al...</td>\n",
       "      <td>. Portlandalways.. Portlandalways.! always</td>\n",
       "      <td>unfortunately.best.worst.always. awful.</td>\n",
       "      <td>ever food oh food ( - i disappointed disappoin...</td>\n",
       "      <td>ever thearia the bad i - scar forecast disapp...</td>\n",
       "      <td>ever food oh food food disgusting ( i sgy</td>\n",
       "      <td>the - the : - food i i WAR.</td>\n",
       "      <td>love food Montgomery Montgomery i tempor temp...</td>\n",
       "      <td>them- food oh nasty food oh terrible!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <POS> <CON_START> i recommend checking this pl...   \n",
       "1  <NEG> <CON_START> not only is there pizza, but...   \n",
       "2  <POS> <CON_START> cool tram that has views goi...   \n",
       "3                 <POS> <CON_START> she was! <START>   \n",
       "4  <NEG> <CON_START> however the food - oh the fo...   \n",
       "\n",
       "                                                gold       content  \\\n",
       "0        i highly recommend checking this place out.        highly   \n",
       "1  not only is there pizza bad, but their custome...  bad horrible   \n",
       "2  cool tram that has great views going up or dow...         great   \n",
       "3                                 she was fantastic!     fantastic   \n",
       "4  however the food - oh the food : ( - i was dis...  disappointed   \n",
       "\n",
       "                                               pred1  \\\n",
       "0                  .always actors fog fog. fog. fog.   \n",
       "1             .ues.always actors fog. actors. actors   \n",
       "2   always actors actors actors actors actors Kur...   \n",
       "3   always!always! adulthood always actors adulth...   \n",
       "4   always. actors actors great. actors actors.al...   \n",
       "\n",
       "                                               pred2  \\\n",
       "0   always proposal great actors. fogalways actor...   \n",
       "1   alwaysalways actorsgreat. great actorsalways ...   \n",
       "2   love great always always fog always actors Ku...   \n",
       "3             love. travel.always removing. travel..   \n",
       "4         . Portlandalways.. Portlandalways.! always   \n",
       "\n",
       "                                             pred_mp  \\\n",
       "0   small definitely. fantastic.always.they recom...   \n",
       "1            .again.wrong always too rude always.att   \n",
       "2       owned awesome.keep no months.cious best love   \n",
       "3   spot! delicious always great <END>  excellent...   \n",
       "4            unfortunately.best.worst.always. awful.   \n",
       "\n",
       "                                         plain_pred1  \\\n",
       "0   highly highly checking place. highly highly t...   \n",
       "1         only is pizza but customer is. Kurd not is   \n",
       "2   tram has views up down down theitt extended e...   \n",
       "3         was!she amazing Kurd!she amazing Kurd Kurd   \n",
       "4  ever food oh food ( - i disappointed disappoin...   \n",
       "\n",
       "                                         plain_pred2  \\\n",
       "0   recommenditageitageitageitageitageitageitagei...   \n",
       "1   even there qualified, wantingetooth wanting w...   \n",
       "2   cool always great Mayor MayorGood Mayor pesti...   \n",
       "3   she! Kurd scarthink Quebecthink awesome fogthink   \n",
       "4   ever thearia the bad i - scar forecast disapp...   \n",
       "\n",
       "                                       plain_pred_mp  \\\n",
       "0   highly highly checking place. highly highly t...   \n",
       "1   only is pizza, their service horrible horribl...   \n",
       "2    ixed tram has going or of psburgh skyline <END>   \n",
       "3         was!she phenomenal <END>  was! <END>  was!   \n",
       "4          ever food oh food food disgusting ( i sgy   \n",
       "\n",
       "                                       both_3l_pred1  \\\n",
       "0   recommend this out.itage actorsigaitage love ...   \n",
       "1           also there'); is pizza but customer is..   \n",
       "2         great tram has views up down down the of p   \n",
       "3                       was!she was! gene! Kurd was!   \n",
       "4                        the - the : - food i i WAR.   \n",
       "\n",
       "                                       both_3l_pred2  \\\n",
       "0   definitely brushelled highly. highly recommen...   \n",
       "1   only is pizza mit decrease collective decreas...   \n",
       "2   cool Mayor Mayor MayorGood or hotel down pestial   \n",
       "3              she. she scar. Kurd actors gene!doors   \n",
       "4   love food Montgomery Montgomery i tempor temp...   \n",
       "\n",
       "                                     both_3l_pred_mp  \n",
       "0   recommend this out you. main mindiquette love...  \n",
       "1   also there is pizza but customer isable <END>...  \n",
       "2    cool that views going or of psburgh skyline the  \n",
       "3           you was!.. <PAD> <PAD> <PAD> <PAD> <PAD>  \n",
       "4             them- food oh nasty food oh terrible!!  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_gpt2_cocon = pd.read_csv('yelp_models/cocon_gpt_preds.csv')\n",
    "df_gpt2_cocon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i highly recommend checking this place out .\n",
      "not only is there pizza bad, but their customer service is horrible .\n",
      "1\n",
      "cool tram that has great views going up or down of the pittsburgh skyline .\n",
      "she was fantastic !\n",
      "however the food - oh the food : ( - i was disappointed .\n",
      "service was excellent, food is good, a great locals place .\n",
      "5\n",
      "i recommend them hands down as the best hometown dealer in the valley .\n",
      "not a fancy place but a great place to get good tasting food .\n",
      "i've had breakfast and dinner here and it has always been good .\n",
      "8\n",
      "tessaro's is my favorite burger spot in the city .\n",
      "9\n",
      "ridiculous !\n",
      "laid back, great beer, and a menu packed with variety for everyone .\n",
      "11\n",
      "our server was a delightfully charming young lady, willing to answer many questions .\n",
      "12\n",
      "this place is awesome .\n",
      "just got done with lunch and service was horrible .\n",
      "me and my boyfriend both loved the crust !\n",
      "fantastic !\n",
      "they have quite a few rental guns and some pretty friendly staff .\n",
      "great place for lunch or breakfast .\n",
      "i would definitely visit this salon again as it was relaxing and fun .\n",
      "the front desk attendants seemed to be relatively bored with helping me .\n",
      "however my engine light did not reset .\n",
      "such a great place to see the desert plant life up close .\n",
      "the dish is very flavorful, cheesy, and my all time favorite !\n",
      "23\n",
      "waited _num_ minutes before someone asked if we needed something .\n",
      "the food is bad !\n",
      "new santa fe on the floor is gorgeous .\n",
      "the chairs were filthy ( food, dirt, bird poop?  )\n",
      "27\n",
      "definitely recommend it for take out or a casual dinner atmosphere .\n",
      "that is how beautiful this place is .\n",
      "great selection of food and beverage products .\n",
      "we love this hotel .\n"
     ]
    }
   ],
   "source": [
    "rets = []\n",
    "for index,row in df_gpt2_cocon.iterrows():\n",
    "    x = str(row['gold'])\n",
    "    if(x[:-2]!=' .' or x[:-2]!=' !'):\n",
    "        x = x[:-1]+' '+x[-1]\n",
    "        print(x)\n",
    "    ret = df.loc[df['sentence'] ==x]#['defr_sent'].to_list()[0]\n",
    "    if(ret.empty):\n",
    "        print(index)\n",
    "        rets.append('<ATTR_WORDS>'+str(row['content'])+row['input'][6:])\n",
    "    else:\n",
    "        rets.append(ret['defr_sent'].to_list()[0])\n",
    "    #rets.append(ret)\n",
    "#print(len(rets), len(df_gpt2_cocon))\n",
    "df_gpt2_cocon['input_ret'] = rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>c1</th>\n",
       "      <th>a1</th>\n",
       "      <th>r1</th>\n",
       "      <th>del_sent</th>\n",
       "      <th>defr_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>excellent food .</td>\n",
       "      <td>POS</td>\n",
       "      <td>food .</td>\n",
       "      <td>excellent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;food .&lt;START&gt;excellent food .&lt;...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;excellent&lt;CON_START&gt;food .&lt;START&gt;e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>superb customer service .</td>\n",
       "      <td>POS</td>\n",
       "      <td>customer service .</td>\n",
       "      <td>superb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;customer service .&lt;START&gt;super...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;superb&lt;CON_START&gt;customer service ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they also have daily specials and ice cream wh...</td>\n",
       "      <td>POS</td>\n",
       "      <td>they also have daily specials and ice cream wh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;they also have daily specials ...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;nan&lt;CON_START&gt;they also have daily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it 's a good toasted hoagie .</td>\n",
       "      <td>POS</td>\n",
       "      <td>it 's a good toasted hoagie .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;it 's a good toasted hoagie .&lt;...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;nan&lt;CON_START&gt;it 's a good toasted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the staff is friendly .</td>\n",
       "      <td>POS</td>\n",
       "      <td>the staff is .</td>\n",
       "      <td>friendly</td>\n",
       "      <td>hostile</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;the staff is .&lt;START&gt;the staff...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;friendly&lt;CON_START&gt;the staff is .&lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment  \\\n",
       "0                                   excellent food .       POS   \n",
       "1                          superb customer service .       POS   \n",
       "2  they also have daily specials and ice cream wh...       POS   \n",
       "3                      it 's a good toasted hoagie .       POS   \n",
       "4                            the staff is friendly .       POS   \n",
       "\n",
       "                                                  c1         a1       r1  \\\n",
       "0                                             food .  excellent      NaN   \n",
       "1                                 customer service .     superb      NaN   \n",
       "2  they also have daily specials and ice cream wh...        NaN      NaN   \n",
       "3                      it 's a good toasted hoagie .        NaN      NaN   \n",
       "4                                     the staff is .   friendly  hostile   \n",
       "\n",
       "                                            del_sent  \\\n",
       "0  <POS><CON_START>food .<START>excellent food .<...   \n",
       "1  <POS><CON_START>customer service .<START>super...   \n",
       "2  <POS><CON_START>they also have daily specials ...   \n",
       "3  <POS><CON_START>it 's a good toasted hoagie .<...   \n",
       "4  <POS><CON_START>the staff is .<START>the staff...   \n",
       "\n",
       "                                           defr_sent  \n",
       "0  <ATTR_WORDS>excellent<CON_START>food .<START>e...  \n",
       "1  <ATTR_WORDS>superb<CON_START>customer service ...  \n",
       "2  <ATTR_WORDS>nan<CON_START>they also have daily...  \n",
       "3  <ATTR_WORDS>nan<CON_START>it 's a good toasted...  \n",
       "4  <ATTR_WORDS>friendly<CON_START>the staff is .<...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = df_gpt2_cocon['input_ret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt(inp,gen_len=30):\n",
    "    input_token = torch.tensor(tokenizer.encode(inp))\n",
    "    if(len(input_token.shape)<3):\n",
    "        input_token = input_token.unsqueeze(0) #batch dim\n",
    "\n",
    "    #Repeat for history TO DO\n",
    "    #implement auto regression TODO\n",
    "    input_token = input_token.to(device)\n",
    "    l = len(input_token[0])\n",
    "    for i in range(gen_len):\n",
    "        #L_alpha\n",
    "        with torch.no_grad():\n",
    "            output = model(input_token)\n",
    "\n",
    "            pred_token_logits = output[0][:,-1:] \n",
    "        #softmax\n",
    "        pred_token_prob = torch.nn.functional.softmax(pred_token_logits, dim=-1)#[:,-1,:]\n",
    "        #sample\n",
    "        pred_token = torch.multinomial(pred_token_prob[0], num_samples=1) #repeat for every elem in batch\n",
    "        #append\n",
    "        input_token = torch.cat((input_token,pred_token),1)\n",
    "        #decode\n",
    "    #pred_text = tokenizer.decode(input_token)\n",
    "    return input_token, [tokenizer.decode(i) for i in input_token[:,l:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ATTR_WORDS>highly<CON_START>i recommend checking this place out .<START>i highly recommend checking this place out .<END>\n",
      "[' highly recommend this out <END> <END>  recommend place. <END>']\n"
     ]
    }
   ],
   "source": [
    "op = generate_gpt(input_ids[0],gen_len=10)\n",
    "print(input_ids[0])\n",
    "print(op[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>gold</th>\n",
       "      <th>content</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred_mp</th>\n",
       "      <th>plain_pred1</th>\n",
       "      <th>plain_pred2</th>\n",
       "      <th>plain_pred_mp</th>\n",
       "      <th>both_3l_pred1</th>\n",
       "      <th>both_3l_pred2</th>\n",
       "      <th>both_3l_pred_mp</th>\n",
       "      <th>input_ret</th>\n",
       "      <th>plain_r_pred1</th>\n",
       "      <th>plain_r_pred2</th>\n",
       "      <th>plain_r_pred_mp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; i recommend checking this pl...</td>\n",
       "      <td>i highly recommend checking this place out.</td>\n",
       "      <td>highly</td>\n",
       "      <td>.always actors fog fog. fog. fog.</td>\n",
       "      <td>always proposal great actors. fogalways actor...</td>\n",
       "      <td>small definitely. fantastic.always.they recom...</td>\n",
       "      <td>highly highly checking place. highly highly t...</td>\n",
       "      <td>recommenditageitageitageitageitageitageitagei...</td>\n",
       "      <td>highly highly checking place. highly highly t...</td>\n",
       "      <td>recommend this out.itage actorsigaitage love ...</td>\n",
       "      <td>definitely brushelled highly. highly recommen...</td>\n",
       "      <td>recommend this out you. main mindiquette love...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;highly&lt;CON_START&gt;i recommend check...</td>\n",
       "      <td>highly recommend this out Kurd Kurd Kurd Kurd...</td>\n",
       "      <td>recommend check municip hal municip.doorstics...</td>\n",
       "      <td>highly recommend this out &lt;END&gt; &lt;END&gt;  recomm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; not only is there pizza, but...</td>\n",
       "      <td>not only is there pizza bad, but their custome...</td>\n",
       "      <td>bad horrible</td>\n",
       "      <td>.ues.always actors fog. actors. actors</td>\n",
       "      <td>alwaysalways actorsgreat. great actorsalways ...</td>\n",
       "      <td>.again.wrong always too rude always.att</td>\n",
       "      <td>only is pizza but customer is. Kurd not is</td>\n",
       "      <td>even there qualified, wantingetooth wanting w...</td>\n",
       "      <td>only is pizza, their service horrible horribl...</td>\n",
       "      <td>also there'); is pizza but customer is..</td>\n",
       "      <td>only is pizza mit decrease collective decreas...</td>\n",
       "      <td>also there is pizza but customer isable &lt;END&gt;...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;bad horrible&lt;CON_START&gt; not only i...</td>\n",
       "      <td>only there pizza but customer is. not pizza but</td>\n",
       "      <td>just is bad Bengals experiments experiments e...</td>\n",
       "      <td>only there air, their service horrible horrib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; cool tram that has views goi...</td>\n",
       "      <td>cool tram that has great views going up or dow...</td>\n",
       "      <td>great</td>\n",
       "      <td>always actors actors actors actors actors Kur...</td>\n",
       "      <td>love great always always fog always actors Ku...</td>\n",
       "      <td>owned awesome.keep no months.cious best love</td>\n",
       "      <td>tram has views up down down theitt extended e...</td>\n",
       "      <td>cool always great Mayor MayorGood Mayor pesti...</td>\n",
       "      <td>ixed tram has going or of psburgh skyline &lt;END&gt;</td>\n",
       "      <td>great tram has views up down down the of p</td>\n",
       "      <td>cool Mayor Mayor MayorGood or hotel down pestial</td>\n",
       "      <td>cool that views going or of psburgh skyline the</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;great&lt;CON_START&gt;cool tram that has...</td>\n",
       "      <td>tram has going or of psburgh.cool that</td>\n",
       "      <td>tr does views wanting down impression impress...</td>\n",
       "      <td>tram has going or of psburgh.cool.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; she was! &lt;START&gt;</td>\n",
       "      <td>she was fantastic!</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>always!always! adulthood always actors adulth...</td>\n",
       "      <td>love. travel.always removing. travel..</td>\n",
       "      <td>spot! delicious always great &lt;END&gt;  excellent...</td>\n",
       "      <td>was!she amazing Kurd!she amazing Kurd Kurd</td>\n",
       "      <td>she! Kurd scarthink Quebecthink awesome fogthink</td>\n",
       "      <td>was!she phenomenal &lt;END&gt;  was! &lt;END&gt;  was!</td>\n",
       "      <td>was!she was! gene! Kurd was!</td>\n",
       "      <td>she. she scar. Kurd actors gene!doors</td>\n",
       "      <td>you was!.. &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt;</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;fantastic&lt;CON_START&gt;she was !&lt;STAR...</td>\n",
       "      <td>was! Kurd! Kurd! Kurd Kurd Kurd Kurd</td>\n",
       "      <td>were. gene Kurddoors Kurd Keithdoorsdoorsdoors</td>\n",
       "      <td>was! &lt;END&gt;  was! &lt;END&gt;  was! &lt;END&gt;  was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; however the food - oh the fo...</td>\n",
       "      <td>however the food - oh the food : ( - i was dis...</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>always. actors actors great. actors actors.al...</td>\n",
       "      <td>. Portlandalways.. Portlandalways.! always</td>\n",
       "      <td>unfortunately.best.worst.always. awful.</td>\n",
       "      <td>ever food oh food ( - i disappointed disappoin...</td>\n",
       "      <td>ever thearia the bad i - scar forecast disapp...</td>\n",
       "      <td>ever food oh food food disgusting ( i sgy</td>\n",
       "      <td>the - the : - food i i WAR.</td>\n",
       "      <td>love food Montgomery Montgomery i tempor temp...</td>\n",
       "      <td>them- food oh nasty food oh terrible!!</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;disappointed&lt;CON_START&gt;however the...</td>\n",
       "      <td>ever food oh food ( - the : i.</td>\n",
       "      <td>everServ headquartersnm ohServ �Serv was �</td>\n",
       "      <td>ever food oh food ( - the : was i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <POS> <CON_START> i recommend checking this pl...   \n",
       "1  <NEG> <CON_START> not only is there pizza, but...   \n",
       "2  <POS> <CON_START> cool tram that has views goi...   \n",
       "3                 <POS> <CON_START> she was! <START>   \n",
       "4  <NEG> <CON_START> however the food - oh the fo...   \n",
       "\n",
       "                                                gold       content  \\\n",
       "0        i highly recommend checking this place out.        highly   \n",
       "1  not only is there pizza bad, but their custome...  bad horrible   \n",
       "2  cool tram that has great views going up or dow...         great   \n",
       "3                                 she was fantastic!     fantastic   \n",
       "4  however the food - oh the food : ( - i was dis...  disappointed   \n",
       "\n",
       "                                               pred1  \\\n",
       "0                  .always actors fog fog. fog. fog.   \n",
       "1             .ues.always actors fog. actors. actors   \n",
       "2   always actors actors actors actors actors Kur...   \n",
       "3   always!always! adulthood always actors adulth...   \n",
       "4   always. actors actors great. actors actors.al...   \n",
       "\n",
       "                                               pred2  \\\n",
       "0   always proposal great actors. fogalways actor...   \n",
       "1   alwaysalways actorsgreat. great actorsalways ...   \n",
       "2   love great always always fog always actors Ku...   \n",
       "3             love. travel.always removing. travel..   \n",
       "4         . Portlandalways.. Portlandalways.! always   \n",
       "\n",
       "                                             pred_mp  \\\n",
       "0   small definitely. fantastic.always.they recom...   \n",
       "1            .again.wrong always too rude always.att   \n",
       "2       owned awesome.keep no months.cious best love   \n",
       "3   spot! delicious always great <END>  excellent...   \n",
       "4            unfortunately.best.worst.always. awful.   \n",
       "\n",
       "                                         plain_pred1  \\\n",
       "0   highly highly checking place. highly highly t...   \n",
       "1         only is pizza but customer is. Kurd not is   \n",
       "2   tram has views up down down theitt extended e...   \n",
       "3         was!she amazing Kurd!she amazing Kurd Kurd   \n",
       "4  ever food oh food ( - i disappointed disappoin...   \n",
       "\n",
       "                                         plain_pred2  \\\n",
       "0   recommenditageitageitageitageitageitageitagei...   \n",
       "1   even there qualified, wantingetooth wanting w...   \n",
       "2   cool always great Mayor MayorGood Mayor pesti...   \n",
       "3   she! Kurd scarthink Quebecthink awesome fogthink   \n",
       "4   ever thearia the bad i - scar forecast disapp...   \n",
       "\n",
       "                                       plain_pred_mp  \\\n",
       "0   highly highly checking place. highly highly t...   \n",
       "1   only is pizza, their service horrible horribl...   \n",
       "2    ixed tram has going or of psburgh skyline <END>   \n",
       "3         was!she phenomenal <END>  was! <END>  was!   \n",
       "4          ever food oh food food disgusting ( i sgy   \n",
       "\n",
       "                                       both_3l_pred1  \\\n",
       "0   recommend this out.itage actorsigaitage love ...   \n",
       "1           also there'); is pizza but customer is..   \n",
       "2         great tram has views up down down the of p   \n",
       "3                       was!she was! gene! Kurd was!   \n",
       "4                        the - the : - food i i WAR.   \n",
       "\n",
       "                                       both_3l_pred2  \\\n",
       "0   definitely brushelled highly. highly recommen...   \n",
       "1   only is pizza mit decrease collective decreas...   \n",
       "2   cool Mayor Mayor MayorGood or hotel down pestial   \n",
       "3              she. she scar. Kurd actors gene!doors   \n",
       "4   love food Montgomery Montgomery i tempor temp...   \n",
       "\n",
       "                                     both_3l_pred_mp  \\\n",
       "0   recommend this out you. main mindiquette love...   \n",
       "1   also there is pizza but customer isable <END>...   \n",
       "2    cool that views going or of psburgh skyline the   \n",
       "3           you was!.. <PAD> <PAD> <PAD> <PAD> <PAD>   \n",
       "4             them- food oh nasty food oh terrible!!   \n",
       "\n",
       "                                           input_ret  \\\n",
       "0  <ATTR_WORDS>highly<CON_START>i recommend check...   \n",
       "1  <ATTR_WORDS>bad horrible<CON_START> not only i...   \n",
       "2  <ATTR_WORDS>great<CON_START>cool tram that has...   \n",
       "3  <ATTR_WORDS>fantastic<CON_START>she was !<STAR...   \n",
       "4  <ATTR_WORDS>disappointed<CON_START>however the...   \n",
       "\n",
       "                                       plain_r_pred1  \\\n",
       "0   highly recommend this out Kurd Kurd Kurd Kurd...   \n",
       "1    only there pizza but customer is. not pizza but   \n",
       "2             tram has going or of psburgh.cool that   \n",
       "3               was! Kurd! Kurd! Kurd Kurd Kurd Kurd   \n",
       "4                     ever food oh food ( - the : i.   \n",
       "\n",
       "                                       plain_r_pred2  \\\n",
       "0   recommend check municip hal municip.doorstics...   \n",
       "1   just is bad Bengals experiments experiments e...   \n",
       "2   tr does views wanting down impression impress...   \n",
       "3     were. gene Kurddoors Kurd Keithdoorsdoorsdoors   \n",
       "4         everServ headquartersnm ohServ �Serv was �   \n",
       "\n",
       "                                     plain_r_pred_mp  \n",
       "0   highly recommend this out <END> <END>  recomm...  \n",
       "1   only there air, their service horrible horrib...  \n",
       "2                 tram has going or of psburgh.cool.  \n",
       "3            was! <END>  was! <END>  was! <END>  was  \n",
       "4                  ever food oh food ( - the : was i  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "outp = []\n",
    "outs1 = []\n",
    "outs2 = []\n",
    "#tqdm_bar = tqdm(eval_dataloader, desc=\"batch iteration\")\n",
    "for i in range(len(input_ids)):\n",
    "    input_ = input_ids[i]\n",
    "    op=preditction_with_beam_search(input_,2, 10)\n",
    "    outs1.append(op[0])\n",
    "    outs2.append(op[1])\n",
    "    op = generate_gpt(input_,gen_len=10)\n",
    "    outp.append(op[1][0])\n",
    "\n",
    "\n",
    "df_gpt2_cocon['plain_r_pred1'] = outs1\n",
    "df_gpt2_cocon['plain_r_pred2'] = outs2\n",
    "df_gpt2_cocon['plain_r_pred_mp'] = outp\n",
    "\n",
    "    #df_gpt2.to_csv('yelp_model/plain_gpt.csv', index=False)\n",
    "df_gpt2_cocon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>gold</th>\n",
       "      <th>content</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred_mp</th>\n",
       "      <th>plain_pred1</th>\n",
       "      <th>plain_pred2</th>\n",
       "      <th>plain_pred_mp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; i recommend checking this pl...</td>\n",
       "      <td>i highly recommend checking this place out.</td>\n",
       "      <td>highly</td>\n",
       "      <td>.always actors fog fog. fog. fog.</td>\n",
       "      <td>always proposal great actors. fogalways actor...</td>\n",
       "      <td>small definitely. fantastic.always.they recom...</td>\n",
       "      <td>highly highly checking place. highly highly t...</td>\n",
       "      <td>recommenditageitageitageitageitageitageitagei...</td>\n",
       "      <td>highly highly checking place. highly highly t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; not only is there pizza, but...</td>\n",
       "      <td>not only is there pizza bad, but their custome...</td>\n",
       "      <td>bad horrible</td>\n",
       "      <td>.ues.always actors fog. actors. actors</td>\n",
       "      <td>alwaysalways actorsgreat. great actorsalways ...</td>\n",
       "      <td>.again.wrong always too rude always.att</td>\n",
       "      <td>only is pizza but customer is. Kurd not is</td>\n",
       "      <td>even there qualified, wantingetooth wanting w...</td>\n",
       "      <td>only is pizza, their service horrible horribl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; cool tram that has views goi...</td>\n",
       "      <td>cool tram that has great views going up or dow...</td>\n",
       "      <td>great</td>\n",
       "      <td>always actors actors actors actors actors Kur...</td>\n",
       "      <td>love great always always fog always actors Ku...</td>\n",
       "      <td>owned awesome.keep no months.cious best love</td>\n",
       "      <td>tram has views up down down theitt extended e...</td>\n",
       "      <td>cool always great Mayor MayorGood Mayor pesti...</td>\n",
       "      <td>ixed tram has going or of psburgh skyline &lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; she was! &lt;START&gt;</td>\n",
       "      <td>she was fantastic!</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>always!always! adulthood always actors adulth...</td>\n",
       "      <td>love. travel.always removing. travel..</td>\n",
       "      <td>spot! delicious always great &lt;END&gt;  excellent...</td>\n",
       "      <td>was!she amazing Kurd!she amazing Kurd Kurd</td>\n",
       "      <td>she! Kurd scarthink Quebecthink awesome fogthink</td>\n",
       "      <td>was!she phenomenal &lt;END&gt;  was! &lt;END&gt;  was!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; however the food - oh the fo...</td>\n",
       "      <td>however the food - oh the food : ( - i was dis...</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>always. actors actors great. actors actors.al...</td>\n",
       "      <td>. Portlandalways.. Portlandalways.! always</td>\n",
       "      <td>unfortunately.best.worst.always. awful.</td>\n",
       "      <td>ever food oh food ( - i disappointed disappoin...</td>\n",
       "      <td>ever thearia the bad i - scar forecast disapp...</td>\n",
       "      <td>ever food oh food food disgusting ( i sgy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <POS> <CON_START> i recommend checking this pl...   \n",
       "1  <NEG> <CON_START> not only is there pizza, but...   \n",
       "2  <POS> <CON_START> cool tram that has views goi...   \n",
       "3                 <POS> <CON_START> she was! <START>   \n",
       "4  <NEG> <CON_START> however the food - oh the fo...   \n",
       "\n",
       "                                                gold       content  \\\n",
       "0        i highly recommend checking this place out.        highly   \n",
       "1  not only is there pizza bad, but their custome...  bad horrible   \n",
       "2  cool tram that has great views going up or dow...         great   \n",
       "3                                 she was fantastic!     fantastic   \n",
       "4  however the food - oh the food : ( - i was dis...  disappointed   \n",
       "\n",
       "                                               pred1  \\\n",
       "0                  .always actors fog fog. fog. fog.   \n",
       "1             .ues.always actors fog. actors. actors   \n",
       "2   always actors actors actors actors actors Kur...   \n",
       "3   always!always! adulthood always actors adulth...   \n",
       "4   always. actors actors great. actors actors.al...   \n",
       "\n",
       "                                               pred2  \\\n",
       "0   always proposal great actors. fogalways actor...   \n",
       "1   alwaysalways actorsgreat. great actorsalways ...   \n",
       "2   love great always always fog always actors Ku...   \n",
       "3             love. travel.always removing. travel..   \n",
       "4         . Portlandalways.. Portlandalways.! always   \n",
       "\n",
       "                                             pred_mp  \\\n",
       "0   small definitely. fantastic.always.they recom...   \n",
       "1            .again.wrong always too rude always.att   \n",
       "2       owned awesome.keep no months.cious best love   \n",
       "3   spot! delicious always great <END>  excellent...   \n",
       "4            unfortunately.best.worst.always. awful.   \n",
       "\n",
       "                                         plain_pred1  \\\n",
       "0   highly highly checking place. highly highly t...   \n",
       "1         only is pizza but customer is. Kurd not is   \n",
       "2   tram has views up down down theitt extended e...   \n",
       "3         was!she amazing Kurd!she amazing Kurd Kurd   \n",
       "4  ever food oh food ( - i disappointed disappoin...   \n",
       "\n",
       "                                         plain_pred2  \\\n",
       "0   recommenditageitageitageitageitageitageitagei...   \n",
       "1   even there qualified, wantingetooth wanting w...   \n",
       "2   cool always great Mayor MayorGood Mayor pesti...   \n",
       "3   she! Kurd scarthink Quebecthink awesome fogthink   \n",
       "4   ever thearia the bad i - scar forecast disapp...   \n",
       "\n",
       "                                       plain_pred_mp  \n",
       "0   highly highly checking place. highly highly t...  \n",
       "1   only is pizza, their service horrible horribl...  \n",
       "2    ixed tram has going or of psburgh skyline <END>  \n",
       "3         was!she phenomenal <END>  was! <END>  was!  \n",
       "4          ever food oh food food disgusting ( i sgy  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpt2_cocon.to_csv('yelp_models/cocon_gpt_preds.csv', index=False)\n",
    "df_gpt2_cocon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>gold</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; we where very and won't be b...</td>\n",
       "      <td>we where very disappointed and won't be back.</td>\n",
       "      <td>where disappointed very and dreams dreams be....</td>\n",
       "      <td>complained rude psychologicalcharge't be intr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; the company's service has be...</td>\n",
       "      <td>the company's service has been very prompt and...</td>\n",
       "      <td>company service always Warning always decreas...</td>\n",
       "      <td>service company been very decrease prompt hor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; they are also super. &lt;START&gt;</td>\n",
       "      <td>they are also super slow.</td>\n",
       "      <td>are super. Kurd they also super. Kurd they</td>\n",
       "      <td>ude Fedriendancouver Fed Fed Fed fog Quin588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; and they had service! &lt;START&gt;</td>\n",
       "      <td>and they had great service!</td>\n",
       "      <td>they great great great fog!and had service Kurd</td>\n",
       "      <td>and excellent margin service! Article decreas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; my sushi in las vegas. &lt;START&gt;</td>\n",
       "      <td>my favorite sushi in las vegas.</td>\n",
       "      <td>favorite spot lasgas laspertydictdict sets sets</td>\n",
       "      <td>initely sushipertypertyperty.. my my best</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <NEG> <CON_START> we where very and won't be b...   \n",
       "1  <POS> <CON_START> the company's service has be...   \n",
       "2     <NEG> <CON_START> they are also super. <START>   \n",
       "3    <POS> <CON_START> and they had service! <START>   \n",
       "4   <POS> <CON_START> my sushi in las vegas. <START>   \n",
       "\n",
       "                                                gold  \\\n",
       "0      we where very disappointed and won't be back.   \n",
       "1  the company's service has been very prompt and...   \n",
       "2                          they are also super slow.   \n",
       "3                        and they had great service!   \n",
       "4                    my favorite sushi in las vegas.   \n",
       "\n",
       "                                               pred1  \\\n",
       "0   where disappointed very and dreams dreams be....   \n",
       "1   company service always Warning always decreas...   \n",
       "2         are super. Kurd they also super. Kurd they   \n",
       "3    they great great great fog!and had service Kurd   \n",
       "4    favorite spot lasgas laspertydictdict sets sets   \n",
       "\n",
       "                                               pred2  \n",
       "0   complained rude psychologicalcharge't be intr...  \n",
       "1   service company been very decrease prompt hor...  \n",
       "2       ude Fedriendancouver Fed Fed Fed fog Quin588  \n",
       "3   and excellent margin service! Article decreas...  \n",
       "4          initely sushipertypertyperty.. my my best  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpt2.to_csv('yelp_models/plain_gpt_preds.csv', index=False)\n",
    "df_gpt2.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "tst_transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
