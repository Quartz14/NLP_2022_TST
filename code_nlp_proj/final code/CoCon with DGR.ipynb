{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:58:00.983332Z",
     "iopub.status.busy": "2022-04-17T02:58:00.98303Z",
     "iopub.status.idle": "2022-04-17T02:58:08.09453Z",
     "shell.execute_reply": "2022-04-17T02:58:08.093737Z",
     "shell.execute_reply.started": "2022-04-17T02:58:00.9833Z"
    },
    "id": "ZcfYGRvnRUvW",
    "outputId": "31de46a3-5623-4ca4-e572-3f63f9a0200b"
   },
   "outputs": [],
   "source": [
    "from transformers_cocon import (AdamW,GPT2Config,GPT2LMHeadModel,GPT2Tokenizer,\n",
    "                                         PreTrainedModel,PreTrainedTokenizer,get_linear_schedule_with_warmup,\n",
    "                                         CoconBlock,HDiscriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:57:56.293109Z",
     "iopub.status.busy": "2022-04-17T02:57:56.292466Z",
     "iopub.status.idle": "2022-04-17T02:57:56.297199Z",
     "shell.execute_reply": "2022-04-17T02:57:56.296326Z",
     "shell.execute_reply.started": "2022-04-17T02:57:56.293069Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../input/yelp-dgr/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:58:19.70593Z",
     "iopub.status.busy": "2022-04-17T02:58:19.705592Z",
     "iopub.status.idle": "2022-04-17T02:58:19.782586Z",
     "shell.execute_reply": "2022-04-17T02:58:19.781838Z",
     "shell.execute_reply.started": "2022-04-17T02:58:19.705892Z"
    },
    "id": "8cSRDouXJl1X",
    "outputId": "8597666b-ab75-4f89-e2df-e7183722f2f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:58:22.178501Z",
     "iopub.status.busy": "2022-04-17T02:58:22.178225Z",
     "iopub.status.idle": "2022-04-17T02:58:22.184646Z",
     "shell.execute_reply": "2022-04-17T02:58:22.18355Z",
     "shell.execute_reply.started": "2022-04-17T02:58:22.178453Z"
    },
    "id": "gvieVnQBJzEO",
    "outputId": "53048095-6d84-4f96-8603-1dadf8ab1e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:58:24.399626Z",
     "iopub.status.busy": "2022-04-17T02:58:24.39884Z",
     "iopub.status.idle": "2022-04-17T02:58:24.408222Z",
     "shell.execute_reply": "2022-04-17T02:58:24.407315Z",
     "shell.execute_reply.started": "2022-04-17T02:58:24.399581Z"
    },
    "id": "XYw8QPQvXiEd"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "#import wget\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed import get_rank, get_world_size\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.modules import ModuleList\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:58:26.257612Z",
     "iopub.status.busy": "2022-04-17T02:58:26.257314Z",
     "iopub.status.idle": "2022-04-17T02:58:54.862188Z",
     "shell.execute_reply": "2022-04-17T02:58:54.86142Z",
     "shell.execute_reply.started": "2022-04-17T02:58:26.25758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:58:54.864227Z",
     "iopub.status.busy": "2022-04-17T02:58:54.863963Z",
     "iopub.status.idle": "2022-04-17T02:58:55.607252Z",
     "shell.execute_reply": "2022-04-17T02:58:55.606515Z",
     "shell.execute_reply.started": "2022-04-17T02:58:54.864191Z"
    },
    "id": "tXTPbbZjXiEx",
    "outputId": "7546f4f6-19a1-4219-ad6a-dfd755c19d5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50264"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = ['<POS>', '<NEG>','<CON_START>','<START>','<END>','<ATTR_WORDS>','<PAD>']\n",
    "special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "start_token_id = tokenizer.convert_tokens_to_ids(['<START>'])[0]\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(['<PAD>'])[0]\n",
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:58:55.609131Z",
     "iopub.status.busy": "2022-04-17T02:58:55.608656Z",
     "iopub.status.idle": "2022-04-17T02:58:56.763367Z",
     "shell.execute_reply": "2022-04-17T02:58:56.762405Z",
     "shell.execute_reply.started": "2022-04-17T02:58:55.609087Z"
    },
    "id": "CXbf-366XiE0"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    #print(param)\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50263, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50263, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict = torch.load('yelp_models/pytorch_model_zero_grad_1.bin', map_location='cpu')\n",
    "model.load_state_dict(model_state_dict,strict=False) #transformer.h.'0'.attn.masked_bias not loaded as abset in older version of transformers library which is used here\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:59:01.062271Z",
     "iopub.status.busy": "2022-04-17T02:59:01.061775Z",
     "iopub.status.idle": "2022-04-17T02:59:01.068494Z",
     "shell.execute_reply": "2022-04-17T02:59:01.067673Z",
     "shell.execute_reply.started": "2022-04-17T02:59:01.062233Z"
    },
    "id": "3atiPh6OXiE6"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:59:01.070108Z",
     "iopub.status.busy": "2022-04-17T02:59:01.069749Z",
     "iopub.status.idle": "2022-04-17T02:59:01.07914Z",
     "shell.execute_reply": "2022-04-17T02:59:01.078301Z",
     "shell.execute_reply.started": "2022-04-17T02:59:01.070068Z"
    },
    "id": "FVd6baWDXiE8"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:59:01.082135Z",
     "iopub.status.busy": "2022-04-17T02:59:01.081932Z",
     "iopub.status.idle": "2022-04-17T02:59:02.090011Z",
     "shell.execute_reply": "2022-04-17T02:59:02.089249Z",
     "shell.execute_reply.started": "2022-04-17T02:59:01.082112Z"
    },
    "id": "qIOMTs7_XiFF"
   },
   "outputs": [],
   "source": [
    "config = GPT2Config.from_pretrained(\"gpt2\", cache_dir='saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:59:02.92799Z",
     "iopub.status.busy": "2022-04-17T02:59:02.927431Z",
     "iopub.status.idle": "2022-04-17T02:59:03.076084Z",
     "shell.execute_reply": "2022-04-17T02:59:03.075337Z",
     "shell.execute_reply.started": "2022-04-17T02:59:02.927952Z"
    },
    "id": "P0b3pCtCXiFF"
   },
   "outputs": [],
   "source": [
    "cocon_block = CoconBlock(config.n_ctx, config, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:59:03.440177Z",
     "iopub.status.busy": "2022-04-17T02:59:03.439924Z",
     "iopub.status.idle": "2022-04-17T02:59:03.457471Z",
     "shell.execute_reply": "2022-04-17T02:59:03.456793Z",
     "shell.execute_reply.started": "2022-04-17T02:59:03.440148Z"
    },
    "id": "9Gd8JKFzKlxW"
   },
   "outputs": [],
   "source": [
    "cocon_block = cocon_block.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ofvV--YlBt0X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cocon_block.load_state_dict(torch.load('yelp_models/cocon_dgr_5.pt', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:59:06.413211Z",
     "iopub.status.busy": "2022-04-17T02:59:06.412709Z",
     "iopub.status.idle": "2022-04-17T02:59:06.421548Z",
     "shell.execute_reply": "2022-04-17T02:59:06.420357Z",
     "shell.execute_reply.started": "2022-04-17T02:59:06.413168Z"
    },
    "id": "lB4dKFFiXiFG",
    "outputId": "835dd28e-2542-48cb-d639-2120cb3b2c41"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "cocon_block.eval()\n",
    "cocon_block.zero_grad()\n",
    "#cocon_block.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_UolW1HXiFH"
   },
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:59:29.830213Z",
     "iopub.status.busy": "2022-04-17T02:59:29.829948Z",
     "iopub.status.idle": "2022-04-17T02:59:32.826065Z",
     "shell.execute_reply": "2022-04-17T02:59:32.825352Z",
     "shell.execute_reply.started": "2022-04-17T02:59:29.830183Z"
    },
    "id": "iETWemBVXiFK",
    "outputId": "5b819de6-7ce9-4692-8812-58675df2ea88"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>c1</th>\n",
       "      <th>a1</th>\n",
       "      <th>r1</th>\n",
       "      <th>del_sent</th>\n",
       "      <th>defr_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>excellent food .</td>\n",
       "      <td>POS</td>\n",
       "      <td>food .</td>\n",
       "      <td>excellent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;food .&lt;START&gt;excellent food .&lt;...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;excellent&lt;CON_START&gt;food .&lt;START&gt;e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>superb customer service .</td>\n",
       "      <td>POS</td>\n",
       "      <td>customer service .</td>\n",
       "      <td>superb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;customer service .&lt;START&gt;super...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;superb&lt;CON_START&gt;customer service ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they also have daily specials and ice cream wh...</td>\n",
       "      <td>POS</td>\n",
       "      <td>they also have daily specials and ice cream wh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;they also have daily specials ...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;nan&lt;CON_START&gt;they also have daily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it 's a good toasted hoagie .</td>\n",
       "      <td>POS</td>\n",
       "      <td>it 's a good toasted hoagie .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;it 's a good toasted hoagie .&lt;...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;nan&lt;CON_START&gt;it 's a good toasted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the staff is friendly .</td>\n",
       "      <td>POS</td>\n",
       "      <td>the staff is .</td>\n",
       "      <td>friendly</td>\n",
       "      <td>hostile</td>\n",
       "      <td>&lt;POS&gt;&lt;CON_START&gt;the staff is .&lt;START&gt;the staff...</td>\n",
       "      <td>&lt;ATTR_WORDS&gt;friendly&lt;CON_START&gt;the staff is .&lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment  \\\n",
       "0                                   excellent food .       POS   \n",
       "1                          superb customer service .       POS   \n",
       "2  they also have daily specials and ice cream wh...       POS   \n",
       "3                      it 's a good toasted hoagie .       POS   \n",
       "4                            the staff is friendly .       POS   \n",
       "\n",
       "                                                  c1         a1       r1  \\\n",
       "0                                             food .  excellent      NaN   \n",
       "1                                 customer service .     superb      NaN   \n",
       "2  they also have daily specials and ice cream wh...        NaN      NaN   \n",
       "3                      it 's a good toasted hoagie .        NaN      NaN   \n",
       "4                                     the staff is .   friendly  hostile   \n",
       "\n",
       "                                            del_sent  \\\n",
       "0  <POS><CON_START>food .<START>excellent food .<...   \n",
       "1  <POS><CON_START>customer service .<START>super...   \n",
       "2  <POS><CON_START>they also have daily specials ...   \n",
       "3  <POS><CON_START>it 's a good toasted hoagie .<...   \n",
       "4  <POS><CON_START>the staff is .<START>the staff...   \n",
       "\n",
       "                                           defr_sent  \n",
       "0  <ATTR_WORDS>excellent<CON_START>food .<START>e...  \n",
       "1  <ATTR_WORDS>superb<CON_START>customer service ...  \n",
       "2  <ATTR_WORDS>nan<CON_START>they also have daily...  \n",
       "3  <ATTR_WORDS>nan<CON_START>it 's a good toasted...  \n",
       "4  <ATTR_WORDS>friendly<CON_START>the staff is .<...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_csv('../input/yelp-dgr/clean_text_unigrams_r.csv')\n",
    "df = pd.read_csv('clean_text_unigrams_r.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:59:37.645479Z",
     "iopub.status.busy": "2022-04-17T02:59:37.64521Z",
     "iopub.status.idle": "2022-04-17T02:59:37.735785Z",
     "shell.execute_reply": "2022-04-17T02:59:37.734913Z",
     "shell.execute_reply.started": "2022-04-17T02:59:37.645449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254591"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)#443261\n",
    "df_ = df.dropna(subset=['a1'])\n",
    "len(df_)#254591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "POS          266042\n",
       "NEG          177219\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts(subset=['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "POS          175768\n",
       "NEG           78823\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.value_counts(subset=['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(train_ds[0]), tokenizer.decode(eval_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:59:46.116137Z",
     "iopub.status.busy": "2022-04-17T02:59:46.115556Z",
     "iopub.status.idle": "2022-04-17T02:59:46.259007Z",
     "shell.execute_reply": "2022-04-17T02:59:46.258233Z",
     "shell.execute_reply.started": "2022-04-17T02:59:46.116098Z"
    },
    "id": "sCCFYM7yTbIU"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "inputs = df_['del_sent'].to_list()\n",
    "y = df_['a1'].to_list()\n",
    "train_ds, eval_ds, train_attr, eval_attr = train_test_split(inputs, y, test_size=0.33, random_state=42)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:59:50.183188Z",
     "iopub.status.busy": "2022-04-17T02:59:50.182476Z",
     "iopub.status.idle": "2022-04-17T02:59:50.191085Z",
     "shell.execute_reply": "2022-04-17T02:59:50.190132Z",
     "shell.execute_reply.started": "2022-04-17T02:59:50.183152Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_ds)+len(eval_ds), len(df_),len(train_attr), len(eval_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:59:53.046243Z",
     "iopub.status.busy": "2022-04-17T02:59:53.045778Z",
     "iopub.status.idle": "2022-04-17T02:59:53.051408Z",
     "shell.execute_reply": "2022-04-17T02:59:53.050545Z",
     "shell.execute_reply.started": "2022-04-17T02:59:53.046206Z"
    },
    "id": "Aig4BbPGt-xM",
    "outputId": "4934309b-16ac-45c5-e1df-d746aab94e75"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_encode(lines):\n",
    "        '''\n",
    "        This method tokenizes the input data and encodes it using the OpenAIGPTTokenizer\n",
    "        :param file_path: Path of the input file, dtype: str\n",
    "        :return: encoded dataset  dtype: list\n",
    "        '''\n",
    "        tokenized_dataset = lines\n",
    "        for i, line in enumerate(tqdm(lines)):\n",
    "            token = tokenizer.tokenize(line)[:512]\n",
    "            tokenized_dataset[i] = tokenizer.convert_tokens_to_ids(token)\n",
    "        return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T02:59:57.875971Z",
     "iopub.status.busy": "2022-04-17T02:59:57.874952Z",
     "iopub.status.idle": "2022-04-17T03:00:44.468881Z",
     "shell.execute_reply": "2022-04-17T03:00:44.467962Z",
     "shell.execute_reply.started": "2022-04-17T02:59:57.875933Z"
    },
    "id": "ZnoMKbrxXiFL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 170575/170575 [00:27<00:00, 6274.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 84016/84016 [00:13<00:00, 6455.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 170575/170575 [00:07<00:00, 24148.32it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 84016/84016 [00:03<00:00, 22196.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input length  51\n",
      "input length  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tokenize_and_encode(train_ds)\n",
    "eval_dataset = tokenize_and_encode(eval_ds)\n",
    "train_attr = tokenize_and_encode(train_attr)\n",
    "eval_attr = tokenize_and_encode(eval_attr)\n",
    "input_length = max(max(len(t) for t in train_dataset), max(len(q) for q in eval_dataset))\n",
    "print('input length ', input_length)\n",
    "input_length = min(input_length, 85)\n",
    "print('input length ', input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T03:00:44.471028Z",
     "iopub.status.busy": "2022-04-17T03:00:44.470683Z",
     "iopub.status.idle": "2022-04-17T03:00:44.482109Z",
     "shell.execute_reply": "2022-04-17T03:00:44.481162Z",
     "shell.execute_reply.started": "2022-04-17T03:00:44.470987Z"
    },
    "id": "A_qOqBVYIYaO"
   },
   "outputs": [],
   "source": [
    "def pre_process_dataset(encoded_dataset, encoded_attr, input_length, start_token_id,pad_token_id=pad_token_id):\n",
    "        \"\"\"\n",
    "        This method is to create torch tensor of input ids and lm labels\n",
    "        :param encoded_dataset: Input dataset, dtype: list\n",
    "        :param input_length: Maximum length of sentence from training and eval dataset, dtype: int\n",
    "        :param start_token_id: id of the '<START>' token, dtype: int\n",
    "        :return: torch.tensor of size [len(encoded_dataset), 2]\n",
    "        \"\"\"\n",
    "\n",
    "        n_batch = len(encoded_dataset)\n",
    "        attr_len = max([len(i) for i in encoded_attr])\n",
    "        attr = np.full(shape=(n_batch, attr_len), fill_value=pad_token_id,  dtype=np.int64)\n",
    "        input_ids = np.full(shape=(n_batch, input_length), fill_value=pad_token_id,  dtype=np.int64)\n",
    "        lm_labels = np.full(shape=(n_batch, input_length), fill_value=-100, dtype=np.int64)\n",
    "\n",
    "        for i, tokens in enumerate(encoded_dataset):\n",
    "            try:\n",
    "                #tokens = tokens[:input_length]\n",
    "                start_id_index = tokens.index(start_token_id)\n",
    "                input_ids[i, :len(tokens)] = tokens\n",
    "                start_id_index = tokens.index(start_token_id)\n",
    "                lm_labels[i, start_id_index : len(tokens)-1] = tokens[start_id_index + 1: len(tokens)]\n",
    "                attr[i, :len(encoded_attr[i])] = encoded_attr[i]\n",
    "                # LM loss calculate only for tokens after <START> token in the sentence\n",
    "                #lm_labels[i, :len(tokens)-1] = tokens[1:]\n",
    "            except ValueError:\n",
    "                print(\"Index {} doesn't have start token\".format(i))\n",
    "\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        lm_labels = torch.tensor(lm_labels)\n",
    "        attr = torch.tensor(attr)\n",
    "        tensor_dataset = (input_ids, attr, lm_labels)\n",
    "        #tensor_dataset.append(torch.tensor(d) for d in all_inputs)\n",
    "\n",
    "        return tensor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T03:00:44.484011Z",
     "iopub.status.busy": "2022-04-17T03:00:44.483748Z",
     "iopub.status.idle": "2022-04-17T03:00:44.495595Z",
     "shell.execute_reply": "2022-04-17T03:00:44.494824Z",
     "shell.execute_reply.started": "2022-04-17T03:00:44.483974Z"
    },
    "id": "hQKOBjyWYoog"
   },
   "outputs": [],
   "source": [
    "train_batch_size = 32\n",
    "num_train_epochs = 2\n",
    "learning_rate = 6.25e-5\n",
    "warmup_proportion = 0.002\n",
    "max_grad_norm = 1\n",
    "weight_decay = 0.01\n",
    "n_gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T03:00:44.497974Z",
     "iopub.status.busy": "2022-04-17T03:00:44.497709Z",
     "iopub.status.idle": "2022-04-17T03:00:44.504337Z",
     "shell.execute_reply": "2022-04-17T03:00:44.50353Z",
     "shell.execute_reply.started": "2022-04-17T03:00:44.49794Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T03:00:44.506228Z",
     "iopub.status.busy": "2022-04-17T03:00:44.506031Z",
     "iopub.status.idle": "2022-04-17T03:00:46.88793Z",
     "shell.execute_reply": "2022-04-17T03:00:46.887179Z",
     "shell.execute_reply.started": "2022-04-17T03:00:44.506205Z"
    },
    "id": "aIjus08-XiFT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Example Input ids= tensor([50258, 50259,    72,   326,  1312,   373, 24053,   764, 50260,    72,\n",
      "         5100,   326,  1312,   373, 24053,   764, 50261, 50262, 50262, 50262,\n",
      "        50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262,\n",
      "        50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262,\n",
      "        50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262,\n",
      "        50262])\n",
      "Training Example attribute ids = tensor([45956,   515, 50262, 50262, 50262, 50262, 50262, 50262, 50262, 50262])\n",
      "Training Example Language Modeling ids = tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,    72,  5100,\n",
      "          326,  1312,   373, 24053,   764, 50261,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100])\n"
     ]
    }
   ],
   "source": [
    "train_tensor_dataset = pre_process_dataset(train_dataset,train_attr, input_length, start_token_id=start_token_id)\n",
    "eval_tensor_dataset = pre_process_dataset(eval_dataset, eval_attr, input_length, start_token_id=start_token_id)\n",
    "\n",
    "print(\"Training Example Input ids= {}\".format(train_tensor_dataset[0][0]))\n",
    "print(\"Training Example attribute ids = {}\".format(train_tensor_dataset[1][0]))\n",
    "print(\"Training Example Language Modeling ids = {}\".format(train_tensor_dataset[2][0]))\n",
    "\n",
    "\n",
    "train_data = TensorDataset(*train_tensor_dataset)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "eval_data = TensorDataset(*eval_tensor_dataset)\n",
    "eval_sampler = RandomSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=train_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T03:00:46.889587Z",
     "iopub.status.busy": "2022-04-17T03:00:46.88937Z",
     "iopub.status.idle": "2022-04-17T03:00:46.921351Z",
     "shell.execute_reply": "2022-04-17T03:00:46.920653Z",
     "shell.execute_reply.started": "2022-04-17T03:00:46.889557Z"
    },
    "id": "If5zlgqxXiFX"
   },
   "outputs": [],
   "source": [
    "#Commented part corresponds to Cycle loss code\n",
    "\n",
    "def train_cocon(train_dataset=train_dataloader, model=model, tokenizer=tokenizer, cocon_block=cocon_block):\n",
    "    t_total = len(train_dataloader) * num_train_epochs\n",
    "    \n",
    "    learning_rate = 5e-5\n",
    "    adam_epsilon = 1e-8\n",
    "    #t_total = len(train_dataloader) #// args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    cocon_block_optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in cocon_block.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\"params\": [p for n, p in cocon_block.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "    cocon_block_optimizer = AdamW(cocon_block_optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "    cocon_block_scheduler = get_linear_schedule_with_warmup(\n",
    "        cocon_block_optimizer, num_warmup_steps=0, num_training_steps=t_total)\n",
    "    \n",
    "    #model, cocon_block, cocon_block_optimizer, train_dataloader = accelerator.prepare(model, cocon_block, cocon_block_optimizer, train_dataloader)\n",
    "    #cocon_block_optimizer.to(device)\n",
    "    #train_dataloader.to(device)\n",
    "    \n",
    "    \n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    #total_loss = 0\n",
    "    lambda_self = 1\n",
    "    lambda_null = 1\n",
    "    lambda_cycle = 1\n",
    "    #epoch_max_steps = 10\n",
    "    #max_steps = 100\n",
    "    #hs_len = 0 \n",
    "    #cs_len = 20 \n",
    "    #tis_len = 20 \n",
    "    \n",
    "    #model.zero_grad()\n",
    "    train_iterator = trange(epochs_trained, num_train_epochs, desc=\"Epoch\")\n",
    "    for epoch_ind in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            \n",
    "            inputs, content, lm_labels = batch\n",
    "            #print(inputs.shape, content.shape, lm_labels.shape)\n",
    "            \n",
    "            #if(inputs.shape[1]<hs_len):\n",
    "            #    continue\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            lm_labels = lm_labels.to(device)\n",
    "            content = content.to(device)\n",
    "\n",
    "            #hs_tis_split_ind = random.randint(0,2)\n",
    "            #hs_len = 10 + hs_tis_split_ind\n",
    "            #cs_len = 20 - hs_tis_split_ind\n",
    "            #tis_len = 20 - hs_tis_split_ind\n",
    "            \n",
    "            #lm_labels = lm_labels[:, :hs_len+tis_len]\n",
    "            #original_context_seq = inputs[:, hs_len:hs_len+cs_len]\n",
    "            #original_history_seq = inputs[:, :hs_len]\n",
    "            #original_transform_input_seq = inputs[:, hs_len:hs_len+tis_len]\n",
    "            \n",
    "            #other_sample_inputs = torch.cat([inputs[-1:], inputs[:-1]], dim=0)\n",
    "            #other_sample_lm_labels = other_sample_inputs[:, :hs_len+tis_len]\n",
    "\n",
    "            #other_sample_context_seq = other_sample_inputs[:, hs_len:hs_len+cs_len]\n",
    "            #other_sample_history_seq = other_sample_inputs[:, :hs_len]\n",
    "            #other_sample_transform_input_seq = other_sample_inputs[:, hs_len:hs_len+tis_len]\n",
    "            \n",
    "            #model.eval()\n",
    "            #cocon_block.train()\n",
    "            model.zero_grad()\n",
    "            cocon_block.zero_grad()\n",
    "            \n",
    "            #SELF LOSS -> Modified to sentiment loss\n",
    "            #forward pass L_alpha(entire_input) #NO AR\n",
    "            #forward pass L_alpha(original_content)\n",
    "            with torch.no_grad():\n",
    "                hidden_states = model(inputs, output_after_block_ind=5)[0] \n",
    "                context_seq_hidden_states =  model(content, output_after_block_ind=5)[0] \n",
    "           \n",
    "            original_hidden_states = hidden_states.to(device)\n",
    "            #print(original_hidden_states.shape)\n",
    "            #original_history_seq_hidden_states = original_hidden_states[:, :hs_len].to(device)\n",
    "            #original_transform_input_seq_hidden_states = original_hidden_states[:, hs_len:hs_len+tis_len].to(device)\n",
    "            original_context_seq_hidden_states = context_seq_hidden_states.to(device)\n",
    "            #print(original_context_seq_hidden_states.shape)\n",
    "    \n",
    "            #other_sample_hidden_states = torch.cat([hidden_states[-1:], hidden_states[:-1]], dim=0).to(device)\n",
    "            #other_sample_history_seq_hidden_states = other_sample_hidden_states[:, :hs_len].to(device)\n",
    "            #other_sample_transform_input_seq_hidden_states = other_sample_hidden_states[:, hs_len:hs_len+tis_len].to(device)\n",
    "    \n",
    "            #other_sample_context_seq_hidden_states = torch.cat([context_seq_hidden_states[-1:], context_seq_hidden_states[:-1]], dim=0).to(device)\n",
    "            \n",
    "            #Cocon(input_hidden, content_hidden, history_hidden) #ROLE of history in cocon = concats to input\n",
    "            self_cocon_hidden_states = cocon_block(original_hidden_states, \n",
    "                                       context_seq=original_context_seq_hidden_states, \n",
    "                                       include_sos_output=True, cs_self_attn_mask_prob=1)\n",
    "            \n",
    "            #print('C: ',self_cocon_hidden_states.shape)\n",
    "            \n",
    "            #self_cocon_hidden_states = cocon_block(original_context_seq_hidden_states, \n",
    "                                       #include_sos_output=True, cs_self_attn_mask_prob=0)\n",
    "            \n",
    "            #print('C: ',self_cocon_hidden_states.shape)\n",
    "\n",
    "            \n",
    "            #self_cocon_lm_tail_input = torch.cat([original_hidden_states[:, :-1], self_cocon_hidden_states], dim=1).to(device)\n",
    "            #print('MUST MATCH LM LABELS: ',self_cocon_hidden_states.shape)\n",
    "            #print(lm_labels.shape)\n",
    "\n",
    "            # Ignore history when computing loss\n",
    "            #lm_logit_first_index = original_history_seq_hidden_states.shape[1] -1\n",
    "            #lm_labels_first_index = lm_logit_first_index + 1\n",
    "            \n",
    "            #L_beta([original_history_hidden + cocon_hidden])\n",
    "            self_cocon_lm_tail_outputs = model(input_hidden_state=self_cocon_hidden_states, \n",
    "                                               labels=lm_labels, lm_logit_first_index=0, \n",
    "                                               lm_labels_first_index=0, \n",
    "                                               input_before_block_ind=5+1)\n",
    "                        \n",
    "            #self_cocon_lm_tail_outputs[0][1] == self_cocon_lm_tail_outputs[1]\n",
    "            #next_token_logits = self_cocon_lm_tail_outputs[1]  # [N,L,C] where C is vocab_size\n",
    "            #if next_token_logits.shape[1] > 1:\n",
    "            #    next_token_logits = next_token_logits[:, -1:]\n",
    "            #next_cocon_output_prob = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            self_cocon_lm_loss = self_cocon_lm_tail_outputs[0] #SELF LOSS\n",
    "            total_loss = lambda_self * self_cocon_lm_loss\n",
    "            \n",
    "            #NULL LOSS ->modified to target sentiment\n",
    "            #L_alpha for input already done above\n",
    "            #L_alpha for content not req as null\n",
    "            #Cocon block [input_hidden, history]\n",
    "            #with torch.no_grad():\n",
    "            #    context_seq_hidden_states_null = model(target, path='half1')\n",
    "            #    context_seq_hidden_states_null = context_seq_hidden_states_null.to(device)\n",
    "\n",
    "            null_cocon_hidden_states = cocon_block(original_hidden_states, \n",
    "                                       context_seq=None,\n",
    "                                       include_sos_output=True)#, cs_self_attn_mask_prob=1- not req as noncontet to copy from\n",
    "            \n",
    "            #null_cocon_lm_tail_input = null_cocon_hidden_states\n",
    "\n",
    "            # Ignore history when computing loss - same as self\n",
    "            #lm_logit_first_index = original_history_seq_hidden_states.shape[1] -1\n",
    "            #lm_labels_first_index = lm_logit_first_index + 1\n",
    "            \n",
    "            #L_beta([original_history_hidden + cocon_hidden])\n",
    "            null_cocon_lm_tail_outputs = model(input_hidden_state=null_cocon_hidden_states, labels=lm_labels, \n",
    "                                               lm_logit_first_index=0, \n",
    "                                               lm_labels_first_index=0, \n",
    "                                               input_before_block_ind=5+1)\n",
    "            #self_cocon_lm_tail_outputs[0][1] == self_cocon_lm_tail_outputs[1]\n",
    "            #next_token_logits = self_cocon_lm_tail_outputs[1]  # [N,L,C] where C is vocab_size\n",
    "            #if next_token_logits.shape[1] > 1:\n",
    "            #    next_token_logits = next_token_logits[:, -1:]\n",
    "            #next_cocon_output_prob = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            null_cocon_lm_loss = null_cocon_lm_tail_outputs[0] #SELF LOSS\n",
    "            total_loss += lambda_null * null_cocon_lm_loss\n",
    "\n",
    "            cur_len = 0\n",
    "\n",
    "            \"\"\"\n",
    "            #CYCLE LOSS\n",
    "            #step1 : [other history, original content] -> compute mixed output\n",
    "            #Done AR so start with input=None\n",
    "             \n",
    "            cocon_block_output = None\n",
    "            cocon_th_gen_output = None \n",
    "            cocon_th_gen_input = None #[same as output but detatched]\n",
    "            cocon_output_embed = None #[used as content for step 2, instead of converting to token and back to hidden]\n",
    "\n",
    "            lm_tail_past = False #To indicate if consider entire embeding or only last one\n",
    "            lm_head_past = False #same as above\n",
    "            max_ar_len = original_transform_input_seq_hidden_states.shape[1]#20\n",
    "            \n",
    "            other_sample_history_seq_one_hot_prob = to_one_hot(other_sample_history_seq, n_dims=config.vocab_size).to(device)\n",
    "            other_sample_history_seq_embeds = torch.matmul(other_sample_history_seq_one_hot_prob, model.wte.weight).to(device)\n",
    "            \n",
    "            #Start auto regressive generation\n",
    "            while cur_len < max_ar_len:\n",
    "                #skipping L_alpha as we already have hidden states from previous computations\n",
    "                cocon_transformed_hidden_states = cocon_block(cocon_th_gen_input, context_seq=original_context_seq_hidden_states, history_seq=other_sample_history_seq_hidden_states, include_sos_output=True, cs_self_attn_mask_prob=1)\n",
    "                if cur_len == 0:\n",
    "                    cocon_block_output = cocon_transformed_hidden_states[:, -1:] #Consider last token as output\n",
    "                else:\n",
    "                    cocon_block_output = torch.cat([cocon_block_output, cocon_transformed_hidden_states[:,-1:]], dim=1)\n",
    "                \n",
    "                #\n",
    "                if cocon_th_gen_input is not None:\n",
    "                    hist_plus_cocon_hidden_states = torch.cat([other_sample_history_seq_hidden_states, cocon_th_gen_input[:, :-1], cocon_transformed_hidden_states[:, -1:]], dim=1)\n",
    "                else:\n",
    "                    hist_plus_cocon_hidden_states = torch.cat([other_sample_history_seq_hidden_states[:, :-1], cocon_transformed_hidden_states[:, -1:]], dim=1)\n",
    "                    \n",
    "                if(lm_tail_past):\n",
    "                    lm_tail_inputs = hist_plus_cocon_hidden_states[:, -1:, :].to(device)\n",
    "                else:\n",
    "                    lm_tail_inputs = hist_plus_cocon_hidden_states.to(device)\n",
    "                    \n",
    "                tail_outputs = model(lm_tail_inputs,path='half2')\n",
    "                next_token_logits = tail_outputs[1]  # [N,L,C] where C is vocab_size\n",
    "                if next_token_logits.shape[1] > 1:\n",
    "                    next_token_logits = next_token_logits[:, -1:]\n",
    "                lm_tail_past = True\n",
    "                next_cocon_output_prob = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "                next_cocon_output_embed = torch.matmul(next_cocon_output_prob, model.wte.weight).to(device)\n",
    "                if cur_len==0:\n",
    "                    cocon_output_embeds = next_cocon_output_embed.to(device)\n",
    "                    hist_plus_cocon_output_embeds = torch.cat([other_sample_history_seq_embeds, next_cocon_output_embed], dim=1).to(device)\n",
    "                else:\n",
    "                    cocon_output_embeds = torch.cat([cocon_output_embeds, next_cocon_output_embed], dim=1).to(device)\n",
    "                    hist_plus_cocon_output_embeds = torch.cat([hist_plus_cocon_output_embeds, next_cocon_output_embed], dim=1).to(device)\n",
    "                    \n",
    "                if(lm_head_past):\n",
    "                    lm_head_inputs = hist_plus_cocon_output_embeds[:, -1:, :].to(device)\n",
    "                else:\n",
    "                    lm_head_inputs = hist_plus_cocon_output_embeds.to(device)\n",
    "                    \n",
    "                head_outputs = model(lm_head_inputs, inputs_embeds=True)\n",
    "                cocon_gen_output_h = head_outputs[0][1] #tail_outputs[0][1].shape\n",
    "                if cocon_gen_output_h.shape[1] > 1:\n",
    "                    next_h = cocon_gen_output_h[:, -1:]\n",
    "                else:\n",
    "                    next_h = cocon_gen_output_h\n",
    "                lm_head_past = True\n",
    "                    \n",
    "                    \n",
    "                h_to_cat_input = next_h.detach()\n",
    "                if(cur_len ==0):\n",
    "                    cocon_th_gen_input = h_to_cat_input.to(device)\n",
    "                    cocon_th_gen_output = next_h.to(device)\n",
    "                else:\n",
    "                    cocon_th_gen_input = torch.cat([cocon_th_gen_input, h_to_cat_input], dim=1).to(device)\n",
    "                    cocon_th_gen_output = torch.cat([cocon_th_gen_output, next_h], dim=1).to(device)\n",
    "                \n",
    "                cur_len = cocon_th_gen_input.shape[1]\n",
    "            \n",
    "            #Completed AR generation \n",
    "            ar_cocon_final_output_embeds = cocon_output_embeds\n",
    "            \n",
    "            #STEP 1 - other history, original content = mixed\n",
    "            other_context_cocon_hidden_states = cocon_block(cocon_th_gen_output, context_seq=original_context_seq_hidden_states, history_seq=other_sample_history_seq_hidden_states, include_sos_output=True,cs_self_attn_mask_prob=1)\n",
    "            other_context_cocon_lm_tail_input = torch.cat([other_sample_history_seq_hidden_states[:, :-1], other_context_cocon_hidden_states], dim=1)\n",
    "            lm_logit_first_index = other_sample_history_seq_hidden_states.shape[1] -1\n",
    "            lm_labels_first_index = lm_logit_first_index + 1\n",
    "            \n",
    "            other_context_cocon_lm_tail_outputs = model(other_context_cocon_lm_tail_input, labels=other_sample_lm_labels, lm_logit_first_index=lm_logit_first_index, lm_labels_first_index=lm_labels_first_index, path='half2')\n",
    "            other_contex_cocon_lm_loss = other_context_cocon_lm_tail_outputs[0][0]\n",
    "            total_loss += other_contex_cocon_lm_loss * lambda_cycle\n",
    "            \n",
    "            \n",
    "            #STEP2 - original history, mixed content = original content\n",
    "            #to get mixed content we use the embedding directly to get hidden state instead of starting from tokens\n",
    "            ar_cocon_output_hidden_states = model(ar_cocon_final_output_embeds, inputs_embeds=True, path='half1')\n",
    "            #cocon - [original_transformed, mixed_content, original_history]\n",
    "            cycle_ar_cocon_recon_hidden_states = cocon_block(original_transform_input_seq_hidden_states, context_seq=ar_cocon_output_hidden_states, history_seq=original_history_seq_hidden_states, include_sos_output=True, cs_self_attn_mask_prob=1)\n",
    "            #update history to include cocon output\n",
    "            cycle_ar_cocon_recon_lm_tail_input = torch.cat([original_history_seq_hidden_states[:, :-1], cycle_ar_cocon_recon_hidden_states], dim=1)\n",
    "            #index after history\n",
    "            lm_logit_first_index = original_history_seq_hidden_states.shape[1] -1\n",
    "            #L_beta prediction\n",
    "            cycle_ar_cocon_recon_lm_tail_outputs = model(cycle_ar_cocon_recon_lm_tail_input, labels=lm_labels, lm_logit_first_index=lm_logit_first_index, lm_labels_first_index=lm_labels_first_index, path='half2')\n",
    "            \n",
    "            cycle_ar_cocon_recon_lm_loss = cycle_ar_cocon_recon_lm_tail_outputs[0][0]\n",
    "            total_loss += cycle_ar_cocon_recon_lm_loss * lambda_cycle\n",
    "            \"\"\"\n",
    "            \n",
    "            total_loss.backward()\n",
    "            #accelerator.backward(total_loss)\n",
    "        \n",
    "            tr_loss += total_loss.item()\n",
    "\n",
    "            #eval_loss, perplexity = evaluate()\n",
    "        \n",
    "            if (step + 1) % 100 == 0: \n",
    "                #ADD validation , bleue TODO\n",
    "                print(\"loss: \", tr_loss/(step+1))\n",
    "                torch.nn.utils.clip_grad_norm_(cocon_block.parameters(), 1)\n",
    "                cocon_block_optimizer.step() # opt.step() does not zero_grad, need to zero.grad() manually\n",
    "                cocon_block_scheduler.step() # Update learning rate schedule \n",
    "                torch.save({\n",
    "                          'epoch': epoch_ind,\n",
    "                          'model_state_dict': cocon_block.state_dict(),\n",
    "                          'optimizer_state_dict': cocon_block_optimizer.state_dict(),\n",
    "                          'loss': tr_loss,\n",
    "                          'step': step}, 'modified.tar')\n",
    "                \n",
    "                cocon_block.zero_grad()\n",
    "                model.zero_grad()\n",
    "            \n",
    "            global_step +=1\n",
    "            #if(step> epoch_max_steps or global_step>max_steps):\n",
    "             #   epoch_iterator.close()\n",
    "             #   break\n",
    "        #if(global_step>max_steps):\n",
    "        #    train_iterator.close()\n",
    "        #    break\n",
    "    return global_step, tr_loss/global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T06:39:35.034249Z",
     "iopub.status.busy": "2022-04-17T06:39:35.033999Z",
     "iopub.status.idle": "2022-04-17T07:29:23.937649Z",
     "shell.execute_reply": "2022-04-17T07:29:23.936818Z",
     "shell.execute_reply.started": "2022-04-17T06:39:35.034221Z"
    },
    "id": "me9haAjgXiFZ",
    "outputId": "e44a5262-aa2d-4a1f-8554-7cb6f1bc90b6"
   },
   "outputs": [],
   "source": [
    "global_step, tr_loss = train_cocon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T07:29:23.940618Z",
     "iopub.status.busy": "2022-04-17T07:29:23.939763Z",
     "iopub.status.idle": "2022-04-17T07:29:23.946710Z",
     "shell.execute_reply": "2022-04-17T07:29:23.946011Z",
     "shell.execute_reply.started": "2022-04-17T07:29:23.940580Z"
    },
    "id": "mdBh0Wn4XiFa",
    "outputId": "5973608f-caa2-46b9-80fb-43e8bb75ec2c"
   },
   "outputs": [],
   "source": [
    "global_step, tr_loss #2 epochs = (10662, 20.512922179115307)\n",
    "#4 epochs (10662, 10.538968369518473)\n",
    "# 6 epochs (10662, 9.399290811856767)\n",
    "#8 epochs (10662, 8.746620702734585)\n",
    "#10 epochs (10662, 8.746620702734585)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-17T07:29:23.948747Z",
     "iopub.status.busy": "2022-04-17T07:29:23.948053Z",
     "iopub.status.idle": "2022-04-17T07:29:24.021175Z",
     "shell.execute_reply": "2022-04-17T07:29:24.020427Z",
     "shell.execute_reply.started": "2022-04-17T07:29:23.948707Z"
    },
    "id": "MzZi3Dg09Ldl"
   },
   "outputs": [],
   "source": [
    "torch.save(cocon_block.state_dict(),'cocon_dgr_5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhmRATlNMWhK"
   },
   "outputs": [],
   "source": [
    "model = TheModelClass(*args, **kwargs)\n",
    "optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50263, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (instance_norm): InstanceNorm1d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50263, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "model_state_dict = torch.load('yelp_models/pytorch_model_zero_grad_1.bin',map_location=torch.device('cpu'))\n",
    "\n",
    "special_tokens = ['<POS>', '<NEG>','<CON_START>','<START>','<END>','<PAD>']\n",
    "special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "start_token_id = tokenizer.convert_tokens_to_ids(['<START>'])[0]\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(['<PAD>'])[0]\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.load_state_dict(model_state_dict, strict=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50260, 4252, 32481, 287, 262, 6766, 50261, 50262, 50262]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<START> sun shines in the sky <END> <PAD> <PAD>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocon_block = CoconBlock(model.config.n_ctx, model.config, scale=True)\n",
    "cocon_block = cocon_block.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cocon_block.load_state_dict(torch.load('yelp_models/cocon_dg_3l_pretained_gpt_2.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token_id = tokenizer.convert_tokens_to_ids(['<END>'])[0]\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(['<PAD>'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50261, 50262)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_token_id, pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uZVfyHS3XiFb"
   },
   "outputs": [],
   "source": [
    "def preditction_with_beam_search(ref_text, content=None,beam_width=3, max_len=30,vocab_length=40483, end_token_id=end_token_id ):\n",
    "    \"\"\"\n",
    "    This function decodes sentences using Beam Seach. \n",
    "    It will output #sentences = beam_width. This function works on a single example.\n",
    "    \n",
    "    ref_text : string : Input sentence\n",
    "    beam_width : int : Width of the output beam\n",
    "    vocab_length : int : Size of the Vocab after adding the special tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    done = [False for i in range(beam_width)] # To track which beams are already decoded\n",
    "    stop_decode = False\n",
    "    decoded_sentences=[] # List of decoded sentences at any given time\n",
    "    \n",
    "    sm = torch.nn.Softmax(dim=-1) # To calculate Softmax over the final layer Logits\n",
    "    tokens = tokenizer.tokenize(ref_text) # Tokenize the input text\n",
    "    if(content is not None):\n",
    "        content_tokens =  tokenizer.tokenize(content)\n",
    "        content_indexed_tokens = tokenizer.convert_tokens_to_ids(content_tokens)\n",
    "        content_index_tokens = [content_indexed_tokens for i in range(beam_width)]\n",
    "        content_torch_tensor = torch.tensor(content_index_tokens).to(device)\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens) # Convert tokens to ids\n",
    "    index_tokens = [indexed_tokens for i in range(beam_width)] # Replication of Input ids for all the beams\n",
    "\n",
    "    #index_tokens = [indexed_tokens for i in range(beam_width)]\n",
    "    torch_tensor = torch.tensor(index_tokens).to(device)\n",
    "    beam_indexes = [[] for i in range(beam_width)] # indexes of the current decoded beams\n",
    "    best_scoes = [0 for i in range(beam_width)] # A list of lists to store Probability values of each decoded token of best beams\n",
    "    count = 0\n",
    "    while count < max_len and not stop_decode:\n",
    "        if count == 0: # For the first step when only one sentence is availabe\n",
    "            with torch.no_grad():\n",
    "                # Calculate output probability distribution over the Vocab,\n",
    "                hidden_states = model(torch_tensor, output_after_block_ind=5)[0] \n",
    "                if(content is not None):\n",
    "                    context_seq_hidden_states =  model(content_torch_tensor, output_after_block_ind=5)[0] \n",
    "                else:\n",
    "                    context_seq_hidden_states = None\n",
    "           \n",
    "                original_hidden_states = hidden_states.to(device)\n",
    "                original_context_seq_hidden_states = context_seq_hidden_states.to(device)\n",
    "                \n",
    "                self_cocon_hidden_states = cocon_block(original_hidden_states, \n",
    "                                       context_seq=original_context_seq_hidden_states, \n",
    "                                       include_sos_output=True, cs_self_attn_mask_prob=0)\n",
    "        \n",
    "                self_cocon_lm_tail_outputs = model(input_hidden_state=self_cocon_hidden_states,lm_logit_first_index=0, \n",
    "                                               lm_labels_first_index=0, \n",
    "                                               input_before_block_ind=5+1)\n",
    "            \n",
    "            #print(self_cocon_lm_tail_outputs[0].shape,self_cocon_lm_tail_outputs[1].shape)\n",
    "                #output = model(torch_tensor)\n",
    "                preds = sm(self_cocon_lm_tail_outputs[0]) #  shape = [beam_bidth, len(input_sen)+1,Vocab_length]\n",
    "            top_v, top_i = preds[:,-1,:].topk(beam_width) # Fatch top indexes and it's values\n",
    "            [beam_indexes[i].append(top_i[0][i].tolist()) for i in range(beam_width)] # Update the Beam indexes\n",
    "            # Update the best_scores, for first time just add the topk values directly\n",
    "            for i in range(beam_width):\n",
    "                best_scoes[i] = top_v[0][i].item()\n",
    "            count += 1\n",
    "        else: # After first step\n",
    "            # Prepare the current_state by concating original input and decoded beam indexes\n",
    "            current_state = torch.cat((torch_tensor, torch.tensor(beam_indexes).to(device)), dim=1)\n",
    "            # Prediction on the current state\n",
    "            with torch.no_grad():\n",
    "                hidden_states = model(current_state, output_after_block_ind=5)[0] \n",
    "                #if(content is not None):\n",
    "                 #   context_seq_hidden_states =  model(content_torch_tensor, output_after_block_ind=5)[0] \n",
    "                #else:\n",
    "                #    context_seq_hidden_states = None\n",
    "           \n",
    "                original_hidden_states = hidden_states.to(device)\n",
    "                #original_context_seq_hidden_states = context_seq_hidden_states.to(device)\n",
    "                \n",
    "                self_cocon_hidden_states = cocon_block(original_hidden_states, \n",
    "                                       context_seq=original_context_seq_hidden_states, \n",
    "                                       include_sos_output=True, cs_self_attn_mask_prob=0)\n",
    "        \n",
    "                self_cocon_lm_tail_outputs = model(input_hidden_state=self_cocon_hidden_states,lm_logit_first_index=0, \n",
    "                                               lm_labels_first_index=0, \n",
    "                                               input_before_block_ind=5+1)\n",
    "                \n",
    "                \n",
    "                #outputs = model(current_state)\n",
    "                preds = sm(self_cocon_lm_tail_outputs[0])\n",
    "            # Multiply new probability predictions with corresponding best scores\n",
    "            # Total socres = beam_width * Vocab_Size\n",
    "            flatten_score = (preds[:,-1,:]*torch.tensor(best_scoes).to(device).unsqueeze(1)).view(-1)\n",
    "            # Fatch the top scores and indexes \n",
    "            vals, inx = flatten_score.topk(beam_width)\n",
    "            # best_score_inx saves the index of best beams after multiplying the probability of new prediction\n",
    "            best_scoes_inx = (inx / vocab_length).tolist()\n",
    "            best_scoes = vals.tolist()\n",
    "            # Unflatten the index \n",
    "            correct_inx = (inx % vocab_length).tolist()\n",
    "            \n",
    "            # Check if done for all the Beams\n",
    "            for i in range(beam_width):\n",
    "                if correct_inx[i] == end_token_id:\n",
    "                    done[i] = True\n",
    "            # Update the best score for each the current Beams\n",
    "            for i in range(beam_width):\n",
    "                if not done[i]:\n",
    "                    best_scoes[i] = vals.tolist()[i]\n",
    "            # Check is All the Beams are Done\n",
    "            if (sum(done) == beam_width):\n",
    "                stop_decode = True\n",
    "            # Prepapre the new beams\n",
    "            temp_lt=[0 for i in range(beam_width)]\n",
    "            for i,x in enumerate(best_scoes_inx):\n",
    "                temp_lt[i] = beam_indexes[i] + [correct_inx[i]]\n",
    "            # Update the Beam indexes\n",
    "            beam_indexes = temp_lt\n",
    "            del temp_lt\n",
    "            count += 1\n",
    "    # Decode All the beam indexes to till <END> token only and convert into sentence\n",
    "    for i in range(beam_width):\n",
    "        try:\n",
    "            end_index = beam_indexes[i].index(end_token_id )\n",
    "        except ValueError:\n",
    "            end_index = len(beam_indexes[i])\n",
    "            \n",
    "        decoded_sentences.append(tokenizer.decode(beam_indexes[i][:end_index]))\n",
    "        \n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt(inp,cont,gen_len=30):\n",
    "    input_token = torch.tensor(tokenizer.encode(inp))\n",
    "    content_token = torch.tensor(tokenizer.encode(cont))\n",
    "    if(len(input_token.shape)<3):\n",
    "        input_token = input_token.unsqueeze(0) #batch dim\n",
    "        content_token = content_token.unsqueeze(0) #batch dim\n",
    "\n",
    "    #Repeat for history TO DO\n",
    "    #implement auto regression TODO\n",
    "    input_token = input_token.to(device)\n",
    "    content_token = content_token.to(device)\n",
    "    l = len(input_token[0])\n",
    "    for i in range(gen_len):\n",
    "        #L_alpha\n",
    "        with torch.no_grad():\n",
    "            hidden_inp = model(input_token, output_after_block_ind=5)[0] \n",
    "            hidden_content = model(content_token, output_after_block_ind=5)[0] \n",
    "            #Cocon             other_context_cocon_hidden_states = cocon_block(cocon_th_gen_output, context_seq=original_context_seq_hidden_states, history_seq=other_sample_history_seq_hidden_states, include_sos_output=True,cs_self_attn_mask_prob=1)\n",
    "            cout = cocon_block(hidden_inp, context_seq=hidden_content)\n",
    "            output = model(input_hidden_state=cout,lm_logit_first_index=0, \n",
    "                                               lm_labels_first_index=0, \n",
    "                                               input_before_block_ind=5+1)\n",
    "            pred_token_logits = output[0][:,-1:] \n",
    "        #softmax\n",
    "        pred_token_prob = torch.nn.functional.softmax(pred_token_logits, dim=-1)#[:,-1,:]\n",
    "        #sample\n",
    "        pred_token = torch.multinomial(pred_token_prob[0], num_samples=1) #repeat for every elem in batch\n",
    "        #append\n",
    "        input_token = torch.cat((input_token,pred_token),1)\n",
    "        #decode\n",
    "    #pred_text = tokenizer.decode(input_token)\n",
    "    return input_token, [tokenizer.decode(i) for i in input_token[:,l:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>gold</th>\n",
       "      <th>content</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred_mp</th>\n",
       "      <th>plain_pred1</th>\n",
       "      <th>plain_pred2</th>\n",
       "      <th>plain_pred_mp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; i recommend checking this pl...</td>\n",
       "      <td>i highly recommend checking this place out.</td>\n",
       "      <td>highly</td>\n",
       "      <td>.always actors fog fog. fog. fog.</td>\n",
       "      <td>always proposal great actors. fogalways actor...</td>\n",
       "      <td>small definitely. fantastic.always.they recom...</td>\n",
       "      <td>highly highly checking place. highly highly t...</td>\n",
       "      <td>recommenditageitageitageitageitageitageitagei...</td>\n",
       "      <td>highly highly checking place. highly highly t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; not only is there pizza, but...</td>\n",
       "      <td>not only is there pizza bad, but their custome...</td>\n",
       "      <td>bad horrible</td>\n",
       "      <td>.ues.always actors fog. actors. actors</td>\n",
       "      <td>alwaysalways actorsgreat. great actorsalways ...</td>\n",
       "      <td>.again.wrong always too rude always.att</td>\n",
       "      <td>only is pizza but customer is. Kurd not is</td>\n",
       "      <td>even there qualified, wantingetooth wanting w...</td>\n",
       "      <td>only is pizza, their service horrible horribl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; cool tram that has views goi...</td>\n",
       "      <td>cool tram that has great views going up or dow...</td>\n",
       "      <td>great</td>\n",
       "      <td>always actors actors actors actors actors Kur...</td>\n",
       "      <td>love great always always fog always actors Ku...</td>\n",
       "      <td>owned awesome.keep no months.cious best love</td>\n",
       "      <td>tram has views up down down theitt extended e...</td>\n",
       "      <td>cool always great Mayor MayorGood Mayor pesti...</td>\n",
       "      <td>ixed tram has going or of psburgh skyline &lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; she was! &lt;START&gt;</td>\n",
       "      <td>she was fantastic!</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>always!always! adulthood always actors adulth...</td>\n",
       "      <td>love. travel.always removing. travel..</td>\n",
       "      <td>spot! delicious always great &lt;END&gt;  excellent...</td>\n",
       "      <td>was!she amazing Kurd!she amazing Kurd Kurd</td>\n",
       "      <td>she! Kurd scarthink Quebecthink awesome fogthink</td>\n",
       "      <td>was!she phenomenal &lt;END&gt;  was! &lt;END&gt;  was!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; however the food - oh the fo...</td>\n",
       "      <td>however the food - oh the food : ( - i was dis...</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>always. actors actors great. actors actors.al...</td>\n",
       "      <td>. Portlandalways.. Portlandalways.! always</td>\n",
       "      <td>unfortunately.best.worst.always. awful.</td>\n",
       "      <td>ever food oh food ( - i disappointed disappoin...</td>\n",
       "      <td>ever thearia the bad i - scar forecast disapp...</td>\n",
       "      <td>ever food oh food food disgusting ( i sgy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <POS> <CON_START> i recommend checking this pl...   \n",
       "1  <NEG> <CON_START> not only is there pizza, but...   \n",
       "2  <POS> <CON_START> cool tram that has views goi...   \n",
       "3                 <POS> <CON_START> she was! <START>   \n",
       "4  <NEG> <CON_START> however the food - oh the fo...   \n",
       "\n",
       "                                                gold       content  \\\n",
       "0        i highly recommend checking this place out.        highly   \n",
       "1  not only is there pizza bad, but their custome...  bad horrible   \n",
       "2  cool tram that has great views going up or dow...         great   \n",
       "3                                 she was fantastic!     fantastic   \n",
       "4  however the food - oh the food : ( - i was dis...  disappointed   \n",
       "\n",
       "                                               pred1  \\\n",
       "0                  .always actors fog fog. fog. fog.   \n",
       "1             .ues.always actors fog. actors. actors   \n",
       "2   always actors actors actors actors actors Kur...   \n",
       "3   always!always! adulthood always actors adulth...   \n",
       "4   always. actors actors great. actors actors.al...   \n",
       "\n",
       "                                               pred2  \\\n",
       "0   always proposal great actors. fogalways actor...   \n",
       "1   alwaysalways actorsgreat. great actorsalways ...   \n",
       "2   love great always always fog always actors Ku...   \n",
       "3             love. travel.always removing. travel..   \n",
       "4         . Portlandalways.. Portlandalways.! always   \n",
       "\n",
       "                                             pred_mp  \\\n",
       "0   small definitely. fantastic.always.they recom...   \n",
       "1            .again.wrong always too rude always.att   \n",
       "2       owned awesome.keep no months.cious best love   \n",
       "3   spot! delicious always great <END>  excellent...   \n",
       "4            unfortunately.best.worst.always. awful.   \n",
       "\n",
       "                                         plain_pred1  \\\n",
       "0   highly highly checking place. highly highly t...   \n",
       "1         only is pizza but customer is. Kurd not is   \n",
       "2   tram has views up down down theitt extended e...   \n",
       "3         was!she amazing Kurd!she amazing Kurd Kurd   \n",
       "4  ever food oh food ( - i disappointed disappoin...   \n",
       "\n",
       "                                         plain_pred2  \\\n",
       "0   recommenditageitageitageitageitageitageitagei...   \n",
       "1   even there qualified, wantingetooth wanting w...   \n",
       "2   cool always great Mayor MayorGood Mayor pesti...   \n",
       "3   she! Kurd scarthink Quebecthink awesome fogthink   \n",
       "4   ever thearia the bad i - scar forecast disapp...   \n",
       "\n",
       "                                       plain_pred_mp  \n",
       "0   highly highly checking place. highly highly t...  \n",
       "1   only is pizza, their service horrible horribl...  \n",
       "2    ixed tram has going or of psburgh skyline <END>  \n",
       "3         was!she phenomenal <END>  was! <END>  was!  \n",
       "4          ever food oh food food disgusting ( i sgy  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_gpt2_cocon = pd.read_csv('yelp_models/cocon_gpt_preds.csv')\n",
    "df_gpt2_cocon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = list(df_gpt2_cocon['input'])\n",
    "content_ids = list(df_gpt2_cocon['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<POS> <CON_START> i recommend checking this place out. <START>',\n",
       " [50257, 50259, 1312, 4313, 10627, 428, 1295, 503, 13, 50260])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs = \n",
    "input_ids[0], tokenizer.encode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<POS> <CON_START> i recommend checking this place out. <START>'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids =torch.tensor(tokenizer.encode(input_ids))\n",
    "content_ids = torch.tensor(tokenizer.encode(content_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(50256), tensor(47444))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[3], content_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50257, 50259,  1312,  4313, 10627,   428,  1295,   503,    13, 50260]]),\n",
       " tensor([[47444]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token = torch.tensor(tokenizer.encode(input_ids[0]))\n",
    "content_token = torch.tensor(tokenizer.encode(content_ids[0]))\n",
    "if(len(input_token.shape)<3):\n",
    "    input_token = input_token.unsqueeze(0) #batch dim\n",
    "    content_token = content_token.unsqueeze(0)\n",
    "    \n",
    "input_token, content_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>gold</th>\n",
       "      <th>content</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred_mp</th>\n",
       "      <th>plain_pred1</th>\n",
       "      <th>plain_pred2</th>\n",
       "      <th>plain_pred_mp</th>\n",
       "      <th>both_3l_pred1</th>\n",
       "      <th>both_3l_pred2</th>\n",
       "      <th>both_3l_pred_mp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; i recommend checking this pl...</td>\n",
       "      <td>i highly recommend checking this place out.</td>\n",
       "      <td>highly</td>\n",
       "      <td>.always actors fog fog. fog. fog.</td>\n",
       "      <td>always proposal great actors. fogalways actor...</td>\n",
       "      <td>small definitely. fantastic.always.they recom...</td>\n",
       "      <td>highly highly checking place. highly highly t...</td>\n",
       "      <td>recommenditageitageitageitageitageitageitagei...</td>\n",
       "      <td>highly highly checking place. highly highly t...</td>\n",
       "      <td>recommend this out.itage actorsigaitage love ...</td>\n",
       "      <td>definitely brushelled highly. highly recommen...</td>\n",
       "      <td>recommend this out you. main mindiquette love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; not only is there pizza, but...</td>\n",
       "      <td>not only is there pizza bad, but their custome...</td>\n",
       "      <td>bad horrible</td>\n",
       "      <td>.ues.always actors fog. actors. actors</td>\n",
       "      <td>alwaysalways actorsgreat. great actorsalways ...</td>\n",
       "      <td>.again.wrong always too rude always.att</td>\n",
       "      <td>only is pizza but customer is. Kurd not is</td>\n",
       "      <td>even there qualified, wantingetooth wanting w...</td>\n",
       "      <td>only is pizza, their service horrible horribl...</td>\n",
       "      <td>also there'); is pizza but customer is..</td>\n",
       "      <td>only is pizza mit decrease collective decreas...</td>\n",
       "      <td>also there is pizza but customer isable &lt;END&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; cool tram that has views goi...</td>\n",
       "      <td>cool tram that has great views going up or dow...</td>\n",
       "      <td>great</td>\n",
       "      <td>always actors actors actors actors actors Kur...</td>\n",
       "      <td>love great always always fog always actors Ku...</td>\n",
       "      <td>owned awesome.keep no months.cious best love</td>\n",
       "      <td>tram has views up down down theitt extended e...</td>\n",
       "      <td>cool always great Mayor MayorGood Mayor pesti...</td>\n",
       "      <td>ixed tram has going or of psburgh skyline &lt;END&gt;</td>\n",
       "      <td>great tram has views up down down the of p</td>\n",
       "      <td>cool Mayor Mayor MayorGood or hotel down pestial</td>\n",
       "      <td>cool that views going or of psburgh skyline the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; she was! &lt;START&gt;</td>\n",
       "      <td>she was fantastic!</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>always!always! adulthood always actors adulth...</td>\n",
       "      <td>love. travel.always removing. travel..</td>\n",
       "      <td>spot! delicious always great &lt;END&gt;  excellent...</td>\n",
       "      <td>was!she amazing Kurd!she amazing Kurd Kurd</td>\n",
       "      <td>she! Kurd scarthink Quebecthink awesome fogthink</td>\n",
       "      <td>was!she phenomenal &lt;END&gt;  was! &lt;END&gt;  was!</td>\n",
       "      <td>was!she was! gene! Kurd was!</td>\n",
       "      <td>she. she scar. Kurd actors gene!doors</td>\n",
       "      <td>you was!.. &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; however the food - oh the fo...</td>\n",
       "      <td>however the food - oh the food : ( - i was dis...</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>always. actors actors great. actors actors.al...</td>\n",
       "      <td>. Portlandalways.. Portlandalways.! always</td>\n",
       "      <td>unfortunately.best.worst.always. awful.</td>\n",
       "      <td>ever food oh food ( - i disappointed disappoin...</td>\n",
       "      <td>ever thearia the bad i - scar forecast disapp...</td>\n",
       "      <td>ever food oh food food disgusting ( i sgy</td>\n",
       "      <td>the - the : - food i i WAR.</td>\n",
       "      <td>love food Montgomery Montgomery i tempor temp...</td>\n",
       "      <td>them- food oh nasty food oh terrible!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <POS> <CON_START> i recommend checking this pl...   \n",
       "1  <NEG> <CON_START> not only is there pizza, but...   \n",
       "2  <POS> <CON_START> cool tram that has views goi...   \n",
       "3                 <POS> <CON_START> she was! <START>   \n",
       "4  <NEG> <CON_START> however the food - oh the fo...   \n",
       "\n",
       "                                                gold       content  \\\n",
       "0        i highly recommend checking this place out.        highly   \n",
       "1  not only is there pizza bad, but their custome...  bad horrible   \n",
       "2  cool tram that has great views going up or dow...         great   \n",
       "3                                 she was fantastic!     fantastic   \n",
       "4  however the food - oh the food : ( - i was dis...  disappointed   \n",
       "\n",
       "                                               pred1  \\\n",
       "0                  .always actors fog fog. fog. fog.   \n",
       "1             .ues.always actors fog. actors. actors   \n",
       "2   always actors actors actors actors actors Kur...   \n",
       "3   always!always! adulthood always actors adulth...   \n",
       "4   always. actors actors great. actors actors.al...   \n",
       "\n",
       "                                               pred2  \\\n",
       "0   always proposal great actors. fogalways actor...   \n",
       "1   alwaysalways actorsgreat. great actorsalways ...   \n",
       "2   love great always always fog always actors Ku...   \n",
       "3             love. travel.always removing. travel..   \n",
       "4         . Portlandalways.. Portlandalways.! always   \n",
       "\n",
       "                                             pred_mp  \\\n",
       "0   small definitely. fantastic.always.they recom...   \n",
       "1            .again.wrong always too rude always.att   \n",
       "2       owned awesome.keep no months.cious best love   \n",
       "3   spot! delicious always great <END>  excellent...   \n",
       "4            unfortunately.best.worst.always. awful.   \n",
       "\n",
       "                                         plain_pred1  \\\n",
       "0   highly highly checking place. highly highly t...   \n",
       "1         only is pizza but customer is. Kurd not is   \n",
       "2   tram has views up down down theitt extended e...   \n",
       "3         was!she amazing Kurd!she amazing Kurd Kurd   \n",
       "4  ever food oh food ( - i disappointed disappoin...   \n",
       "\n",
       "                                         plain_pred2  \\\n",
       "0   recommenditageitageitageitageitageitageitagei...   \n",
       "1   even there qualified, wantingetooth wanting w...   \n",
       "2   cool always great Mayor MayorGood Mayor pesti...   \n",
       "3   she! Kurd scarthink Quebecthink awesome fogthink   \n",
       "4   ever thearia the bad i - scar forecast disapp...   \n",
       "\n",
       "                                       plain_pred_mp  \\\n",
       "0   highly highly checking place. highly highly t...   \n",
       "1   only is pizza, their service horrible horribl...   \n",
       "2    ixed tram has going or of psburgh skyline <END>   \n",
       "3         was!she phenomenal <END>  was! <END>  was!   \n",
       "4          ever food oh food food disgusting ( i sgy   \n",
       "\n",
       "                                       both_3l_pred1  \\\n",
       "0   recommend this out.itage actorsigaitage love ...   \n",
       "1           also there'); is pizza but customer is..   \n",
       "2         great tram has views up down down the of p   \n",
       "3                       was!she was! gene! Kurd was!   \n",
       "4                        the - the : - food i i WAR.   \n",
       "\n",
       "                                       both_3l_pred2  \\\n",
       "0   definitely brushelled highly. highly recommen...   \n",
       "1   only is pizza mit decrease collective decreas...   \n",
       "2   cool Mayor Mayor MayorGood or hotel down pestial   \n",
       "3              she. she scar. Kurd actors gene!doors   \n",
       "4   love food Montgomery Montgomery i tempor temp...   \n",
       "\n",
       "                                     both_3l_pred_mp  \n",
       "0   recommend this out you. main mindiquette love...  \n",
       "1   also there is pizza but customer isable <END>...  \n",
       "2    cool that views going or of psburgh skyline the  \n",
       "3           you was!.. <PAD> <PAD> <PAD> <PAD> <PAD>  \n",
       "4             them- food oh nasty food oh terrible!!  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "outs_mp = []\n",
    "outs1 = []\n",
    "outs2 = []\n",
    "#tqdm_bar = tqdm(eval_dataloader, desc=\"batch iteration\")\n",
    "for i in range(len(input_ids)):\n",
    "    op=preditction_with_beam_search(input_ids[i],content_ids[i],2, 10)\n",
    "    outs1.append(op[0])\n",
    "    outs2.append(op[1])\n",
    "    op = generate_gpt(input_ids[i],content_ids[i],gen_len=10)\n",
    "    outs_mp.append(op[1][0])\n",
    "    #print(input_ids[0], content_ids[0])\n",
    "    #break\n",
    "\n",
    "\n",
    "df_gpt2_cocon['plain_r_pred1'] = outs1\n",
    "df_gpt2_cocon['plain_r_pred2'] = outs2\n",
    "df_gpt2_cocon['plain_r_pred_mp'] = outs_mp\n",
    "#print(outs1, outs2, outs_mp)\n",
    "    #df_gpt2.to_csv('yelp_model/plain_gpt.csv', index=False)\n",
    "df_gpt2_cocon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|                                                                              | 0/2626 [01:20<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>gold</th>\n",
       "      <th>content</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred_mp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; i recommend checking this pl...</td>\n",
       "      <td>i highly recommend checking this place out.</td>\n",
       "      <td>highly</td>\n",
       "      <td>.always actors fog fog. fog. fog.</td>\n",
       "      <td>always proposal great actors. fogalways actor...</td>\n",
       "      <td>small definitely. fantastic.always.they recom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <POS> <CON_START> i recommend checking this pl...   \n",
       "\n",
       "                                          gold content  \\\n",
       "0  i highly recommend checking this place out.  highly   \n",
       "\n",
       "                               pred1  \\\n",
       "0  .always actors fog fog. fog. fog.   \n",
       "\n",
       "                                               pred2  \\\n",
       "0   always proposal great actors. fogalways actor...   \n",
       "\n",
       "                                             pred_mp  \n",
       "0   small definitely. fantastic.always.they recom...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpt2_cocon = pd.DataFrame()\n",
    "inpu = []\n",
    "outp = []\n",
    "conp = []\n",
    "outs1 = []\n",
    "outs2 = []\n",
    "outs_mp = []\n",
    "\n",
    "epoch_iterator = tqdm(eval_dataloader, desc=\"Iteration\")\n",
    "for step, batch in enumerate(epoch_iterator):\n",
    "    inputs, content, lm_labels = batch\n",
    "    input_ids = inputs.to(device)\n",
    "    lm_labels = lm_labels.to(device)\n",
    "    content = content.to(device)\n",
    "    for i in range(len(input_ids)):\n",
    "        #print(tokenizer.decode(input_ids[i]))\n",
    "        b = input_ids[i]==start_token_id\n",
    "        indices = b.nonzero()[0][0]\n",
    "        input_ = input_ids[i][:indices + 1]\n",
    "        \n",
    "        b = input_ids[i]==end_token_id\n",
    "        indices_end = b.nonzero()[0][0]\n",
    "        \n",
    "        target = tokenizer.decode(input_ids[i][indices + 1:indices_end])\n",
    "        #print(tokenizer.decode(input_))\n",
    "        input_ = tokenizer.decode(input_)\n",
    "        \n",
    "        b = content[i]==pad_token_id\n",
    "        indices = b.nonzero()[0][0]\n",
    "        content_ = content[i][:indices]\n",
    "        \n",
    "        content_ = tokenizer.decode(content_)\n",
    "        #inpu.append(input_)\n",
    "        #outp.append(target)\n",
    "        inpu.append(input_)\n",
    "        outp.append(target)\n",
    "        conp.append(content_)\n",
    "        op=preditction_with_beam_search(input_,content_,2, 10)\n",
    "        outs1.append(op[0])\n",
    "        outs2.append(op[1])\n",
    "        op = generate_gpt(input_,content_,gen_len=10)\n",
    "        outs_mp.append(op[1][0])\n",
    "    break\n",
    "    \n",
    "df_gpt2_cocon['input'] = inpu\n",
    "df_gpt2_cocon['gold'] = outp\n",
    "df_gpt2_cocon['content'] = conp\n",
    "df_gpt2_cocon['pred1'] = outs1\n",
    "df_gpt2_cocon['pred2'] = outs2\n",
    "df_gpt2_cocon['pred_mp'] = outs_mp\n",
    "    \n",
    "df_gpt2_cocon.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>gold</th>\n",
       "      <th>content</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred_mp</th>\n",
       "      <th>plain_pred1</th>\n",
       "      <th>plain_pred2</th>\n",
       "      <th>plain_pred_mp</th>\n",
       "      <th>both_3l_pred1</th>\n",
       "      <th>both_3l_pred2</th>\n",
       "      <th>both_3l_pred_mp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; i recommend checking this pl...</td>\n",
       "      <td>i highly recommend checking this place out.</td>\n",
       "      <td>highly</td>\n",
       "      <td>.always actors fog fog. fog. fog.</td>\n",
       "      <td>always proposal great actors. fogalways actor...</td>\n",
       "      <td>small definitely. fantastic.always.they recom...</td>\n",
       "      <td>highly highly checking place. highly highly t...</td>\n",
       "      <td>recommenditageitageitageitageitageitageitagei...</td>\n",
       "      <td>highly highly checking place. highly highly t...</td>\n",
       "      <td>recommend this out.itage actorsigaitage love ...</td>\n",
       "      <td>definitely brushelled highly. highly recommen...</td>\n",
       "      <td>recommend this out you. main mindiquette love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; not only is there pizza, but...</td>\n",
       "      <td>not only is there pizza bad, but their custome...</td>\n",
       "      <td>bad horrible</td>\n",
       "      <td>.ues.always actors fog. actors. actors</td>\n",
       "      <td>alwaysalways actorsgreat. great actorsalways ...</td>\n",
       "      <td>.again.wrong always too rude always.att</td>\n",
       "      <td>only is pizza but customer is. Kurd not is</td>\n",
       "      <td>even there qualified, wantingetooth wanting w...</td>\n",
       "      <td>only is pizza, their service horrible horribl...</td>\n",
       "      <td>also there'); is pizza but customer is..</td>\n",
       "      <td>only is pizza mit decrease collective decreas...</td>\n",
       "      <td>also there is pizza but customer isable &lt;END&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; cool tram that has views goi...</td>\n",
       "      <td>cool tram that has great views going up or dow...</td>\n",
       "      <td>great</td>\n",
       "      <td>always actors actors actors actors actors Kur...</td>\n",
       "      <td>love great always always fog always actors Ku...</td>\n",
       "      <td>owned awesome.keep no months.cious best love</td>\n",
       "      <td>tram has views up down down theitt extended e...</td>\n",
       "      <td>cool always great Mayor MayorGood Mayor pesti...</td>\n",
       "      <td>ixed tram has going or of psburgh skyline &lt;END&gt;</td>\n",
       "      <td>great tram has views up down down the of p</td>\n",
       "      <td>cool Mayor Mayor MayorGood or hotel down pestial</td>\n",
       "      <td>cool that views going or of psburgh skyline the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;POS&gt; &lt;CON_START&gt; she was! &lt;START&gt;</td>\n",
       "      <td>she was fantastic!</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>always!always! adulthood always actors adulth...</td>\n",
       "      <td>love. travel.always removing. travel..</td>\n",
       "      <td>spot! delicious always great &lt;END&gt;  excellent...</td>\n",
       "      <td>was!she amazing Kurd!she amazing Kurd Kurd</td>\n",
       "      <td>she! Kurd scarthink Quebecthink awesome fogthink</td>\n",
       "      <td>was!she phenomenal &lt;END&gt;  was! &lt;END&gt;  was!</td>\n",
       "      <td>was!she was! gene! Kurd was!</td>\n",
       "      <td>she. she scar. Kurd actors gene!doors</td>\n",
       "      <td>you was!.. &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;NEG&gt; &lt;CON_START&gt; however the food - oh the fo...</td>\n",
       "      <td>however the food - oh the food : ( - i was dis...</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>always. actors actors great. actors actors.al...</td>\n",
       "      <td>. Portlandalways.. Portlandalways.! always</td>\n",
       "      <td>unfortunately.best.worst.always. awful.</td>\n",
       "      <td>ever food oh food ( - i disappointed disappoin...</td>\n",
       "      <td>ever thearia the bad i - scar forecast disapp...</td>\n",
       "      <td>ever food oh food food disgusting ( i sgy</td>\n",
       "      <td>the - the : - food i i WAR.</td>\n",
       "      <td>love food Montgomery Montgomery i tempor temp...</td>\n",
       "      <td>them- food oh nasty food oh terrible!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  <POS> <CON_START> i recommend checking this pl...   \n",
       "1  <NEG> <CON_START> not only is there pizza, but...   \n",
       "2  <POS> <CON_START> cool tram that has views goi...   \n",
       "3                 <POS> <CON_START> she was! <START>   \n",
       "4  <NEG> <CON_START> however the food - oh the fo...   \n",
       "\n",
       "                                                gold       content  \\\n",
       "0        i highly recommend checking this place out.        highly   \n",
       "1  not only is there pizza bad, but their custome...  bad horrible   \n",
       "2  cool tram that has great views going up or dow...         great   \n",
       "3                                 she was fantastic!     fantastic   \n",
       "4  however the food - oh the food : ( - i was dis...  disappointed   \n",
       "\n",
       "                                               pred1  \\\n",
       "0                  .always actors fog fog. fog. fog.   \n",
       "1             .ues.always actors fog. actors. actors   \n",
       "2   always actors actors actors actors actors Kur...   \n",
       "3   always!always! adulthood always actors adulth...   \n",
       "4   always. actors actors great. actors actors.al...   \n",
       "\n",
       "                                               pred2  \\\n",
       "0   always proposal great actors. fogalways actor...   \n",
       "1   alwaysalways actorsgreat. great actorsalways ...   \n",
       "2   love great always always fog always actors Ku...   \n",
       "3             love. travel.always removing. travel..   \n",
       "4         . Portlandalways.. Portlandalways.! always   \n",
       "\n",
       "                                             pred_mp  \\\n",
       "0   small definitely. fantastic.always.they recom...   \n",
       "1            .again.wrong always too rude always.att   \n",
       "2       owned awesome.keep no months.cious best love   \n",
       "3   spot! delicious always great <END>  excellent...   \n",
       "4            unfortunately.best.worst.always. awful.   \n",
       "\n",
       "                                         plain_pred1  \\\n",
       "0   highly highly checking place. highly highly t...   \n",
       "1         only is pizza but customer is. Kurd not is   \n",
       "2   tram has views up down down theitt extended e...   \n",
       "3         was!she amazing Kurd!she amazing Kurd Kurd   \n",
       "4  ever food oh food ( - i disappointed disappoin...   \n",
       "\n",
       "                                         plain_pred2  \\\n",
       "0   recommenditageitageitageitageitageitageitagei...   \n",
       "1   even there qualified, wantingetooth wanting w...   \n",
       "2   cool always great Mayor MayorGood Mayor pesti...   \n",
       "3   she! Kurd scarthink Quebecthink awesome fogthink   \n",
       "4   ever thearia the bad i - scar forecast disapp...   \n",
       "\n",
       "                                       plain_pred_mp  \\\n",
       "0   highly highly checking place. highly highly t...   \n",
       "1   only is pizza, their service horrible horribl...   \n",
       "2    ixed tram has going or of psburgh skyline <END>   \n",
       "3         was!she phenomenal <END>  was! <END>  was!   \n",
       "4          ever food oh food food disgusting ( i sgy   \n",
       "\n",
       "                                       both_3l_pred1  \\\n",
       "0   recommend this out.itage actorsigaitage love ...   \n",
       "1           also there'); is pizza but customer is..   \n",
       "2         great tram has views up down down the of p   \n",
       "3                       was!she was! gene! Kurd was!   \n",
       "4                        the - the : - food i i WAR.   \n",
       "\n",
       "                                       both_3l_pred2  \\\n",
       "0   definitely brushelled highly. highly recommen...   \n",
       "1   only is pizza mit decrease collective decreas...   \n",
       "2   cool Mayor Mayor MayorGood or hotel down pestial   \n",
       "3              she. she scar. Kurd actors gene!doors   \n",
       "4   love food Montgomery Montgomery i tempor temp...   \n",
       "\n",
       "                                     both_3l_pred_mp  \n",
       "0   recommend this out you. main mindiquette love...  \n",
       "1   also there is pizza but customer isable <END>...  \n",
       "2    cool that views going or of psburgh skyline the  \n",
       "3           you was!.. <PAD> <PAD> <PAD> <PAD> <PAD>  \n",
       "4             them- food oh nasty food oh terrible!!  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpt2_cocon.to_csv('yelp_models/cocon_gpt_preds.csv', index=False)\n",
    "df_gpt2_cocon.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
